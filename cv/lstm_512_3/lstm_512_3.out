using CUDA on GPU 0...	
loading data files...	
cutting off end of data so that the batches/sequences divide evenly	
reshaping tensor...	
data load done. Number of data batches in train: 54, val: 3, test: 0	
vocab size: 91	
creating an lstm with 3 layers	
setting forget gate biases to 1 in LSTM layer 1	
setting forget gate biases to 1 in LSTM layer 2	
setting forget gate biases to 1 in LSTM layer 3	
number of parameters in the model: 5488219	
cloning rnn	
cloning criterion	
1/2700 (epoch 0.019), train_loss = 4.59415655, grad/param norm = 1.0180e+00, time/batch = 0.5251s	
2/2700 (epoch 0.037), train_loss = 3.63685184, grad/param norm = 1.0193e+00, time/batch = 0.2403s	
3/2700 (epoch 0.056), train_loss = 4.38810417, grad/param norm = 8.8699e-01, time/batch = 0.2110s	
4/2700 (epoch 0.074), train_loss = 3.68647412, grad/param norm = 7.6795e-01, time/batch = 0.1986s	
5/2700 (epoch 0.093), train_loss = 3.76491065, grad/param norm = 1.0246e+00, time/batch = 0.1973s	
6/2700 (epoch 0.111), train_loss = 3.55306460, grad/param norm = 7.8573e-01, time/batch = 0.1966s	
7/2700 (epoch 0.130), train_loss = 3.39466545, grad/param norm = 6.4588e-01, time/batch = 0.1971s	
8/2700 (epoch 0.148), train_loss = 3.39736273, grad/param norm = 6.2166e-01, time/batch = 0.1966s	
9/2700 (epoch 0.167), train_loss = 3.27675179, grad/param norm = 2.2635e-01, time/batch = 0.1970s	
10/2700 (epoch 0.185), train_loss = 3.24965968, grad/param norm = 1.7796e-01, time/batch = 0.1971s	
11/2700 (epoch 0.204), train_loss = 3.18314244, grad/param norm = 1.9695e-01, time/batch = 0.1972s	
12/2700 (epoch 0.222), train_loss = 3.16424140, grad/param norm = 3.0160e-01, time/batch = 0.1962s	
13/2700 (epoch 0.241), train_loss = 3.17516294, grad/param norm = 1.8791e-01, time/batch = 0.1966s	
14/2700 (epoch 0.259), train_loss = 3.20962595, grad/param norm = 1.3766e-01, time/batch = 0.1964s	
15/2700 (epoch 0.278), train_loss = 3.28771706, grad/param norm = 1.9387e-01, time/batch = 0.1970s	
16/2700 (epoch 0.296), train_loss = 3.29038743, grad/param norm = 2.0910e-01, time/batch = 0.1961s	
17/2700 (epoch 0.315), train_loss = 3.26348241, grad/param norm = 1.6727e-01, time/batch = 0.1968s	
18/2700 (epoch 0.333), train_loss = 3.34038277, grad/param norm = 1.5182e-01, time/batch = 0.1961s	
19/2700 (epoch 0.352), train_loss = 3.35094255, grad/param norm = 1.9580e-01, time/batch = 0.1964s	
20/2700 (epoch 0.370), train_loss = 3.30021142, grad/param norm = 2.1805e-01, time/batch = 0.1971s	
21/2700 (epoch 0.389), train_loss = 3.26128806, grad/param norm = 1.9073e-01, time/batch = 0.1973s	
22/2700 (epoch 0.407), train_loss = 3.28044179, grad/param norm = 1.6259e-01, time/batch = 0.1966s	
23/2700 (epoch 0.426), train_loss = 3.28310371, grad/param norm = 1.6444e-01, time/batch = 0.1964s	
24/2700 (epoch 0.444), train_loss = 3.21136866, grad/param norm = 1.9332e-01, time/batch = 0.1969s	
25/2700 (epoch 0.463), train_loss = 3.26079635, grad/param norm = 2.7153e-01, time/batch = 0.1971s	
26/2700 (epoch 0.481), train_loss = 3.34017257, grad/param norm = 2.9584e-01, time/batch = 0.1962s	
27/2700 (epoch 0.500), train_loss = 3.38180449, grad/param norm = 3.1906e-01, time/batch = 0.1971s	
28/2700 (epoch 0.519), train_loss = 3.33733388, grad/param norm = 3.0426e-01, time/batch = 0.1962s	
29/2700 (epoch 0.537), train_loss = 3.33392388, grad/param norm = 2.5940e-01, time/batch = 0.1975s	
30/2700 (epoch 0.556), train_loss = 3.27474011, grad/param norm = 2.1047e-01, time/batch = 0.1966s	
31/2700 (epoch 0.574), train_loss = 3.23396564, grad/param norm = 1.7903e-01, time/batch = 0.1976s	
32/2700 (epoch 0.593), train_loss = 3.24074646, grad/param norm = 2.3152e-01, time/batch = 0.1966s	
33/2700 (epoch 0.611), train_loss = 3.18062982, grad/param norm = 1.7942e-01, time/batch = 0.1966s	
34/2700 (epoch 0.630), train_loss = 3.21974547, grad/param norm = 1.8707e-01, time/batch = 0.1976s	
35/2700 (epoch 0.648), train_loss = 3.29390060, grad/param norm = 1.9479e-01, time/batch = 0.1974s	
36/2700 (epoch 0.667), train_loss = 3.22382727, grad/param norm = 1.6052e-01, time/batch = 0.1971s	
37/2700 (epoch 0.685), train_loss = 3.21783044, grad/param norm = 1.7036e-01, time/batch = 0.1973s	
38/2700 (epoch 0.704), train_loss = 3.19537945, grad/param norm = 2.3140e-01, time/batch = 0.1963s	
39/2700 (epoch 0.722), train_loss = 3.18473309, grad/param norm = 2.0421e-01, time/batch = 0.1966s	
40/2700 (epoch 0.741), train_loss = 3.31674178, grad/param norm = 2.5021e-01, time/batch = 0.1967s	
41/2700 (epoch 0.759), train_loss = 3.26919610, grad/param norm = 2.8306e-01, time/batch = 0.1977s	
42/2700 (epoch 0.778), train_loss = 3.26141341, grad/param norm = 2.5994e-01, time/batch = 0.1968s	
43/2700 (epoch 0.796), train_loss = 3.25288535, grad/param norm = 2.4824e-01, time/batch = 0.1968s	
44/2700 (epoch 0.815), train_loss = 3.20425522, grad/param norm = 2.0484e-01, time/batch = 0.1972s	
45/2700 (epoch 0.833), train_loss = 3.23845943, grad/param norm = 1.7680e-01, time/batch = 0.1971s	
46/2700 (epoch 0.852), train_loss = 3.22539635, grad/param norm = 1.6690e-01, time/batch = 0.1964s	
47/2700 (epoch 0.870), train_loss = 3.22021522, grad/param norm = 1.3590e-01, time/batch = 0.1969s	
48/2700 (epoch 0.889), train_loss = 3.25514427, grad/param norm = 1.4061e-01, time/batch = 0.1971s	
49/2700 (epoch 0.907), train_loss = 3.30840302, grad/param norm = 1.9771e-01, time/batch = 0.1968s	
50/2700 (epoch 0.926), train_loss = 3.26259189, grad/param norm = 2.1569e-01, time/batch = 0.1978s	
51/2700 (epoch 0.944), train_loss = 3.27069993, grad/param norm = 1.8572e-01, time/batch = 0.1977s	
52/2700 (epoch 0.963), train_loss = 3.34943178, grad/param norm = 1.9986e-01, time/batch = 0.1971s	
53/2700 (epoch 0.981), train_loss = 3.40710675, grad/param norm = 1.9848e-01, time/batch = 0.1969s	
54/2700 (epoch 1.000), train_loss = 3.30973918, grad/param norm = 1.9376e-01, time/batch = 0.1968s	
55/2700 (epoch 1.019), train_loss = 3.25224289, grad/param norm = 2.2295e-01, time/batch = 0.1971s	
56/2700 (epoch 1.037), train_loss = 3.26484515, grad/param norm = 2.1336e-01, time/batch = 0.1973s	
57/2700 (epoch 1.056), train_loss = 3.26216802, grad/param norm = 1.4380e-01, time/batch = 0.1966s	
58/2700 (epoch 1.074), train_loss = 3.29322640, grad/param norm = 1.4937e-01, time/batch = 0.1965s	
59/2700 (epoch 1.093), train_loss = 3.30316037, grad/param norm = 1.9692e-01, time/batch = 0.1972s	
60/2700 (epoch 1.111), train_loss = 3.27675804, grad/param norm = 2.0637e-01, time/batch = 0.1971s	
61/2700 (epoch 1.130), train_loss = 3.29467466, grad/param norm = 2.0228e-01, time/batch = 0.1976s	
62/2700 (epoch 1.148), train_loss = 3.25917571, grad/param norm = 2.8750e-01, time/batch = 0.1962s	
63/2700 (epoch 1.167), train_loss = 3.27382638, grad/param norm = 3.0704e-01, time/batch = 0.1962s	
64/2700 (epoch 1.185), train_loss = 3.25582743, grad/param norm = 2.9330e-01, time/batch = 0.1972s	
65/2700 (epoch 1.204), train_loss = 3.18886681, grad/param norm = 3.3167e-01, time/batch = 0.1971s	
66/2700 (epoch 1.222), train_loss = 3.17104388, grad/param norm = 3.7304e-01, time/batch = 0.1962s	
67/2700 (epoch 1.241), train_loss = 3.17784526, grad/param norm = 2.6761e-01, time/batch = 0.1966s	
68/2700 (epoch 1.259), train_loss = 3.21581886, grad/param norm = 2.5907e-01, time/batch = 0.1961s	
69/2700 (epoch 1.278), train_loss = 3.29114141, grad/param norm = 2.9509e-01, time/batch = 0.1971s	
70/2700 (epoch 1.296), train_loss = 3.29041595, grad/param norm = 2.5012e-01, time/batch = 0.1971s	
71/2700 (epoch 1.315), train_loss = 3.26559140, grad/param norm = 2.0131e-01, time/batch = 0.1973s	
72/2700 (epoch 1.333), train_loss = 3.34067770, grad/param norm = 1.6385e-01, time/batch = 0.1966s	
73/2700 (epoch 1.352), train_loss = 3.34643429, grad/param norm = 1.6495e-01, time/batch = 0.1968s	
74/2700 (epoch 1.370), train_loss = 3.29370052, grad/param norm = 1.8808e-01, time/batch = 0.1967s	
75/2700 (epoch 1.389), train_loss = 3.26069334, grad/param norm = 1.9002e-01, time/batch = 0.1969s	
76/2700 (epoch 1.407), train_loss = 3.28125188, grad/param norm = 1.6605e-01, time/batch = 0.1960s	
77/2700 (epoch 1.426), train_loss = 3.28403097, grad/param norm = 1.7022e-01, time/batch = 0.1963s	
78/2700 (epoch 1.444), train_loss = 3.21114880, grad/param norm = 1.8888e-01, time/batch = 0.1962s	
79/2700 (epoch 1.463), train_loss = 3.25636746, grad/param norm = 2.3074e-01, time/batch = 0.1974s	
80/2700 (epoch 1.481), train_loss = 3.33497919, grad/param norm = 2.3960e-01, time/batch = 0.1967s	
81/2700 (epoch 1.500), train_loss = 3.38187187, grad/param norm = 2.8466e-01, time/batch = 0.1976s	
82/2700 (epoch 1.519), train_loss = 3.33705411, grad/param norm = 2.8685e-01, time/batch = 0.1966s	
83/2700 (epoch 1.537), train_loss = 3.33890391, grad/param norm = 2.7753e-01, time/batch = 0.1972s	
84/2700 (epoch 1.556), train_loss = 3.27204916, grad/param norm = 2.0627e-01, time/batch = 0.1973s	
85/2700 (epoch 1.574), train_loss = 3.23379602, grad/param norm = 1.7897e-01, time/batch = 0.1980s	
86/2700 (epoch 1.593), train_loss = 3.24022439, grad/param norm = 2.3000e-01, time/batch = 0.1965s	
87/2700 (epoch 1.611), train_loss = 3.17880733, grad/param norm = 1.7791e-01, time/batch = 0.1969s	
88/2700 (epoch 1.630), train_loss = 3.21768751, grad/param norm = 1.7837e-01, time/batch = 0.1972s	
89/2700 (epoch 1.648), train_loss = 3.29027857, grad/param norm = 1.8260e-01, time/batch = 0.1977s	
90/2700 (epoch 1.667), train_loss = 3.22016567, grad/param norm = 1.4551e-01, time/batch = 0.1969s	
91/2700 (epoch 1.685), train_loss = 3.21504021, grad/param norm = 1.5127e-01, time/batch = 0.1973s	
92/2700 (epoch 1.704), train_loss = 3.19116852, grad/param norm = 2.0607e-01, time/batch = 0.1962s	
93/2700 (epoch 1.722), train_loss = 3.18195722, grad/param norm = 1.7689e-01, time/batch = 0.1966s	
94/2700 (epoch 1.741), train_loss = 3.31514804, grad/param norm = 2.1662e-01, time/batch = 0.1971s	
95/2700 (epoch 1.759), train_loss = 3.26530664, grad/param norm = 2.5223e-01, time/batch = 0.1977s	
96/2700 (epoch 1.778), train_loss = 3.25947051, grad/param norm = 2.4362e-01, time/batch = 0.1967s	
97/2700 (epoch 1.796), train_loss = 3.25464928, grad/param norm = 2.5151e-01, time/batch = 0.1974s	
98/2700 (epoch 1.815), train_loss = 3.20383574, grad/param norm = 2.0894e-01, time/batch = 0.1969s	
99/2700 (epoch 1.833), train_loss = 3.23894096, grad/param norm = 1.7941e-01, time/batch = 0.1966s	
100/2700 (epoch 1.852), train_loss = 3.22481798, grad/param norm = 1.6371e-01, time/batch = 0.1968s	
101/2700 (epoch 1.870), train_loss = 3.21971392, grad/param norm = 1.3574e-01, time/batch = 0.1975s	
102/2700 (epoch 1.889), train_loss = 3.25439404, grad/param norm = 1.3604e-01, time/batch = 0.1967s	
103/2700 (epoch 1.907), train_loss = 3.30729491, grad/param norm = 1.9283e-01, time/batch = 0.1964s	
104/2700 (epoch 1.926), train_loss = 3.26025769, grad/param norm = 2.0046e-01, time/batch = 0.1974s	
105/2700 (epoch 1.944), train_loss = 3.26855673, grad/param norm = 1.7859e-01, time/batch = 0.1970s	
106/2700 (epoch 1.963), train_loss = 3.35005471, grad/param norm = 2.0143e-01, time/batch = 0.1962s	
107/2700 (epoch 1.981), train_loss = 3.40431806, grad/param norm = 1.9014e-01, time/batch = 0.1970s	
108/2700 (epoch 2.000), train_loss = 3.30933340, grad/param norm = 2.1153e-01, time/batch = 0.1964s	
109/2700 (epoch 2.019), train_loss = 3.24937630, grad/param norm = 2.7667e-01, time/batch = 0.1967s	
110/2700 (epoch 2.037), train_loss = 3.27308819, grad/param norm = 3.1432e-01, time/batch = 0.1971s	
111/2700 (epoch 2.056), train_loss = 3.26938946, grad/param norm = 2.8954e-01, time/batch = 0.1981s	
112/2700 (epoch 2.074), train_loss = 3.30271641, grad/param norm = 2.8124e-01, time/batch = 0.1962s	
113/2700 (epoch 2.093), train_loss = 3.30639425, grad/param norm = 2.6733e-01, time/batch = 0.1966s	
114/2700 (epoch 2.111), train_loss = 3.27546570, grad/param norm = 2.4369e-01, time/batch = 0.1969s	
115/2700 (epoch 2.130), train_loss = 3.29232704, grad/param norm = 2.1926e-01, time/batch = 0.1967s	
116/2700 (epoch 2.148), train_loss = 3.25175950, grad/param norm = 2.4540e-01, time/batch = 0.1964s	
117/2700 (epoch 2.167), train_loss = 3.26252998, grad/param norm = 2.3838e-01, time/batch = 0.1973s	
118/2700 (epoch 2.185), train_loss = 3.24217266, grad/param norm = 1.6429e-01, time/batch = 0.1963s	
119/2700 (epoch 2.204), train_loss = 3.17276391, grad/param norm = 1.5589e-01, time/batch = 0.1968s	
120/2700 (epoch 2.222), train_loss = 3.14890013, grad/param norm = 2.1642e-01, time/batch = 0.1967s	
121/2700 (epoch 2.241), train_loss = 3.16629725, grad/param norm = 1.6699e-01, time/batch = 0.1974s	
122/2700 (epoch 2.259), train_loss = 3.22191544, grad/param norm = 3.8610e-01, time/batch = 0.1967s	
123/2700 (epoch 2.278), train_loss = 3.36750913, grad/param norm = 5.9102e-01, time/batch = 0.1970s	
124/2700 (epoch 2.296), train_loss = 3.29469769, grad/param norm = 2.6428e-01, time/batch = 0.1967s	
125/2700 (epoch 2.315), train_loss = 3.26651759, grad/param norm = 2.2140e-01, time/batch = 0.1976s	
126/2700 (epoch 2.333), train_loss = 3.34432077, grad/param norm = 2.0091e-01, time/batch = 0.1962s	
127/2700 (epoch 2.352), train_loss = 3.35211235, grad/param norm = 2.2678e-01, time/batch = 0.1970s	
128/2700 (epoch 2.370), train_loss = 3.29577409, grad/param norm = 2.2179e-01, time/batch = 0.1963s	
129/2700 (epoch 2.389), train_loss = 3.25488605, grad/param norm = 1.6080e-01, time/batch = 0.1965s	
130/2700 (epoch 2.407), train_loss = 3.27321908, grad/param norm = 1.3092e-01, time/batch = 0.1966s	
131/2700 (epoch 2.426), train_loss = 3.27343988, grad/param norm = 1.4275e-01, time/batch = 0.1973s	
132/2700 (epoch 2.444), train_loss = 3.19823541, grad/param norm = 1.3089e-01, time/batch = 0.1963s	
133/2700 (epoch 2.463), train_loss = 3.24679872, grad/param norm = 1.8820e-01, time/batch = 0.1969s	
134/2700 (epoch 2.481), train_loss = 3.35651718, grad/param norm = 4.7363e-01, time/batch = 0.1968s	
135/2700 (epoch 2.500), train_loss = 3.45511351, grad/param norm = 5.6751e-01, time/batch = 0.1971s	
136/2700 (epoch 2.519), train_loss = 3.33598349, grad/param norm = 2.3665e-01, time/batch = 0.1968s	
137/2700 (epoch 2.537), train_loss = 3.32570209, grad/param norm = 1.7490e-01, time/batch = 0.1972s	
138/2700 (epoch 2.556), train_loss = 3.26071681, grad/param norm = 1.3453e-01, time/batch = 0.1967s	
139/2700 (epoch 2.574), train_loss = 3.22544386, grad/param norm = 1.5389e-01, time/batch = 0.1973s	
140/2700 (epoch 2.593), train_loss = 3.22356446, grad/param norm = 2.0790e-01, time/batch = 0.1969s	
141/2700 (epoch 2.611), train_loss = 3.16792512, grad/param norm = 2.1129e-01, time/batch = 0.1975s	
142/2700 (epoch 2.630), train_loss = 3.20712529, grad/param norm = 3.1120e-01, time/batch = 0.1963s	
143/2700 (epoch 2.648), train_loss = 3.29601887, grad/param norm = 3.5499e-01, time/batch = 0.1971s	
144/2700 (epoch 2.667), train_loss = 3.21466204, grad/param norm = 2.8216e-01, time/batch = 0.1966s	
145/2700 (epoch 2.685), train_loss = 3.22612798, grad/param norm = 3.2304e-01, time/batch = 0.1970s	
146/2700 (epoch 2.704), train_loss = 3.19557272, grad/param norm = 3.5854e-01, time/batch = 0.1970s	
147/2700 (epoch 2.722), train_loss = 3.18326578, grad/param norm = 3.4218e-01, time/batch = 0.1970s	
148/2700 (epoch 2.741), train_loss = 3.29001883, grad/param norm = 2.1962e-01, time/batch = 0.1963s	
149/2700 (epoch 2.759), train_loss = 3.22744516, grad/param norm = 1.7330e-01, time/batch = 0.1972s	
150/2700 (epoch 2.778), train_loss = 3.22450061, grad/param norm = 2.2059e-01, time/batch = 0.1974s	
151/2700 (epoch 2.796), train_loss = 3.21631422, grad/param norm = 2.5531e-01, time/batch = 0.1975s	
152/2700 (epoch 2.815), train_loss = 3.15985171, grad/param norm = 2.7325e-01, time/batch = 0.1969s	
153/2700 (epoch 2.833), train_loss = 3.21199542, grad/param norm = 3.5485e-01, time/batch = 0.1965s	
154/2700 (epoch 2.852), train_loss = 3.40201462, grad/param norm = 6.5238e-01, time/batch = 0.1972s	
155/2700 (epoch 2.870), train_loss = 3.33826190, grad/param norm = 5.5409e-01, time/batch = 0.1971s	
156/2700 (epoch 2.889), train_loss = 3.22916105, grad/param norm = 2.0405e-01, time/batch = 0.1964s	
157/2700 (epoch 2.907), train_loss = 3.25732328, grad/param norm = 1.7569e-01, time/batch = 0.1967s	
158/2700 (epoch 2.926), train_loss = 3.18295252, grad/param norm = 1.4956e-01, time/batch = 0.1962s	
159/2700 (epoch 2.944), train_loss = 3.18081230, grad/param norm = 1.5511e-01, time/batch = 0.1966s	
160/2700 (epoch 2.963), train_loss = 3.24606828, grad/param norm = 1.7828e-01, time/batch = 0.1971s	
161/2700 (epoch 2.981), train_loss = 3.41120117, grad/param norm = 5.1808e-01, time/batch = 0.1976s	
162/2700 (epoch 3.000), train_loss = 3.33095889, grad/param norm = 4.5655e-01, time/batch = 0.1965s	
163/2700 (epoch 3.019), train_loss = 3.13944381, grad/param norm = 1.7653e-01, time/batch = 0.1969s	
164/2700 (epoch 3.037), train_loss = 3.14568291, grad/param norm = 1.5607e-01, time/batch = 0.1971s	
165/2700 (epoch 3.056), train_loss = 3.13348633, grad/param norm = 1.9152e-01, time/batch = 0.1976s	
166/2700 (epoch 3.074), train_loss = 3.20061389, grad/param norm = 3.1370e-01, time/batch = 0.1969s	
167/2700 (epoch 3.093), train_loss = 3.22565962, grad/param norm = 4.1169e-01, time/batch = 0.1968s	
168/2700 (epoch 3.111), train_loss = 3.16545779, grad/param norm = 2.6442e-01, time/batch = 0.1966s	
169/2700 (epoch 3.130), train_loss = 3.14085694, grad/param norm = 1.4170e-01, time/batch = 0.1968s	
170/2700 (epoch 3.148), train_loss = 3.06760687, grad/param norm = 1.9155e-01, time/batch = 0.1967s	
171/2700 (epoch 3.167), train_loss = 3.09809793, grad/param norm = 4.1204e-01, time/batch = 0.1979s	
172/2700 (epoch 3.185), train_loss = 3.17753770, grad/param norm = 5.1227e-01, time/batch = 0.1969s	
173/2700 (epoch 3.204), train_loss = 3.07961645, grad/param norm = 2.9972e-01, time/batch = 0.1969s	
174/2700 (epoch 3.222), train_loss = 2.98583359, grad/param norm = 2.0868e-01, time/batch = 0.1975s	
175/2700 (epoch 3.241), train_loss = 2.97963234, grad/param norm = 1.6084e-01, time/batch = 0.1973s	
176/2700 (epoch 3.259), train_loss = 2.98170048, grad/param norm = 1.9301e-01, time/batch = 0.1969s	
177/2700 (epoch 3.278), train_loss = 3.06410436, grad/param norm = 2.3649e-01, time/batch = 0.1975s	
178/2700 (epoch 3.296), train_loss = 3.05668453, grad/param norm = 2.6292e-01, time/batch = 0.1973s	
179/2700 (epoch 3.315), train_loss = 3.09499902, grad/param norm = 3.9646e-01, time/batch = 0.1969s	
180/2700 (epoch 3.333), train_loss = 3.16583281, grad/param norm = 3.6939e-01, time/batch = 0.1975s	
181/2700 (epoch 3.352), train_loss = 3.14311635, grad/param norm = 1.6352e-01, time/batch = 0.1981s	
182/2700 (epoch 3.370), train_loss = 3.11129452, grad/param norm = 2.1508e-01, time/batch = 0.1973s	
183/2700 (epoch 3.389), train_loss = 3.04944656, grad/param norm = 2.4263e-01, time/batch = 0.1965s	
184/2700 (epoch 3.407), train_loss = 3.03592949, grad/param norm = 1.9297e-01, time/batch = 0.1973s	
185/2700 (epoch 3.426), train_loss = 3.08125036, grad/param norm = 1.9581e-01, time/batch = 0.1972s	
186/2700 (epoch 3.444), train_loss = 3.04222268, grad/param norm = 4.2486e-01, time/batch = 0.1964s	
187/2700 (epoch 3.463), train_loss = 3.12283813, grad/param norm = 5.0671e-01, time/batch = 0.1975s	
188/2700 (epoch 3.481), train_loss = 3.14369482, grad/param norm = 3.6849e-01, time/batch = 0.1973s	
189/2700 (epoch 3.500), train_loss = 3.12674908, grad/param norm = 1.7065e-01, time/batch = 0.1976s	
190/2700 (epoch 3.519), train_loss = 3.03003629, grad/param norm = 1.2339e-01, time/batch = 0.1969s	
191/2700 (epoch 3.537), train_loss = 3.02930515, grad/param norm = 1.1372e-01, time/batch = 0.1976s	
192/2700 (epoch 3.556), train_loss = 2.98097559, grad/param norm = 1.3303e-01, time/batch = 0.1965s	
193/2700 (epoch 3.574), train_loss = 3.17080472, grad/param norm = 6.0528e-01, time/batch = 0.1966s	
194/2700 (epoch 3.593), train_loss = 3.13893062, grad/param norm = 5.3674e-01, time/batch = 0.1967s	
195/2700 (epoch 3.611), train_loss = 2.90435147, grad/param norm = 2.3891e-01, time/batch = 0.1971s	
196/2700 (epoch 3.630), train_loss = 2.90425966, grad/param norm = 1.8496e-01, time/batch = 0.1963s	
197/2700 (epoch 3.648), train_loss = 3.02017593, grad/param norm = 2.5638e-01, time/batch = 0.1972s	
198/2700 (epoch 3.667), train_loss = 2.90732112, grad/param norm = 2.0404e-01, time/batch = 0.1967s	
199/2700 (epoch 3.685), train_loss = 2.91653697, grad/param norm = 2.2048e-01, time/batch = 0.1966s	
200/2700 (epoch 3.704), train_loss = 2.92591611, grad/param norm = 3.2788e-01, time/batch = 0.1969s	
201/2700 (epoch 3.722), train_loss = 2.94088275, grad/param norm = 3.4142e-01, time/batch = 0.1974s	
202/2700 (epoch 3.741), train_loss = 3.04038767, grad/param norm = 2.3504e-01, time/batch = 0.1968s	
203/2700 (epoch 3.759), train_loss = 2.95001191, grad/param norm = 1.3357e-01, time/batch = 0.1965s	
204/2700 (epoch 3.778), train_loss = 2.90324366, grad/param norm = 1.3312e-01, time/batch = 0.1970s	
205/2700 (epoch 3.796), train_loss = 2.91743557, grad/param norm = 1.7894e-01, time/batch = 0.1969s	
206/2700 (epoch 3.815), train_loss = 2.90699066, grad/param norm = 2.6440e-01, time/batch = 0.1964s	
207/2700 (epoch 3.833), train_loss = 2.92404646, grad/param norm = 2.4309e-01, time/batch = 0.1969s	
208/2700 (epoch 3.852), train_loss = 2.91535904, grad/param norm = 1.9491e-01, time/batch = 0.1965s	
209/2700 (epoch 3.870), train_loss = 2.87521679, grad/param norm = 2.2119e-01, time/batch = 0.1971s	
210/2700 (epoch 3.889), train_loss = 2.95013638, grad/param norm = 2.9832e-01, time/batch = 0.1970s	
211/2700 (epoch 3.907), train_loss = 3.04469169, grad/param norm = 4.0411e-01, time/batch = 0.1982s	
212/2700 (epoch 3.926), train_loss = 2.94091630, grad/param norm = 2.6748e-01, time/batch = 0.1963s	
213/2700 (epoch 3.944), train_loss = 2.91612835, grad/param norm = 1.2545e-01, time/batch = 0.1966s	
214/2700 (epoch 3.963), train_loss = 2.97029824, grad/param norm = 1.1609e-01, time/batch = 0.1972s	
215/2700 (epoch 3.981), train_loss = 2.97686219, grad/param norm = 1.1717e-01, time/batch = 0.1977s	
216/2700 (epoch 4.000), train_loss = 3.29388345, grad/param norm = 1.5627e+00, time/batch = 0.1970s	
217/2700 (epoch 4.019), train_loss = 3.12914505, grad/param norm = 4.3508e-01, time/batch = 0.1975s	
218/2700 (epoch 4.037), train_loss = 3.10379985, grad/param norm = 4.8404e-01, time/batch = 0.1967s	
219/2700 (epoch 4.056), train_loss = 3.08602993, grad/param norm = 4.8090e-01, time/batch = 0.1975s	
220/2700 (epoch 4.074), train_loss = 3.01863248, grad/param norm = 2.5711e-01, time/batch = 0.1974s	
221/2700 (epoch 4.093), train_loss = 2.94835349, grad/param norm = 1.1715e-01, time/batch = 0.1977s	
222/2700 (epoch 4.111), train_loss = 2.88773269, grad/param norm = 9.8762e-02, time/batch = 0.1967s	
223/2700 (epoch 4.130), train_loss = 2.91463697, grad/param norm = 9.2119e-02, time/batch = 0.1967s	
224/2700 (epoch 4.148), train_loss = 2.82337377, grad/param norm = 1.2222e-01, time/batch = 0.1966s	
225/2700 (epoch 4.167), train_loss = 2.88880904, grad/param norm = 2.5091e-01, time/batch = 0.1976s	
226/2700 (epoch 4.185), train_loss = 2.91435004, grad/param norm = 3.4969e-01, time/batch = 0.1972s	
227/2700 (epoch 4.204), train_loss = 2.82580450, grad/param norm = 2.9904e-01, time/batch = 0.1968s	
228/2700 (epoch 4.222), train_loss = 2.73514453, grad/param norm = 1.9323e-01, time/batch = 0.1965s	
229/2700 (epoch 4.241), train_loss = 2.78341985, grad/param norm = 1.7259e-01, time/batch = 0.1966s	
230/2700 (epoch 4.259), train_loss = 2.77295690, grad/param norm = 1.8148e-01, time/batch = 0.1971s	
231/2700 (epoch 4.278), train_loss = 2.88158142, grad/param norm = 1.9913e-01, time/batch = 0.1976s	
232/2700 (epoch 4.296), train_loss = 2.84100478, grad/param norm = 1.6393e-01, time/batch = 0.1970s	
233/2700 (epoch 4.315), train_loss = 2.84489477, grad/param norm = 1.5519e-01, time/batch = 0.1974s	
234/2700 (epoch 4.333), train_loss = 2.87090303, grad/param norm = 1.6508e-01, time/batch = 0.1977s	
235/2700 (epoch 4.352), train_loss = 2.94331150, grad/param norm = 2.0387e-01, time/batch = 0.1976s	
236/2700 (epoch 4.370), train_loss = 2.85362800, grad/param norm = 2.3847e-01, time/batch = 0.1966s	
237/2700 (epoch 4.389), train_loss = 2.80052756, grad/param norm = 2.1408e-01, time/batch = 0.1968s	
238/2700 (epoch 4.407), train_loss = 2.79606198, grad/param norm = 1.7698e-01, time/batch = 0.1963s	
239/2700 (epoch 4.426), train_loss = 2.83097594, grad/param norm = 1.3116e-01, time/batch = 0.1967s	
240/2700 (epoch 4.444), train_loss = 2.70042746, grad/param norm = 1.3655e-01, time/batch = 0.1968s	
241/2700 (epoch 4.463), train_loss = 2.77320828, grad/param norm = 1.8407e-01, time/batch = 0.1972s	
242/2700 (epoch 4.481), train_loss = 2.86693289, grad/param norm = 2.2790e-01, time/batch = 0.1970s	
243/2700 (epoch 4.500), train_loss = 2.95154386, grad/param norm = 3.5157e-01, time/batch = 0.1963s	
244/2700 (epoch 4.519), train_loss = 2.96866229, grad/param norm = 3.4518e-01, time/batch = 0.1965s	
245/2700 (epoch 4.537), train_loss = 2.91044255, grad/param norm = 2.4432e-01, time/batch = 0.1974s	
246/2700 (epoch 4.556), train_loss = 2.89320496, grad/param norm = 2.3680e-01, time/batch = 0.1969s	
247/2700 (epoch 4.574), train_loss = 2.82092905, grad/param norm = 2.7017e-01, time/batch = 0.1972s	
248/2700 (epoch 4.593), train_loss = 2.82238283, grad/param norm = 2.8558e-01, time/batch = 0.1970s	
249/2700 (epoch 4.611), train_loss = 2.67597721, grad/param norm = 1.8186e-01, time/batch = 0.1972s	
250/2700 (epoch 4.630), train_loss = 2.68957915, grad/param norm = 1.1336e-01, time/batch = 0.1971s	
251/2700 (epoch 4.648), train_loss = 2.73353591, grad/param norm = 1.1792e-01, time/batch = 0.1976s	
252/2700 (epoch 4.667), train_loss = 2.68655125, grad/param norm = 2.6273e-01, time/batch = 0.1967s	
253/2700 (epoch 4.685), train_loss = 2.81095422, grad/param norm = 2.8727e-01, time/batch = 0.1964s	
254/2700 (epoch 4.704), train_loss = 2.74455686, grad/param norm = 2.2359e-01, time/batch = 0.1970s	
255/2700 (epoch 4.722), train_loss = 2.66005681, grad/param norm = 1.1504e-01, time/batch = 0.1971s	
256/2700 (epoch 4.741), train_loss = 2.83120321, grad/param norm = 1.6732e-01, time/batch = 0.1965s	
257/2700 (epoch 4.759), train_loss = 2.82430184, grad/param norm = 2.8332e-01, time/batch = 0.1967s	
258/2700 (epoch 4.778), train_loss = 2.80877823, grad/param norm = 2.8691e-01, time/batch = 0.1964s	
259/2700 (epoch 4.796), train_loss = 2.74129323, grad/param norm = 2.3348e-01, time/batch = 0.1974s	
260/2700 (epoch 4.815), train_loss = 2.68267067, grad/param norm = 1.4952e-01, time/batch = 0.1971s	
261/2700 (epoch 4.833), train_loss = 2.67729040, grad/param norm = 1.0628e-01, time/batch = 0.1976s	
262/2700 (epoch 4.852), train_loss = 2.67169489, grad/param norm = 1.1030e-01, time/batch = 0.1969s	
263/2700 (epoch 4.870), train_loss = 2.64735696, grad/param norm = 1.3076e-01, time/batch = 0.1973s	
264/2700 (epoch 4.889), train_loss = 2.72871244, grad/param norm = 3.2384e-01, time/batch = 0.1970s	
265/2700 (epoch 4.907), train_loss = 2.93934483, grad/param norm = 3.2334e-01, time/batch = 0.1977s	
266/2700 (epoch 4.926), train_loss = 2.97174151, grad/param norm = 4.3894e-01, time/batch = 0.1970s	
267/2700 (epoch 4.944), train_loss = 2.97142632, grad/param norm = 3.8926e-01, time/batch = 0.1972s	
268/2700 (epoch 4.963), train_loss = 2.92859239, grad/param norm = 3.4689e-01, time/batch = 0.1967s	
269/2700 (epoch 4.981), train_loss = 2.88089243, grad/param norm = 2.0268e-01, time/batch = 0.1968s	
270/2700 (epoch 5.000), train_loss = 2.78683105, grad/param norm = 1.1378e-01, time/batch = 0.1969s	
271/2700 (epoch 5.019), train_loss = 2.69856362, grad/param norm = 1.3752e-01, time/batch = 0.1987s	
272/2700 (epoch 5.037), train_loss = 2.72672264, grad/param norm = 1.1372e-01, time/batch = 0.1971s	
273/2700 (epoch 5.056), train_loss = 2.67481159, grad/param norm = 1.1748e-01, time/batch = 0.1975s	
274/2700 (epoch 5.074), train_loss = 2.70024756, grad/param norm = 1.6186e-01, time/batch = 0.1979s	
275/2700 (epoch 5.093), train_loss = 2.75560480, grad/param norm = 1.9453e-01, time/batch = 0.1979s	
276/2700 (epoch 5.111), train_loss = 2.67880459, grad/param norm = 1.2256e-01, time/batch = 0.1970s	
277/2700 (epoch 5.130), train_loss = 2.67543207, grad/param norm = 1.2351e-01, time/batch = 0.1968s	
278/2700 (epoch 5.148), train_loss = 2.62091486, grad/param norm = 1.3920e-01, time/batch = 0.1973s	
279/2700 (epoch 5.167), train_loss = 2.65104270, grad/param norm = 1.4333e-01, time/batch = 0.1969s	
280/2700 (epoch 5.185), train_loss = 2.61019619, grad/param norm = 1.5156e-01, time/batch = 0.1968s	
281/2700 (epoch 5.204), train_loss = 2.59980623, grad/param norm = 2.2698e-01, time/batch = 0.1973s	
282/2700 (epoch 5.222), train_loss = 2.60579364, grad/param norm = 3.2734e-01, time/batch = 0.1964s	
283/2700 (epoch 5.241), train_loss = 2.57078415, grad/param norm = 2.9428e-01, time/batch = 0.1967s	
284/2700 (epoch 5.259), train_loss = 2.61834821, grad/param norm = 2.7340e-01, time/batch = 0.1967s	
285/2700 (epoch 5.278), train_loss = 2.73630453, grad/param norm = 3.1235e-01, time/batch = 0.1972s	
286/2700 (epoch 5.296), train_loss = 2.67811136, grad/param norm = 2.3754e-01, time/batch = 0.1967s	
287/2700 (epoch 5.315), train_loss = 2.74851707, grad/param norm = 1.6778e-01, time/batch = 0.1969s	
288/2700 (epoch 5.333), train_loss = 2.67978521, grad/param norm = 1.0062e-01, time/batch = 0.1966s	
289/2700 (epoch 5.352), train_loss = 2.69182310, grad/param norm = 9.3130e-02, time/batch = 0.1966s	
290/2700 (epoch 5.370), train_loss = 2.61732916, grad/param norm = 1.3398e-01, time/batch = 0.1974s	
291/2700 (epoch 5.389), train_loss = 2.63319425, grad/param norm = 1.9785e-01, time/batch = 0.1987s	
292/2700 (epoch 5.407), train_loss = 2.63108866, grad/param norm = 1.9287e-01, time/batch = 0.1974s	
293/2700 (epoch 5.426), train_loss = 2.65987033, grad/param norm = 1.8247e-01, time/batch = 0.1969s	
294/2700 (epoch 5.444), train_loss = 2.56982838, grad/param norm = 2.6418e-01, time/batch = 0.1971s	
295/2700 (epoch 5.463), train_loss = 2.76382116, grad/param norm = 3.3044e-01, time/batch = 0.1974s	
296/2700 (epoch 5.481), train_loss = 2.86387747, grad/param norm = 3.1133e-01, time/batch = 0.1974s	
297/2700 (epoch 5.500), train_loss = 2.75195217, grad/param norm = 2.2128e-01, time/batch = 0.1972s	
298/2700 (epoch 5.519), train_loss = 2.65956201, grad/param norm = 1.4214e-01, time/batch = 0.1968s	
299/2700 (epoch 5.537), train_loss = 2.66271150, grad/param norm = 2.9292e-01, time/batch = 0.1966s	
300/2700 (epoch 5.556), train_loss = 2.71347597, grad/param norm = 1.7412e-01, time/batch = 0.1972s	
301/2700 (epoch 5.574), train_loss = 2.55221120, grad/param norm = 1.3358e-01, time/batch = 0.1975s	
302/2700 (epoch 5.593), train_loss = 2.52042880, grad/param norm = 1.2743e-01, time/batch = 0.1964s	
303/2700 (epoch 5.611), train_loss = 2.47474172, grad/param norm = 1.8143e-01, time/batch = 0.1978s	
304/2700 (epoch 5.630), train_loss = 2.64083102, grad/param norm = 3.6475e-01, time/batch = 0.1971s	
305/2700 (epoch 5.648), train_loss = 2.71265595, grad/param norm = 3.8273e-01, time/batch = 0.1973s	
306/2700 (epoch 5.667), train_loss = 2.55594266, grad/param norm = 2.6234e-01, time/batch = 0.1967s	
307/2700 (epoch 5.685), train_loss = 2.53992958, grad/param norm = 1.4132e-01, time/batch = 0.1977s	
308/2700 (epoch 5.704), train_loss = 2.51759119, grad/param norm = 1.4911e-01, time/batch = 0.1967s	
309/2700 (epoch 5.722), train_loss = 2.48680721, grad/param norm = 1.0842e-01, time/batch = 0.1982s	
310/2700 (epoch 5.741), train_loss = 2.63045562, grad/param norm = 1.0359e-01, time/batch = 0.1974s	
311/2700 (epoch 5.759), train_loss = 2.59434270, grad/param norm = 1.6681e-01, time/batch = 0.1979s	
312/2700 (epoch 5.778), train_loss = 2.63811103, grad/param norm = 2.1546e-01, time/batch = 0.1970s	
313/2700 (epoch 5.796), train_loss = 2.57928414, grad/param norm = 2.2656e-01, time/batch = 0.1966s	
314/2700 (epoch 5.815), train_loss = 2.52773870, grad/param norm = 1.4917e-01, time/batch = 0.1974s	
315/2700 (epoch 5.833), train_loss = 2.50111254, grad/param norm = 1.2577e-01, time/batch = 0.1972s	
316/2700 (epoch 5.852), train_loss = 2.51436663, grad/param norm = 1.1464e-01, time/batch = 0.1976s	
317/2700 (epoch 5.870), train_loss = 2.48281998, grad/param norm = 1.1861e-01, time/batch = 0.1977s	
318/2700 (epoch 5.889), train_loss = 2.51219639, grad/param norm = 1.4129e-01, time/batch = 0.1965s	
319/2700 (epoch 5.907), train_loss = 2.64351414, grad/param norm = 1.9909e-01, time/batch = 0.1973s	
320/2700 (epoch 5.926), train_loss = 2.59249179, grad/param norm = 2.5178e-01, time/batch = 0.1975s	
321/2700 (epoch 5.944), train_loss = 2.60799666, grad/param norm = 2.4044e-01, time/batch = 0.1977s	
322/2700 (epoch 5.963), train_loss = 2.62434629, grad/param norm = 2.3852e-01, time/batch = 0.1971s	
323/2700 (epoch 5.981), train_loss = 2.58949862, grad/param norm = 2.1480e-01, time/batch = 0.1965s	
324/2700 (epoch 6.000), train_loss = 2.59015228, grad/param norm = 1.5133e-01, time/batch = 0.1968s	
325/2700 (epoch 6.019), train_loss = 2.52222732, grad/param norm = 1.3161e-01, time/batch = 0.1972s	
326/2700 (epoch 6.037), train_loss = 2.55805449, grad/param norm = 1.6278e-01, time/batch = 0.1966s	
327/2700 (epoch 6.056), train_loss = 2.50959246, grad/param norm = 1.5551e-01, time/batch = 0.1969s	
328/2700 (epoch 6.074), train_loss = 2.49883117, grad/param norm = 1.5669e-01, time/batch = 0.1970s	
329/2700 (epoch 6.093), train_loss = 2.51375669, grad/param norm = 1.7277e-01, time/batch = 0.1975s	
330/2700 (epoch 6.111), train_loss = 2.50372098, grad/param norm = 2.2227e-01, time/batch = 0.1974s	
331/2700 (epoch 6.130), train_loss = 2.57942937, grad/param norm = 3.1327e-01, time/batch = 0.1984s	
332/2700 (epoch 6.148), train_loss = 2.51675624, grad/param norm = 2.5600e-01, time/batch = 0.1968s	
333/2700 (epoch 6.167), train_loss = 2.49677782, grad/param norm = 1.4919e-01, time/batch = 0.1975s	
334/2700 (epoch 6.185), train_loss = 2.43074846, grad/param norm = 1.3931e-01, time/batch = 0.1970s	
335/2700 (epoch 6.204), train_loss = 2.43256175, grad/param norm = 1.1985e-01, time/batch = 0.1981s	
336/2700 (epoch 6.222), train_loss = 2.35924619, grad/param norm = 1.2051e-01, time/batch = 0.1965s	
337/2700 (epoch 6.241), train_loss = 2.33155683, grad/param norm = 1.3278e-01, time/batch = 0.1971s	
338/2700 (epoch 6.259), train_loss = 2.48032790, grad/param norm = 3.3232e-01, time/batch = 0.1966s	
339/2700 (epoch 6.278), train_loss = 2.68307675, grad/param norm = 2.6638e-01, time/batch = 0.1970s	
340/2700 (epoch 6.296), train_loss = 2.54195976, grad/param norm = 2.6752e-01, time/batch = 0.1970s	
341/2700 (epoch 6.315), train_loss = 2.62932931, grad/param norm = 2.2292e-01, time/batch = 0.1980s	
342/2700 (epoch 6.333), train_loss = 2.49418026, grad/param norm = 1.4144e-01, time/batch = 0.1966s	
343/2700 (epoch 6.352), train_loss = 2.47683254, grad/param norm = 1.1438e-01, time/batch = 0.1965s	
344/2700 (epoch 6.370), train_loss = 2.44741992, grad/param norm = 1.1657e-01, time/batch = 0.1971s	
345/2700 (epoch 6.389), train_loss = 2.42409556, grad/param norm = 1.6958e-01, time/batch = 0.1976s	
346/2700 (epoch 6.407), train_loss = 2.45000155, grad/param norm = 2.0691e-01, time/batch = 0.1970s	
347/2700 (epoch 6.426), train_loss = 2.49174270, grad/param norm = 2.2191e-01, time/batch = 0.1972s	
348/2700 (epoch 6.444), train_loss = 2.36789394, grad/param norm = 1.9774e-01, time/batch = 0.1972s	
349/2700 (epoch 6.463), train_loss = 2.43818519, grad/param norm = 1.8759e-01, time/batch = 0.1972s	
350/2700 (epoch 6.481), train_loss = 2.47862551, grad/param norm = 1.7465e-01, time/batch = 0.1970s	
351/2700 (epoch 6.500), train_loss = 2.55901828, grad/param norm = 2.0279e-01, time/batch = 0.1980s	
352/2700 (epoch 6.519), train_loss = 2.54837765, grad/param norm = 4.4782e-01, time/batch = 0.1970s	
353/2700 (epoch 6.537), train_loss = 2.56952515, grad/param norm = 2.0333e-01, time/batch = 0.1968s	
354/2700 (epoch 6.556), train_loss = 2.49336772, grad/param norm = 2.4836e-01, time/batch = 0.1969s	
355/2700 (epoch 6.574), train_loss = 2.41713526, grad/param norm = 2.5214e-01, time/batch = 0.1978s	
356/2700 (epoch 6.593), train_loss = 2.39796747, grad/param norm = 2.2704e-01, time/batch = 0.1971s	
357/2700 (epoch 6.611), train_loss = 2.33438784, grad/param norm = 1.9934e-01, time/batch = 0.1970s	
358/2700 (epoch 6.630), train_loss = 2.37262153, grad/param norm = 1.5511e-01, time/batch = 0.1966s	
359/2700 (epoch 6.648), train_loss = 2.38300969, grad/param norm = 1.3977e-01, time/batch = 0.1970s	
360/2700 (epoch 6.667), train_loss = 2.30152508, grad/param norm = 1.0823e-01, time/batch = 0.1969s	
361/2700 (epoch 6.685), train_loss = 2.34498847, grad/param norm = 1.2744e-01, time/batch = 0.1981s	
362/2700 (epoch 6.704), train_loss = 2.36200494, grad/param norm = 1.8056e-01, time/batch = 0.1968s	
363/2700 (epoch 6.722), train_loss = 2.35954282, grad/param norm = 1.9100e-01, time/batch = 0.1972s	
364/2700 (epoch 6.741), train_loss = 2.54151683, grad/param norm = 2.0659e-01, time/batch = 0.1975s	
365/2700 (epoch 6.759), train_loss = 2.47701977, grad/param norm = 2.0122e-01, time/batch = 0.1976s	
366/2700 (epoch 6.778), train_loss = 2.46301737, grad/param norm = 1.9763e-01, time/batch = 0.1970s	
367/2700 (epoch 6.796), train_loss = 2.35176047, grad/param norm = 1.6005e-01, time/batch = 0.1974s	
368/2700 (epoch 6.815), train_loss = 2.34630742, grad/param norm = 1.0990e-01, time/batch = 0.1971s	
369/2700 (epoch 6.833), train_loss = 2.31651832, grad/param norm = 1.1627e-01, time/batch = 0.1966s	
370/2700 (epoch 6.852), train_loss = 2.33464988, grad/param norm = 1.2154e-01, time/batch = 0.1975s	
371/2700 (epoch 6.870), train_loss = 2.31083496, grad/param norm = 1.5343e-01, time/batch = 0.1981s	
372/2700 (epoch 6.889), train_loss = 2.34036963, grad/param norm = 2.0597e-01, time/batch = 0.1967s	
373/2700 (epoch 6.907), train_loss = 2.48135965, grad/param norm = 2.2912e-01, time/batch = 0.1969s	
374/2700 (epoch 6.926), train_loss = 2.37520489, grad/param norm = 1.8800e-01, time/batch = 0.1970s	
375/2700 (epoch 6.944), train_loss = 2.34805003, grad/param norm = 1.5386e-01, time/batch = 0.1975s	
376/2700 (epoch 6.963), train_loss = 2.36031681, grad/param norm = 1.2968e-01, time/batch = 0.1969s	
377/2700 (epoch 6.981), train_loss = 2.33263002, grad/param norm = 1.1833e-01, time/batch = 0.1970s	
378/2700 (epoch 7.000), train_loss = 2.36581104, grad/param norm = 1.3479e-01, time/batch = 0.1969s	
379/2700 (epoch 7.019), train_loss = 2.37285811, grad/param norm = 1.6410e-01, time/batch = 0.1968s	
380/2700 (epoch 7.037), train_loss = 2.38932044, grad/param norm = 1.7774e-01, time/batch = 0.1975s	
381/2700 (epoch 7.056), train_loss = 2.33642090, grad/param norm = 1.4787e-01, time/batch = 0.1977s	
382/2700 (epoch 7.074), train_loss = 2.28947682, grad/param norm = 1.4325e-01, time/batch = 0.1967s	
383/2700 (epoch 7.093), train_loss = 2.32315997, grad/param norm = 1.4814e-01, time/batch = 0.1971s	
384/2700 (epoch 7.111), train_loss = 2.32016605, grad/param norm = 1.5948e-01, time/batch = 0.1968s	
385/2700 (epoch 7.130), train_loss = 2.33300361, grad/param norm = 1.7581e-01, time/batch = 0.1979s	
386/2700 (epoch 7.148), train_loss = 2.32687266, grad/param norm = 2.5706e-01, time/batch = 0.1965s	
387/2700 (epoch 7.167), train_loss = 2.47414941, grad/param norm = 2.9163e-01, time/batch = 0.1975s	
388/2700 (epoch 7.185), train_loss = 2.33134454, grad/param norm = 2.2310e-01, time/batch = 0.1975s	
389/2700 (epoch 7.204), train_loss = 2.25018387, grad/param norm = 1.1174e-01, time/batch = 0.1976s	
390/2700 (epoch 7.222), train_loss = 2.17568392, grad/param norm = 8.8392e-02, time/batch = 0.1972s	
391/2700 (epoch 7.241), train_loss = 2.13392848, grad/param norm = 7.6513e-02, time/batch = 0.1977s	
392/2700 (epoch 7.259), train_loss = 2.16324760, grad/param norm = 1.0143e-01, time/batch = 0.1977s	
393/2700 (epoch 7.278), train_loss = 2.27601863, grad/param norm = 1.5062e-01, time/batch = 0.1965s	
394/2700 (epoch 7.296), train_loss = 2.28365972, grad/param norm = 1.8698e-01, time/batch = 0.1969s	
395/2700 (epoch 7.315), train_loss = 2.30893235, grad/param norm = 1.6216e-01, time/batch = 0.1980s	
396/2700 (epoch 7.333), train_loss = 2.26830040, grad/param norm = 1.2752e-01, time/batch = 0.1967s	
397/2700 (epoch 7.352), train_loss = 2.27442261, grad/param norm = 1.2537e-01, time/batch = 0.1975s	
398/2700 (epoch 7.370), train_loss = 2.27610301, grad/param norm = 1.2177e-01, time/batch = 0.1964s	
399/2700 (epoch 7.389), train_loss = 2.22377612, grad/param norm = 9.9200e-02, time/batch = 0.1973s	
400/2700 (epoch 7.407), train_loss = 2.21807041, grad/param norm = 9.5208e-02, time/batch = 0.1975s	
401/2700 (epoch 7.426), train_loss = 2.27878998, grad/param norm = 1.2381e-01, time/batch = 0.1981s	
402/2700 (epoch 7.444), train_loss = 2.19610116, grad/param norm = 1.5665e-01, time/batch = 0.1972s	
403/2700 (epoch 7.463), train_loss = 2.27230659, grad/param norm = 1.5450e-01, time/batch = 0.1966s	
404/2700 (epoch 7.481), train_loss = 2.27835267, grad/param norm = 1.5375e-01, time/batch = 0.1969s	
405/2700 (epoch 7.500), train_loss = 2.28164372, grad/param norm = 1.8242e-01, time/batch = 0.1978s	
406/2700 (epoch 7.519), train_loss = 2.25403794, grad/param norm = 1.9456e-01, time/batch = 0.1970s	
407/2700 (epoch 7.537), train_loss = 2.26150690, grad/param norm = 1.5679e-01, time/batch = 0.1968s	
408/2700 (epoch 7.556), train_loss = 2.22123594, grad/param norm = 1.3021e-01, time/batch = 0.1967s	
409/2700 (epoch 7.574), train_loss = 2.18572598, grad/param norm = 1.2281e-01, time/batch = 0.1976s	
410/2700 (epoch 7.593), train_loss = 2.17743703, grad/param norm = 1.3013e-01, time/batch = 0.1974s	
411/2700 (epoch 7.611), train_loss = 2.10334823, grad/param norm = 1.1640e-01, time/batch = 0.1980s	
412/2700 (epoch 7.630), train_loss = 2.14317835, grad/param norm = 1.2786e-01, time/batch = 0.1964s	
413/2700 (epoch 7.648), train_loss = 2.20408905, grad/param norm = 1.8043e-01, time/batch = 0.1969s	
414/2700 (epoch 7.667), train_loss = 2.24214862, grad/param norm = 2.8327e-01, time/batch = 0.1975s	
415/2700 (epoch 7.685), train_loss = 2.29534294, grad/param norm = 2.7568e-01, time/batch = 0.1979s	
416/2700 (epoch 7.704), train_loss = 2.25976725, grad/param norm = 2.3124e-01, time/batch = 0.1964s	
417/2700 (epoch 7.722), train_loss = 2.19166053, grad/param norm = 1.8567e-01, time/batch = 0.1971s	
418/2700 (epoch 7.741), train_loss = 2.22604342, grad/param norm = 1.1923e-01, time/batch = 0.1966s	
419/2700 (epoch 7.759), train_loss = 2.24060766, grad/param norm = 1.1004e-01, time/batch = 0.1967s	
420/2700 (epoch 7.778), train_loss = 2.21979368, grad/param norm = 1.1647e-01, time/batch = 0.1970s	
421/2700 (epoch 7.796), train_loss = 2.14882390, grad/param norm = 1.1099e-01, time/batch = 0.1980s	
422/2700 (epoch 7.815), train_loss = 2.15329889, grad/param norm = 9.1774e-02, time/batch = 0.1968s	
423/2700 (epoch 7.833), train_loss = 2.12447836, grad/param norm = 9.3004e-02, time/batch = 0.1970s	
424/2700 (epoch 7.852), train_loss = 2.14468528, grad/param norm = 1.0214e-01, time/batch = 0.1971s	
425/2700 (epoch 7.870), train_loss = 2.12398666, grad/param norm = 1.1882e-01, time/batch = 0.1970s	
426/2700 (epoch 7.889), train_loss = 2.16515498, grad/param norm = 1.3858e-01, time/batch = 0.1965s	
427/2700 (epoch 7.907), train_loss = 2.29154004, grad/param norm = 1.4319e-01, time/batch = 0.1967s	
428/2700 (epoch 7.926), train_loss = 2.17103102, grad/param norm = 1.0585e-01, time/batch = 0.1968s	
429/2700 (epoch 7.944), train_loss = 2.13701115, grad/param norm = 8.2540e-02, time/batch = 0.1969s	
430/2700 (epoch 7.963), train_loss = 2.14522596, grad/param norm = 7.3937e-02, time/batch = 0.1972s	
431/2700 (epoch 7.981), train_loss = 2.12150678, grad/param norm = 7.6148e-02, time/batch = 0.1982s	
432/2700 (epoch 8.000), train_loss = 2.15860896, grad/param norm = 8.9937e-02, time/batch = 0.1970s	
433/2700 (epoch 8.019), train_loss = 2.19540713, grad/param norm = 1.3970e-01, time/batch = 0.1974s	
434/2700 (epoch 8.037), train_loss = 2.26114644, grad/param norm = 1.6816e-01, time/batch = 0.1977s	
435/2700 (epoch 8.056), train_loss = 2.18010463, grad/param norm = 1.6310e-01, time/batch = 0.1976s	
436/2700 (epoch 8.074), train_loss = 2.12118545, grad/param norm = 1.7503e-01, time/batch = 0.1978s	
437/2700 (epoch 8.093), train_loss = 2.16196055, grad/param norm = 1.9291e-01, time/batch = 0.1969s	
438/2700 (epoch 8.111), train_loss = 2.17276515, grad/param norm = 2.0605e-01, time/batch = 0.1977s	
439/2700 (epoch 8.130), train_loss = 2.21482350, grad/param norm = 1.8518e-01, time/batch = 0.1977s	
440/2700 (epoch 8.148), train_loss = 2.11489875, grad/param norm = 1.7331e-01, time/batch = 0.1978s	
441/2700 (epoch 8.167), train_loss = 2.20454051, grad/param norm = 1.5780e-01, time/batch = 0.1990s	
442/2700 (epoch 8.185), train_loss = 2.08869194, grad/param norm = 1.3283e-01, time/batch = 0.1968s	
443/2700 (epoch 8.204), train_loss = 2.08008352, grad/param norm = 9.7350e-02, time/batch = 0.1967s	
444/2700 (epoch 8.222), train_loss = 2.01416853, grad/param norm = 1.0852e-01, time/batch = 0.1972s	
445/2700 (epoch 8.241), train_loss = 1.96432372, grad/param norm = 1.1611e-01, time/batch = 0.1972s	
446/2700 (epoch 8.259), train_loss = 2.01099785, grad/param norm = 1.2933e-01, time/batch = 0.1966s	
447/2700 (epoch 8.278), train_loss = 2.10708971, grad/param norm = 1.5670e-01, time/batch = 0.1969s	
448/2700 (epoch 8.296), train_loss = 2.07710305, grad/param norm = 1.4711e-01, time/batch = 0.1970s	
449/2700 (epoch 8.315), train_loss = 2.07934143, grad/param norm = 1.2223e-01, time/batch = 0.1970s	
450/2700 (epoch 8.333), train_loss = 2.06778615, grad/param norm = 1.1619e-01, time/batch = 0.1973s	
451/2700 (epoch 8.352), train_loss = 2.09680957, grad/param norm = 1.2631e-01, time/batch = 0.1975s	
452/2700 (epoch 8.370), train_loss = 2.11632287, grad/param norm = 1.2533e-01, time/batch = 0.1974s	
453/2700 (epoch 8.389), train_loss = 2.06583539, grad/param norm = 1.1623e-01, time/batch = 0.1972s	
454/2700 (epoch 8.407), train_loss = 2.05824285, grad/param norm = 1.0361e-01, time/batch = 0.1969s	
455/2700 (epoch 8.426), train_loss = 2.10275403, grad/param norm = 9.4922e-02, time/batch = 0.1973s	
456/2700 (epoch 8.444), train_loss = 1.98574017, grad/param norm = 1.1318e-01, time/batch = 0.1968s	
457/2700 (epoch 8.463), train_loss = 2.06971475, grad/param norm = 1.4043e-01, time/batch = 0.1970s	
458/2700 (epoch 8.481), train_loss = 2.06195783, grad/param norm = 1.2703e-01, time/batch = 0.1967s	
459/2700 (epoch 8.500), train_loss = 2.03883379, grad/param norm = 1.2806e-01, time/batch = 0.1968s	
460/2700 (epoch 8.519), train_loss = 2.05971656, grad/param norm = 1.4863e-01, time/batch = 0.1973s	
461/2700 (epoch 8.537), train_loss = 2.07884487, grad/param norm = 1.5524e-01, time/batch = 0.1986s	
462/2700 (epoch 8.556), train_loss = 2.05267810, grad/param norm = 1.6949e-01, time/batch = 0.1966s	
463/2700 (epoch 8.574), train_loss = 2.08659905, grad/param norm = 1.7623e-01, time/batch = 0.1965s	
464/2700 (epoch 8.593), train_loss = 2.07247892, grad/param norm = 1.7464e-01, time/batch = 0.1974s	
465/2700 (epoch 8.611), train_loss = 1.97510446, grad/param norm = 1.3266e-01, time/batch = 0.1970s	
466/2700 (epoch 8.630), train_loss = 1.97940532, grad/param norm = 1.2144e-01, time/batch = 0.1968s	
467/2700 (epoch 8.648), train_loss = 2.00117151, grad/param norm = 9.6967e-02, time/batch = 0.1970s	
468/2700 (epoch 8.667), train_loss = 1.96868536, grad/param norm = 8.7509e-02, time/batch = 0.1967s	
469/2700 (epoch 8.685), train_loss = 1.96356783, grad/param norm = 1.0331e-01, time/batch = 0.1970s	
470/2700 (epoch 8.704), train_loss = 1.99392562, grad/param norm = 1.2977e-01, time/batch = 0.1971s	
471/2700 (epoch 8.722), train_loss = 1.96795766, grad/param norm = 1.3537e-01, time/batch = 0.1982s	
472/2700 (epoch 8.741), train_loss = 2.04930262, grad/param norm = 1.7960e-01, time/batch = 0.1966s	
473/2700 (epoch 8.759), train_loss = 2.09684438, grad/param norm = 1.5532e-01, time/batch = 0.1971s	
474/2700 (epoch 8.778), train_loss = 2.10276627, grad/param norm = 1.5498e-01, time/batch = 0.1973s	
475/2700 (epoch 8.796), train_loss = 2.05868798, grad/param norm = 1.5609e-01, time/batch = 0.1983s	
476/2700 (epoch 8.815), train_loss = 2.04499048, grad/param norm = 1.2621e-01, time/batch = 0.1966s	
477/2700 (epoch 8.833), train_loss = 1.99133133, grad/param norm = 1.0299e-01, time/batch = 0.1968s	
478/2700 (epoch 8.852), train_loss = 1.96411631, grad/param norm = 8.9366e-02, time/batch = 0.1969s	
479/2700 (epoch 8.870), train_loss = 1.94526811, grad/param norm = 1.0457e-01, time/batch = 0.1973s	
480/2700 (epoch 8.889), train_loss = 1.98786367, grad/param norm = 1.2615e-01, time/batch = 0.1971s	
481/2700 (epoch 8.907), train_loss = 2.08419069, grad/param norm = 1.1542e-01, time/batch = 0.1999s	
482/2700 (epoch 8.926), train_loss = 2.01077239, grad/param norm = 1.2043e-01, time/batch = 0.1966s	
483/2700 (epoch 8.944), train_loss = 1.99123242, grad/param norm = 1.1850e-01, time/batch = 0.1968s	
484/2700 (epoch 8.963), train_loss = 2.01311780, grad/param norm = 1.1932e-01, time/batch = 0.1976s	
485/2700 (epoch 8.981), train_loss = 2.00144393, grad/param norm = 1.3755e-01, time/batch = 0.1977s	
486/2700 (epoch 9.000), train_loss = 2.05699639, grad/param norm = 1.5623e-01, time/batch = 0.1970s	
487/2700 (epoch 9.019), train_loss = 2.09391819, grad/param norm = 1.6364e-01, time/batch = 0.1977s	
488/2700 (epoch 9.037), train_loss = 2.07287871, grad/param norm = 1.3761e-01, time/batch = 0.1966s	
489/2700 (epoch 9.056), train_loss = 1.98236560, grad/param norm = 1.2920e-01, time/batch = 0.1970s	
490/2700 (epoch 9.074), train_loss = 1.91260311, grad/param norm = 9.5505e-02, time/batch = 0.1973s	
491/2700 (epoch 9.093), train_loss = 1.89545338, grad/param norm = 8.4729e-02, time/batch = 0.1977s	
492/2700 (epoch 9.111), train_loss = 1.88984019, grad/param norm = 1.1176e-01, time/batch = 0.1970s	
493/2700 (epoch 9.130), train_loss = 1.97427111, grad/param norm = 1.4437e-01, time/batch = 0.1975s	
494/2700 (epoch 9.148), train_loss = 1.94471895, grad/param norm = 1.8140e-01, time/batch = 0.1975s	
495/2700 (epoch 9.167), train_loss = 2.03979582, grad/param norm = 1.4838e-01, time/batch = 0.1972s	
496/2700 (epoch 9.185), train_loss = 1.89475998, grad/param norm = 9.0150e-02, time/batch = 0.1970s	
497/2700 (epoch 9.204), train_loss = 1.91523402, grad/param norm = 7.9107e-02, time/batch = 0.1978s	
498/2700 (epoch 9.222), train_loss = 1.85659755, grad/param norm = 8.6873e-02, time/batch = 0.1972s	
499/2700 (epoch 9.241), train_loss = 1.76487545, grad/param norm = 8.2273e-02, time/batch = 0.1974s	
500/2700 (epoch 9.259), train_loss = 1.81463791, grad/param norm = 7.6011e-02, time/batch = 0.1972s	
501/2700 (epoch 9.278), train_loss = 1.91187075, grad/param norm = 1.0212e-01, time/batch = 0.1985s	
502/2700 (epoch 9.296), train_loss = 1.91369269, grad/param norm = 1.1055e-01, time/batch = 0.1971s	
503/2700 (epoch 9.315), train_loss = 1.92900476, grad/param norm = 1.0763e-01, time/batch = 0.1968s	
504/2700 (epoch 9.333), train_loss = 1.91341120, grad/param norm = 1.1315e-01, time/batch = 0.1975s	
505/2700 (epoch 9.352), train_loss = 1.92186202, grad/param norm = 1.2545e-01, time/batch = 0.1972s	
506/2700 (epoch 9.370), train_loss = 1.96869190, grad/param norm = 1.2766e-01, time/batch = 0.1967s	
507/2700 (epoch 9.389), train_loss = 1.92217036, grad/param norm = 1.2019e-01, time/batch = 0.1975s	
508/2700 (epoch 9.407), train_loss = 1.93567357, grad/param norm = 1.2703e-01, time/batch = 0.1969s	
509/2700 (epoch 9.426), train_loss = 1.97953352, grad/param norm = 1.3798e-01, time/batch = 0.1976s	
510/2700 (epoch 9.444), train_loss = 1.87134529, grad/param norm = 1.2592e-01, time/batch = 0.1973s	
511/2700 (epoch 9.463), train_loss = 1.95963211, grad/param norm = 1.4954e-01, time/batch = 0.1976s	
512/2700 (epoch 9.481), train_loss = 1.98618528, grad/param norm = 1.3295e-01, time/batch = 0.1970s	
513/2700 (epoch 9.500), train_loss = 1.99337777, grad/param norm = 3.1307e-01, time/batch = 0.1971s	
514/2700 (epoch 9.519), train_loss = 2.11924331, grad/param norm = 1.9651e-01, time/batch = 0.1970s	
515/2700 (epoch 9.537), train_loss = 1.95517800, grad/param norm = 1.1859e-01, time/batch = 0.1976s	
516/2700 (epoch 9.556), train_loss = 1.86634458, grad/param norm = 9.0822e-02, time/batch = 0.1970s	
517/2700 (epoch 9.574), train_loss = 1.84523179, grad/param norm = 9.0972e-02, time/batch = 0.1970s	
518/2700 (epoch 9.593), train_loss = 1.86830471, grad/param norm = 1.2261e-01, time/batch = 0.1969s	
519/2700 (epoch 9.611), train_loss = 1.80342034, grad/param norm = 1.3357e-01, time/batch = 0.1969s	
520/2700 (epoch 9.630), train_loss = 1.81776438, grad/param norm = 1.1814e-01, time/batch = 0.1975s	
521/2700 (epoch 9.648), train_loss = 1.83846258, grad/param norm = 1.0125e-01, time/batch = 0.1977s	
522/2700 (epoch 9.667), train_loss = 1.81410403, grad/param norm = 9.4109e-02, time/batch = 0.1971s	
523/2700 (epoch 9.685), train_loss = 1.83073640, grad/param norm = 1.0127e-01, time/batch = 0.1971s	
524/2700 (epoch 9.704), train_loss = 1.84881305, grad/param norm = 1.0859e-01, time/batch = 0.1980s	
525/2700 (epoch 9.722), train_loss = 1.81325578, grad/param norm = 8.7938e-02, time/batch = 0.1979s	
526/2700 (epoch 9.741), train_loss = 1.81732039, grad/param norm = 7.1538e-02, time/batch = 0.1972s	
527/2700 (epoch 9.759), train_loss = 1.88014819, grad/param norm = 1.0007e-01, time/batch = 0.1973s	
528/2700 (epoch 9.778), train_loss = 1.94036712, grad/param norm = 1.4674e-01, time/batch = 0.1970s	
529/2700 (epoch 9.796), train_loss = 1.91428373, grad/param norm = 1.7035e-01, time/batch = 0.1968s	
530/2700 (epoch 9.815), train_loss = 1.93182129, grad/param norm = 1.5984e-01, time/batch = 0.1970s	
531/2700 (epoch 9.833), train_loss = 1.88748836, grad/param norm = 1.3431e-01, time/batch = 0.1976s	
532/2700 (epoch 9.852), train_loss = 1.82729767, grad/param norm = 1.0103e-01, time/batch = 0.1971s	
533/2700 (epoch 9.870), train_loss = 1.79364161, grad/param norm = 8.3437e-02, time/batch = 0.1968s	
534/2700 (epoch 9.889), train_loss = 1.82494255, grad/param norm = 8.8224e-02, time/batch = 0.1969s	
535/2700 (epoch 9.907), train_loss = 1.92915353, grad/param norm = 1.0545e-01, time/batch = 0.1974s	
536/2700 (epoch 9.926), train_loss = 1.87764915, grad/param norm = 1.2224e-01, time/batch = 0.1970s	
537/2700 (epoch 9.944), train_loss = 1.87281190, grad/param norm = 1.3081e-01, time/batch = 0.1969s	
538/2700 (epoch 9.963), train_loss = 1.86502982, grad/param norm = 1.1728e-01, time/batch = 0.1964s	
539/2700 (epoch 9.981), train_loss = 1.83069642, grad/param norm = 9.3868e-02, time/batch = 0.1974s	
decayed learning rate by a factor 0.97 to 0.00194	
540/2700 (epoch 10.000), train_loss = 1.86765347, grad/param norm = 9.5723e-02, time/batch = 0.1975s	
541/2700 (epoch 10.019), train_loss = 1.89614128, grad/param norm = 1.0395e-01, time/batch = 0.1982s	
542/2700 (epoch 10.037), train_loss = 1.88727424, grad/param norm = 9.5463e-02, time/batch = 0.1966s	
543/2700 (epoch 10.056), train_loss = 1.81928041, grad/param norm = 1.0095e-01, time/batch = 0.1970s	
544/2700 (epoch 10.074), train_loss = 1.78117565, grad/param norm = 9.1861e-02, time/batch = 0.1974s	
545/2700 (epoch 10.093), train_loss = 1.74543650, grad/param norm = 7.1500e-02, time/batch = 0.1975s	
546/2700 (epoch 10.111), train_loss = 1.72161676, grad/param norm = 7.1846e-02, time/batch = 0.1971s	
547/2700 (epoch 10.130), train_loss = 1.78820788, grad/param norm = 8.1686e-02, time/batch = 0.1974s	
548/2700 (epoch 10.148), train_loss = 1.72889074, grad/param norm = 9.3504e-02, time/batch = 0.1968s	
549/2700 (epoch 10.167), train_loss = 1.83992911, grad/param norm = 9.5161e-02, time/batch = 0.1974s	
550/2700 (epoch 10.185), train_loss = 1.76132752, grad/param norm = 1.0052e-01, time/batch = 0.1982s	
551/2700 (epoch 10.204), train_loss = 1.81514384, grad/param norm = 1.2365e-01, time/batch = 0.1990s	
552/2700 (epoch 10.222), train_loss = 1.78553802, grad/param norm = 1.3948e-01, time/batch = 0.1972s	
553/2700 (epoch 10.241), train_loss = 1.68264122, grad/param norm = 1.2418e-01, time/batch = 0.1968s	
554/2700 (epoch 10.259), train_loss = 1.71023660, grad/param norm = 8.6441e-02, time/batch = 0.1979s	
555/2700 (epoch 10.278), train_loss = 1.77954819, grad/param norm = 8.6320e-02, time/batch = 0.1979s	
556/2700 (epoch 10.296), train_loss = 1.76985279, grad/param norm = 8.9973e-02, time/batch = 0.1970s	
557/2700 (epoch 10.315), train_loss = 1.76816563, grad/param norm = 8.4878e-02, time/batch = 0.1971s	
558/2700 (epoch 10.333), train_loss = 1.75763573, grad/param norm = 8.3816e-02, time/batch = 0.1967s	
559/2700 (epoch 10.352), train_loss = 1.75062991, grad/param norm = 7.9570e-02, time/batch = 0.1974s	
560/2700 (epoch 10.370), train_loss = 1.78908608, grad/param norm = 7.7265e-02, time/batch = 0.1971s	
561/2700 (epoch 10.389), train_loss = 1.76961241, grad/param norm = 8.6419e-02, time/batch = 0.1979s	
562/2700 (epoch 10.407), train_loss = 1.78798028, grad/param norm = 8.2017e-02, time/batch = 0.1978s	
563/2700 (epoch 10.426), train_loss = 1.82376415, grad/param norm = 8.6460e-02, time/batch = 0.1969s	
564/2700 (epoch 10.444), train_loss = 1.72165345, grad/param norm = 9.3200e-02, time/batch = 0.1971s	
565/2700 (epoch 10.463), train_loss = 1.82664898, grad/param norm = 1.1507e-01, time/batch = 0.1983s	
566/2700 (epoch 10.481), train_loss = 1.82367250, grad/param norm = 1.0892e-01, time/batch = 0.1976s	
567/2700 (epoch 10.500), train_loss = 1.76516337, grad/param norm = 9.3762e-02, time/batch = 0.1979s	
568/2700 (epoch 10.519), train_loss = 1.80155564, grad/param norm = 8.9111e-02, time/batch = 0.1983s	
569/2700 (epoch 10.537), train_loss = 1.78028708, grad/param norm = 9.6694e-02, time/batch = 0.1977s	
570/2700 (epoch 10.556), train_loss = 1.73884403, grad/param norm = 1.0786e-01, time/batch = 0.1981s	
571/2700 (epoch 10.574), train_loss = 1.73249841, grad/param norm = 1.1263e-01, time/batch = 0.1995s	
572/2700 (epoch 10.593), train_loss = 1.74883016, grad/param norm = 1.2337e-01, time/batch = 0.1976s	
573/2700 (epoch 10.611), train_loss = 1.69261965, grad/param norm = 1.2961e-01, time/batch = 0.1974s	
574/2700 (epoch 10.630), train_loss = 1.71539285, grad/param norm = 1.2298e-01, time/batch = 0.1976s	
575/2700 (epoch 10.648), train_loss = 1.71103808, grad/param norm = 8.5371e-02, time/batch = 0.1984s	
576/2700 (epoch 10.667), train_loss = 1.69066228, grad/param norm = 8.2709e-02, time/batch = 0.1974s	
577/2700 (epoch 10.685), train_loss = 1.72128253, grad/param norm = 9.4262e-02, time/batch = 0.1978s	
578/2700 (epoch 10.704), train_loss = 1.73994524, grad/param norm = 9.8252e-02, time/batch = 0.1982s	
579/2700 (epoch 10.722), train_loss = 1.69440148, grad/param norm = 7.9037e-02, time/batch = 0.1978s	
580/2700 (epoch 10.741), train_loss = 1.68513729, grad/param norm = 7.5230e-02, time/batch = 0.1977s	
581/2700 (epoch 10.759), train_loss = 1.74041818, grad/param norm = 9.8864e-02, time/batch = 0.1982s	
582/2700 (epoch 10.778), train_loss = 1.81208814, grad/param norm = 1.2562e-01, time/batch = 0.1975s	
583/2700 (epoch 10.796), train_loss = 1.76630070, grad/param norm = 1.3220e-01, time/batch = 0.1968s	
584/2700 (epoch 10.815), train_loss = 1.79556879, grad/param norm = 1.2782e-01, time/batch = 0.1976s	
585/2700 (epoch 10.833), train_loss = 1.77838597, grad/param norm = 1.2276e-01, time/batch = 0.1975s	
586/2700 (epoch 10.852), train_loss = 1.72876545, grad/param norm = 9.9013e-02, time/batch = 0.1971s	
587/2700 (epoch 10.870), train_loss = 1.68781566, grad/param norm = 8.4418e-02, time/batch = 0.1970s	
588/2700 (epoch 10.889), train_loss = 1.71880823, grad/param norm = 9.8620e-02, time/batch = 0.1970s	
589/2700 (epoch 10.907), train_loss = 1.84778176, grad/param norm = 1.3037e-01, time/batch = 0.1981s	
590/2700 (epoch 10.926), train_loss = 1.77852820, grad/param norm = 1.2717e-01, time/batch = 0.1978s	
591/2700 (epoch 10.944), train_loss = 1.74539823, grad/param norm = 1.0787e-01, time/batch = 0.1985s	
592/2700 (epoch 10.963), train_loss = 1.72012978, grad/param norm = 8.7355e-02, time/batch = 0.1972s	
593/2700 (epoch 10.981), train_loss = 1.68212897, grad/param norm = 7.6554e-02, time/batch = 0.1977s	
decayed learning rate by a factor 0.97 to 0.0018818	
594/2700 (epoch 11.000), train_loss = 1.73322603, grad/param norm = 8.1001e-02, time/batch = 0.1979s	
595/2700 (epoch 11.019), train_loss = 1.76823730, grad/param norm = 9.3829e-02, time/batch = 0.1981s	
596/2700 (epoch 11.037), train_loss = 1.77089377, grad/param norm = 9.9217e-02, time/batch = 0.1972s	
597/2700 (epoch 11.056), train_loss = 1.70854661, grad/param norm = 1.0621e-01, time/batch = 0.1975s	
598/2700 (epoch 11.074), train_loss = 1.68484169, grad/param norm = 8.7375e-02, time/batch = 0.1964s	
599/2700 (epoch 11.093), train_loss = 1.63485739, grad/param norm = 7.4520e-02, time/batch = 0.1972s	
600/2700 (epoch 11.111), train_loss = 1.61299602, grad/param norm = 7.8206e-02, time/batch = 0.1969s	
601/2700 (epoch 11.130), train_loss = 1.67632745, grad/param norm = 8.3958e-02, time/batch = 0.1977s	
602/2700 (epoch 11.148), train_loss = 1.61636407, grad/param norm = 8.0241e-02, time/batch = 0.1971s	
603/2700 (epoch 11.167), train_loss = 1.70953314, grad/param norm = 7.5754e-02, time/batch = 0.1966s	
604/2700 (epoch 11.185), train_loss = 1.61945001, grad/param norm = 6.8080e-02, time/batch = 0.1975s	
605/2700 (epoch 11.204), train_loss = 1.68028394, grad/param norm = 9.0635e-02, time/batch = 0.1976s	
606/2700 (epoch 11.222), train_loss = 1.65737263, grad/param norm = 1.1432e-01, time/batch = 0.1973s	
607/2700 (epoch 11.241), train_loss = 1.57337901, grad/param norm = 1.0512e-01, time/batch = 0.1981s	
608/2700 (epoch 11.259), train_loss = 1.61852680, grad/param norm = 8.7867e-02, time/batch = 0.1974s	
609/2700 (epoch 11.278), train_loss = 1.69202076, grad/param norm = 9.0753e-02, time/batch = 0.1969s	
610/2700 (epoch 11.296), train_loss = 1.68206425, grad/param norm = 1.0434e-01, time/batch = 0.1972s	
611/2700 (epoch 11.315), train_loss = 1.66582972, grad/param norm = 1.0811e-01, time/batch = 0.1982s	
612/2700 (epoch 11.333), train_loss = 1.65876720, grad/param norm = 9.1890e-02, time/batch = 0.1966s	
613/2700 (epoch 11.352), train_loss = 1.65277980, grad/param norm = 8.3655e-02, time/batch = 0.1972s	
614/2700 (epoch 11.370), train_loss = 1.67041509, grad/param norm = 8.8163e-02, time/batch = 0.1977s	
615/2700 (epoch 11.389), train_loss = 1.65480997, grad/param norm = 9.0014e-02, time/batch = 0.1977s	
616/2700 (epoch 11.407), train_loss = 1.70024287, grad/param norm = 9.1274e-02, time/batch = 0.1967s	
617/2700 (epoch 11.426), train_loss = 1.74188855, grad/param norm = 9.3129e-02, time/batch = 0.1969s	
618/2700 (epoch 11.444), train_loss = 1.59850036, grad/param norm = 7.2659e-02, time/batch = 0.1967s	
619/2700 (epoch 11.463), train_loss = 1.68811067, grad/param norm = 8.6966e-02, time/batch = 0.1969s	
620/2700 (epoch 11.481), train_loss = 1.70278537, grad/param norm = 1.0143e-01, time/batch = 0.1971s	
621/2700 (epoch 11.500), train_loss = 1.66504507, grad/param norm = 1.0131e-01, time/batch = 0.1981s	
622/2700 (epoch 11.519), train_loss = 1.72251262, grad/param norm = 9.6978e-02, time/batch = 0.1969s	
623/2700 (epoch 11.537), train_loss = 1.66791873, grad/param norm = 8.4613e-02, time/batch = 0.1970s	
624/2700 (epoch 11.556), train_loss = 1.60111490, grad/param norm = 8.2154e-02, time/batch = 0.1975s	
625/2700 (epoch 11.574), train_loss = 1.59858069, grad/param norm = 8.7888e-02, time/batch = 0.1980s	
626/2700 (epoch 11.593), train_loss = 1.63561441, grad/param norm = 9.6167e-02, time/batch = 0.1968s	
627/2700 (epoch 11.611), train_loss = 1.55323571, grad/param norm = 8.3376e-02, time/batch = 0.1972s	
628/2700 (epoch 11.630), train_loss = 1.56472066, grad/param norm = 8.6206e-02, time/batch = 0.1969s	
629/2700 (epoch 11.648), train_loss = 1.63552495, grad/param norm = 1.1293e-01, time/batch = 0.1975s	
630/2700 (epoch 11.667), train_loss = 1.62669634, grad/param norm = 1.1820e-01, time/batch = 0.1975s	
631/2700 (epoch 11.685), train_loss = 1.64215333, grad/param norm = 1.1471e-01, time/batch = 0.1975s	
632/2700 (epoch 11.704), train_loss = 1.67471000, grad/param norm = 1.2714e-01, time/batch = 0.1968s	
633/2700 (epoch 11.722), train_loss = 1.62921875, grad/param norm = 1.0159e-01, time/batch = 0.1969s	
634/2700 (epoch 11.741), train_loss = 1.59294832, grad/param norm = 8.2937e-02, time/batch = 0.1970s	
635/2700 (epoch 11.759), train_loss = 1.62392927, grad/param norm = 9.0495e-02, time/batch = 0.1975s	
636/2700 (epoch 11.778), train_loss = 1.69297552, grad/param norm = 1.0951e-01, time/batch = 0.1966s	
637/2700 (epoch 11.796), train_loss = 1.61546378, grad/param norm = 8.8969e-02, time/batch = 0.1975s	
638/2700 (epoch 11.815), train_loss = 1.65273627, grad/param norm = 7.0810e-02, time/batch = 0.1966s	
639/2700 (epoch 11.833), train_loss = 1.61298681, grad/param norm = 6.9377e-02, time/batch = 0.1971s	
640/2700 (epoch 11.852), train_loss = 1.58503166, grad/param norm = 7.2415e-02, time/batch = 0.1971s	
641/2700 (epoch 11.870), train_loss = 1.58788804, grad/param norm = 7.5983e-02, time/batch = 0.1978s	
642/2700 (epoch 11.889), train_loss = 1.61080650, grad/param norm = 7.9318e-02, time/batch = 0.1970s	
643/2700 (epoch 11.907), train_loss = 1.72011750, grad/param norm = 8.3156e-02, time/batch = 0.1968s	
644/2700 (epoch 11.926), train_loss = 1.66856529, grad/param norm = 9.0156e-02, time/batch = 0.1974s	
645/2700 (epoch 11.944), train_loss = 1.64533625, grad/param norm = 1.0674e-01, time/batch = 0.1973s	
646/2700 (epoch 11.963), train_loss = 1.65688287, grad/param norm = 9.0410e-02, time/batch = 0.1966s	
647/2700 (epoch 11.981), train_loss = 1.59575246, grad/param norm = 8.3852e-02, time/batch = 0.1974s	
decayed learning rate by a factor 0.97 to 0.001825346	
648/2700 (epoch 12.000), train_loss = 1.62924534, grad/param norm = 8.0015e-02, time/batch = 0.1970s	
649/2700 (epoch 12.019), train_loss = 1.64923060, grad/param norm = 7.5680e-02, time/batch = 0.1972s	
650/2700 (epoch 12.037), train_loss = 1.64673371, grad/param norm = 8.2719e-02, time/batch = 0.1973s	
651/2700 (epoch 12.056), train_loss = 1.60301528, grad/param norm = 1.0360e-01, time/batch = 0.1977s	
652/2700 (epoch 12.074), train_loss = 1.58318276, grad/param norm = 9.0736e-02, time/batch = 0.1973s	
653/2700 (epoch 12.093), train_loss = 1.54516681, grad/param norm = 8.5124e-02, time/batch = 0.1973s	
654/2700 (epoch 12.111), train_loss = 1.52056248, grad/param norm = 7.9690e-02, time/batch = 0.1978s	
655/2700 (epoch 12.130), train_loss = 1.57941488, grad/param norm = 7.3723e-02, time/batch = 0.1981s	
656/2700 (epoch 12.148), train_loss = 1.51679340, grad/param norm = 7.2759e-02, time/batch = 0.1977s	
657/2700 (epoch 12.167), train_loss = 1.61598915, grad/param norm = 7.1273e-02, time/batch = 0.1975s	
658/2700 (epoch 12.185), train_loss = 1.52916897, grad/param norm = 7.8184e-02, time/batch = 0.1973s	
659/2700 (epoch 12.204), train_loss = 1.59701808, grad/param norm = 9.8482e-02, time/batch = 0.1982s	
660/2700 (epoch 12.222), train_loss = 1.55260983, grad/param norm = 9.4703e-02, time/batch = 0.1981s	
661/2700 (epoch 12.241), train_loss = 1.45548841, grad/param norm = 8.5142e-02, time/batch = 0.1986s	
662/2700 (epoch 12.259), train_loss = 1.51126518, grad/param norm = 8.0561e-02, time/batch = 0.1980s	
663/2700 (epoch 12.278), train_loss = 1.60108106, grad/param norm = 9.8114e-02, time/batch = 0.1977s	
664/2700 (epoch 12.296), train_loss = 1.57643250, grad/param norm = 7.8991e-02, time/batch = 0.1978s	
665/2700 (epoch 12.315), train_loss = 1.54592227, grad/param norm = 6.7063e-02, time/batch = 0.1984s	
666/2700 (epoch 12.333), train_loss = 1.53955391, grad/param norm = 6.7182e-02, time/batch = 0.1977s	
667/2700 (epoch 12.352), train_loss = 1.54734516, grad/param norm = 7.9155e-02, time/batch = 0.1980s	
668/2700 (epoch 12.370), train_loss = 1.56122552, grad/param norm = 7.5461e-02, time/batch = 0.1978s	
669/2700 (epoch 12.389), train_loss = 1.54257310, grad/param norm = 8.5829e-02, time/batch = 0.1976s	
670/2700 (epoch 12.407), train_loss = 1.60798100, grad/param norm = 8.1050e-02, time/batch = 0.1976s	
671/2700 (epoch 12.426), train_loss = 1.63591638, grad/param norm = 8.9904e-02, time/batch = 0.1990s	
672/2700 (epoch 12.444), train_loss = 1.53192156, grad/param norm = 9.1120e-02, time/batch = 0.1980s	
673/2700 (epoch 12.463), train_loss = 1.60810063, grad/param norm = 8.9933e-02, time/batch = 0.1979s	
674/2700 (epoch 12.481), train_loss = 1.60790055, grad/param norm = 8.7125e-02, time/batch = 0.1976s	
675/2700 (epoch 12.500), train_loss = 1.55748742, grad/param norm = 8.4408e-02, time/batch = 0.1980s	
676/2700 (epoch 12.519), train_loss = 1.60292445, grad/param norm = 7.6546e-02, time/batch = 0.1975s	
677/2700 (epoch 12.537), train_loss = 1.54960855, grad/param norm = 7.7175e-02, time/batch = 0.1976s	
678/2700 (epoch 12.556), train_loss = 1.51106703, grad/param norm = 9.3075e-02, time/batch = 0.1971s	
679/2700 (epoch 12.574), train_loss = 1.51003208, grad/param norm = 8.8693e-02, time/batch = 0.1976s	
680/2700 (epoch 12.593), train_loss = 1.55151986, grad/param norm = 9.4619e-02, time/batch = 0.1974s	
681/2700 (epoch 12.611), train_loss = 1.47914989, grad/param norm = 9.2486e-02, time/batch = 0.1975s	
682/2700 (epoch 12.630), train_loss = 1.47617669, grad/param norm = 7.8811e-02, time/batch = 0.1973s	
683/2700 (epoch 12.648), train_loss = 1.52090584, grad/param norm = 7.9968e-02, time/batch = 0.1970s	
684/2700 (epoch 12.667), train_loss = 1.49938751, grad/param norm = 7.4459e-02, time/batch = 0.1975s	
685/2700 (epoch 12.685), train_loss = 1.53617008, grad/param norm = 8.4611e-02, time/batch = 0.1977s	
686/2700 (epoch 12.704), train_loss = 1.57062682, grad/param norm = 1.0085e-01, time/batch = 0.1964s	
687/2700 (epoch 12.722), train_loss = 1.54203665, grad/param norm = 9.5095e-02, time/batch = 0.1969s	
688/2700 (epoch 12.741), train_loss = 1.49149481, grad/param norm = 8.5136e-02, time/batch = 0.1966s	
689/2700 (epoch 12.759), train_loss = 1.49366652, grad/param norm = 6.8805e-02, time/batch = 0.1970s	
690/2700 (epoch 12.778), train_loss = 1.56497091, grad/param norm = 7.2828e-02, time/batch = 0.1976s	
691/2700 (epoch 12.796), train_loss = 1.53644686, grad/param norm = 9.1182e-02, time/batch = 0.1985s	
692/2700 (epoch 12.815), train_loss = 1.58667804, grad/param norm = 8.4989e-02, time/batch = 0.1966s	
693/2700 (epoch 12.833), train_loss = 1.52143255, grad/param norm = 7.6472e-02, time/batch = 0.1970s	
694/2700 (epoch 12.852), train_loss = 1.48785041, grad/param norm = 7.1581e-02, time/batch = 0.1971s	
695/2700 (epoch 12.870), train_loss = 1.48631151, grad/param norm = 7.3862e-02, time/batch = 0.1978s	
696/2700 (epoch 12.889), train_loss = 1.49095080, grad/param norm = 7.0779e-02, time/batch = 0.1974s	
697/2700 (epoch 12.907), train_loss = 1.59382440, grad/param norm = 8.0275e-02, time/batch = 0.1974s	
698/2700 (epoch 12.926), train_loss = 1.56973956, grad/param norm = 9.6391e-02, time/batch = 0.1971s	
699/2700 (epoch 12.944), train_loss = 1.54142881, grad/param norm = 9.0456e-02, time/batch = 0.1976s	
700/2700 (epoch 12.963), train_loss = 1.54623843, grad/param norm = 8.2019e-02, time/batch = 0.1973s	
701/2700 (epoch 12.981), train_loss = 1.50527098, grad/param norm = 8.1211e-02, time/batch = 0.1978s	
decayed learning rate by a factor 0.97 to 0.00177058562	
702/2700 (epoch 13.000), train_loss = 1.58348180, grad/param norm = 1.0284e-01, time/batch = 0.1975s	
703/2700 (epoch 13.019), train_loss = 1.63450642, grad/param norm = 1.0161e-01, time/batch = 0.1976s	
704/2700 (epoch 13.037), train_loss = 1.56055330, grad/param norm = 8.2445e-02, time/batch = 0.1976s	
705/2700 (epoch 13.056), train_loss = 1.49018746, grad/param norm = 8.2306e-02, time/batch = 0.1980s	
706/2700 (epoch 13.074), train_loss = 1.48498157, grad/param norm = 7.5540e-02, time/batch = 0.1970s	
707/2700 (epoch 13.093), train_loss = 1.44462988, grad/param norm = 7.4113e-02, time/batch = 0.1978s	
708/2700 (epoch 13.111), train_loss = 1.41968984, grad/param norm = 6.6365e-02, time/batch = 0.1976s	
709/2700 (epoch 13.130), train_loss = 1.49187822, grad/param norm = 7.4868e-02, time/batch = 0.1970s	
710/2700 (epoch 13.148), train_loss = 1.43648063, grad/param norm = 8.7552e-02, time/batch = 0.1975s	
711/2700 (epoch 13.167), train_loss = 1.55474611, grad/param norm = 9.5043e-02, time/batch = 0.1980s	
712/2700 (epoch 13.185), train_loss = 1.46516337, grad/param norm = 9.2147e-02, time/batch = 0.1974s	
713/2700 (epoch 13.204), train_loss = 1.49442348, grad/param norm = 9.3386e-02, time/batch = 0.1967s	
714/2700 (epoch 13.222), train_loss = 1.45883369, grad/param norm = 8.8334e-02, time/batch = 0.1968s	
715/2700 (epoch 13.241), train_loss = 1.38354945, grad/param norm = 7.9217e-02, time/batch = 0.1979s	
716/2700 (epoch 13.259), train_loss = 1.42049506, grad/param norm = 7.3563e-02, time/batch = 0.1974s	
717/2700 (epoch 13.278), train_loss = 1.49159806, grad/param norm = 6.7542e-02, time/batch = 0.1976s	
718/2700 (epoch 13.296), train_loss = 1.45901830, grad/param norm = 6.3409e-02, time/batch = 0.1966s	
719/2700 (epoch 13.315), train_loss = 1.44466411, grad/param norm = 7.9646e-02, time/batch = 0.1969s	
720/2700 (epoch 13.333), train_loss = 1.47517736, grad/param norm = 9.8296e-02, time/batch = 0.1971s	
721/2700 (epoch 13.352), train_loss = 1.47320078, grad/param norm = 9.8024e-02, time/batch = 0.1980s	
722/2700 (epoch 13.370), train_loss = 1.46066562, grad/param norm = 8.3020e-02, time/batch = 0.1975s	
723/2700 (epoch 13.389), train_loss = 1.43015225, grad/param norm = 8.0801e-02, time/batch = 0.1967s	
724/2700 (epoch 13.407), train_loss = 1.51737574, grad/param norm = 8.3831e-02, time/batch = 0.1977s	
725/2700 (epoch 13.426), train_loss = 1.53718470, grad/param norm = 9.1397e-02, time/batch = 0.1981s	
726/2700 (epoch 13.444), train_loss = 1.42053109, grad/param norm = 7.4958e-02, time/batch = 0.1967s	
727/2700 (epoch 13.463), train_loss = 1.49205082, grad/param norm = 7.8252e-02, time/batch = 0.1974s	
728/2700 (epoch 13.481), train_loss = 1.49672336, grad/param norm = 9.0517e-02, time/batch = 0.1971s	
729/2700 (epoch 13.500), train_loss = 1.46415549, grad/param norm = 9.7501e-02, time/batch = 0.1970s	
730/2700 (epoch 13.519), train_loss = 1.52694814, grad/param norm = 9.4199e-02, time/batch = 0.1973s	
731/2700 (epoch 13.537), train_loss = 1.49599577, grad/param norm = 9.6341e-02, time/batch = 0.1980s	
732/2700 (epoch 13.556), train_loss = 1.39883262, grad/param norm = 7.3660e-02, time/batch = 0.1970s	
733/2700 (epoch 13.574), train_loss = 1.41536510, grad/param norm = 8.9191e-02, time/batch = 0.1968s	
734/2700 (epoch 13.593), train_loss = 1.46052321, grad/param norm = 8.9838e-02, time/batch = 0.1971s	
735/2700 (epoch 13.611), train_loss = 1.36280706, grad/param norm = 7.1920e-02, time/batch = 0.1973s	
736/2700 (epoch 13.630), train_loss = 1.40807893, grad/param norm = 8.1165e-02, time/batch = 0.1971s	
737/2700 (epoch 13.648), train_loss = 1.44329806, grad/param norm = 9.0739e-02, time/batch = 0.1972s	
738/2700 (epoch 13.667), train_loss = 1.42391577, grad/param norm = 8.1864e-02, time/batch = 0.1970s	
739/2700 (epoch 13.685), train_loss = 1.45451408, grad/param norm = 8.8632e-02, time/batch = 0.1972s	
740/2700 (epoch 13.704), train_loss = 1.47327987, grad/param norm = 8.2463e-02, time/batch = 0.1980s	
741/2700 (epoch 13.722), train_loss = 1.44603869, grad/param norm = 7.8068e-02, time/batch = 0.1978s	
742/2700 (epoch 13.741), train_loss = 1.39716111, grad/param norm = 6.9995e-02, time/batch = 0.1972s	
743/2700 (epoch 13.759), train_loss = 1.40332242, grad/param norm = 6.9785e-02, time/batch = 0.1968s	
744/2700 (epoch 13.778), train_loss = 1.47146622, grad/param norm = 7.3678e-02, time/batch = 0.1974s	
745/2700 (epoch 13.796), train_loss = 1.39718192, grad/param norm = 7.1065e-02, time/batch = 0.1977s	
746/2700 (epoch 13.815), train_loss = 1.46309796, grad/param norm = 6.9403e-02, time/batch = 0.1969s	
747/2700 (epoch 13.833), train_loss = 1.43633615, grad/param norm = 7.8179e-02, time/batch = 0.1972s	
748/2700 (epoch 13.852), train_loss = 1.41930443, grad/param norm = 8.7078e-02, time/batch = 0.1972s	
749/2700 (epoch 13.870), train_loss = 1.41300515, grad/param norm = 7.6764e-02, time/batch = 0.1976s	
750/2700 (epoch 13.889), train_loss = 1.40111901, grad/param norm = 7.5935e-02, time/batch = 0.1974s	
751/2700 (epoch 13.907), train_loss = 1.50399071, grad/param norm = 9.6932e-02, time/batch = 0.1983s	
752/2700 (epoch 13.926), train_loss = 1.49149811, grad/param norm = 1.1033e-01, time/batch = 0.1978s	
753/2700 (epoch 13.944), train_loss = 1.46882953, grad/param norm = 1.0292e-01, time/batch = 0.1975s	
754/2700 (epoch 13.963), train_loss = 1.45838184, grad/param norm = 1.0397e-01, time/batch = 0.1978s	
755/2700 (epoch 13.981), train_loss = 1.42749462, grad/param norm = 1.0999e-01, time/batch = 0.1977s	
decayed learning rate by a factor 0.97 to 0.0017174680514	
756/2700 (epoch 14.000), train_loss = 1.48282088, grad/param norm = 1.0687e-01, time/batch = 0.1970s	
757/2700 (epoch 14.019), train_loss = 1.50407966, grad/param norm = 7.7494e-02, time/batch = 0.1971s	
758/2700 (epoch 14.037), train_loss = 1.45147104, grad/param norm = 7.5210e-02, time/batch = 0.1967s	
759/2700 (epoch 14.056), train_loss = 1.40821632, grad/param norm = 8.6558e-02, time/batch = 0.1975s	
760/2700 (epoch 14.074), train_loss = 1.39253494, grad/param norm = 6.4579e-02, time/batch = 0.1972s	
761/2700 (epoch 14.093), train_loss = 1.34468151, grad/param norm = 5.6573e-02, time/batch = 0.1976s	
762/2700 (epoch 14.111), train_loss = 1.34167834, grad/param norm = 6.8017e-02, time/batch = 0.1967s	
763/2700 (epoch 14.130), train_loss = 1.40662220, grad/param norm = 7.8249e-02, time/batch = 0.1968s	
764/2700 (epoch 14.148), train_loss = 1.33126767, grad/param norm = 7.0468e-02, time/batch = 0.1973s	
765/2700 (epoch 14.167), train_loss = 1.43627481, grad/param norm = 7.3147e-02, time/batch = 0.1980s	
766/2700 (epoch 14.185), train_loss = 1.36081164, grad/param norm = 7.5734e-02, time/batch = 0.1973s	
767/2700 (epoch 14.204), train_loss = 1.40403805, grad/param norm = 8.5995e-02, time/batch = 0.1975s	
768/2700 (epoch 14.222), train_loss = 1.38243711, grad/param norm = 9.4049e-02, time/batch = 0.1971s	
769/2700 (epoch 14.241), train_loss = 1.31577860, grad/param norm = 8.4795e-02, time/batch = 0.1970s	
770/2700 (epoch 14.259), train_loss = 1.34350124, grad/param norm = 8.4575e-02, time/batch = 0.1971s	
771/2700 (epoch 14.278), train_loss = 1.42992493, grad/param norm = 8.5221e-02, time/batch = 0.1976s	
772/2700 (epoch 14.296), train_loss = 1.41577255, grad/param norm = 9.2357e-02, time/batch = 0.1968s	
773/2700 (epoch 14.315), train_loss = 1.34921639, grad/param norm = 8.1528e-02, time/batch = 0.1968s	
774/2700 (epoch 14.333), train_loss = 1.35297014, grad/param norm = 7.3083e-02, time/batch = 0.1975s	
775/2700 (epoch 14.352), train_loss = 1.35048396, grad/param norm = 7.9298e-02, time/batch = 0.1975s	
776/2700 (epoch 14.370), train_loss = 1.37474673, grad/param norm = 9.0678e-02, time/batch = 0.1973s	
777/2700 (epoch 14.389), train_loss = 1.36302378, grad/param norm = 9.3772e-02, time/batch = 0.1975s	
778/2700 (epoch 14.407), train_loss = 1.45290942, grad/param norm = 9.7222e-02, time/batch = 0.1967s	
779/2700 (epoch 14.426), train_loss = 1.45715247, grad/param norm = 7.7376e-02, time/batch = 0.1969s	
780/2700 (epoch 14.444), train_loss = 1.34367286, grad/param norm = 6.8691e-02, time/batch = 0.1971s	
781/2700 (epoch 14.463), train_loss = 1.39899426, grad/param norm = 7.5239e-02, time/batch = 0.1976s	
782/2700 (epoch 14.481), train_loss = 1.40116328, grad/param norm = 9.1153e-02, time/batch = 0.1972s	
783/2700 (epoch 14.500), train_loss = 1.36391025, grad/param norm = 8.6021e-02, time/batch = 0.1978s	
784/2700 (epoch 14.519), train_loss = 1.42425331, grad/param norm = 8.1123e-02, time/batch = 0.1977s	
785/2700 (epoch 14.537), train_loss = 1.39493567, grad/param norm = 9.2521e-02, time/batch = 0.1979s	
786/2700 (epoch 14.556), train_loss = 1.35869250, grad/param norm = 1.1471e-01, time/batch = 0.1966s	
787/2700 (epoch 14.574), train_loss = 1.34984241, grad/param norm = 9.5131e-02, time/batch = 0.1973s	
788/2700 (epoch 14.593), train_loss = 1.37623982, grad/param norm = 7.9536e-02, time/batch = 0.1967s	
789/2700 (epoch 14.611), train_loss = 1.30335995, grad/param norm = 7.3879e-02, time/batch = 0.1982s	
790/2700 (epoch 14.630), train_loss = 1.31534340, grad/param norm = 8.3599e-02, time/batch = 0.1977s	
791/2700 (epoch 14.648), train_loss = 1.33541047, grad/param norm = 8.1094e-02, time/batch = 0.1982s	
792/2700 (epoch 14.667), train_loss = 1.34902483, grad/param norm = 8.8045e-02, time/batch = 0.1969s	
793/2700 (epoch 14.685), train_loss = 1.36900719, grad/param norm = 8.3596e-02, time/batch = 0.1967s	
794/2700 (epoch 14.704), train_loss = 1.39759287, grad/param norm = 8.5430e-02, time/batch = 0.1973s	
795/2700 (epoch 14.722), train_loss = 1.36258997, grad/param norm = 6.9715e-02, time/batch = 0.1978s	
796/2700 (epoch 14.741), train_loss = 1.31427856, grad/param norm = 7.2015e-02, time/batch = 0.1968s	
797/2700 (epoch 14.759), train_loss = 1.31150337, grad/param norm = 7.9091e-02, time/batch = 0.1976s	
798/2700 (epoch 14.778), train_loss = 1.38842594, grad/param norm = 8.0210e-02, time/batch = 0.1967s	
799/2700 (epoch 14.796), train_loss = 1.32436143, grad/param norm = 7.7091e-02, time/batch = 0.1972s	
800/2700 (epoch 14.815), train_loss = 1.36987534, grad/param norm = 7.1615e-02, time/batch = 0.1971s	
801/2700 (epoch 14.833), train_loss = 1.36986041, grad/param norm = 8.5909e-02, time/batch = 0.1978s	
802/2700 (epoch 14.852), train_loss = 1.33117088, grad/param norm = 9.6097e-02, time/batch = 0.1974s	
803/2700 (epoch 14.870), train_loss = 1.33638350, grad/param norm = 8.5344e-02, time/batch = 0.1971s	
804/2700 (epoch 14.889), train_loss = 1.33519256, grad/param norm = 8.4788e-02, time/batch = 0.1975s	
805/2700 (epoch 14.907), train_loss = 1.40297063, grad/param norm = 8.6542e-02, time/batch = 0.1975s	
806/2700 (epoch 14.926), train_loss = 1.36641066, grad/param norm = 7.7519e-02, time/batch = 0.1968s	
807/2700 (epoch 14.944), train_loss = 1.35959097, grad/param norm = 9.1654e-02, time/batch = 0.1975s	
808/2700 (epoch 14.963), train_loss = 1.37415761, grad/param norm = 1.0166e-01, time/batch = 0.1971s	
809/2700 (epoch 14.981), train_loss = 1.32724424, grad/param norm = 9.0683e-02, time/batch = 0.1974s	
decayed learning rate by a factor 0.97 to 0.001665944009858	
810/2700 (epoch 15.000), train_loss = 1.38283358, grad/param norm = 8.0645e-02, time/batch = 0.1984s	
811/2700 (epoch 15.019), train_loss = 1.43229114, grad/param norm = 1.0215e-01, time/batch = 0.1990s	
812/2700 (epoch 15.037), train_loss = 1.39527756, grad/param norm = 9.7369e-02, time/batch = 0.1969s	
813/2700 (epoch 15.056), train_loss = 1.31307368, grad/param norm = 7.7592e-02, time/batch = 0.1971s	
814/2700 (epoch 15.074), train_loss = 1.32601506, grad/param norm = 7.9397e-02, time/batch = 0.1974s	
815/2700 (epoch 15.093), train_loss = 1.28995327, grad/param norm = 7.5709e-02, time/batch = 0.1977s	
816/2700 (epoch 15.111), train_loss = 1.25731815, grad/param norm = 6.4762e-02, time/batch = 0.1980s	
817/2700 (epoch 15.130), train_loss = 1.30675914, grad/param norm = 6.4414e-02, time/batch = 0.1988s	
818/2700 (epoch 15.148), train_loss = 1.24004848, grad/param norm = 6.3020e-02, time/batch = 0.1983s	
819/2700 (epoch 15.167), train_loss = 1.33277555, grad/param norm = 6.8040e-02, time/batch = 0.1984s	
820/2700 (epoch 15.185), train_loss = 1.26346605, grad/param norm = 6.9778e-02, time/batch = 0.1979s	
821/2700 (epoch 15.204), train_loss = 1.30732476, grad/param norm = 8.1346e-02, time/batch = 0.1983s	
822/2700 (epoch 15.222), train_loss = 1.28353300, grad/param norm = 7.6891e-02, time/batch = 0.1972s	
823/2700 (epoch 15.241), train_loss = 1.22352834, grad/param norm = 6.7894e-02, time/batch = 0.1976s	
824/2700 (epoch 15.259), train_loss = 1.26022372, grad/param norm = 8.3723e-02, time/batch = 0.1980s	
825/2700 (epoch 15.278), train_loss = 1.35431272, grad/param norm = 1.0072e-01, time/batch = 0.1978s	
826/2700 (epoch 15.296), train_loss = 1.32903392, grad/param norm = 9.7705e-02, time/batch = 0.1977s	
827/2700 (epoch 15.315), train_loss = 1.26566316, grad/param norm = 8.6506e-02, time/batch = 0.1975s	
828/2700 (epoch 15.333), train_loss = 1.30147495, grad/param norm = 9.2634e-02, time/batch = 0.1971s	
829/2700 (epoch 15.352), train_loss = 1.27787476, grad/param norm = 8.4263e-02, time/batch = 0.1974s	
830/2700 (epoch 15.370), train_loss = 1.26928633, grad/param norm = 8.0958e-02, time/batch = 0.1972s	
831/2700 (epoch 15.389), train_loss = 1.24880458, grad/param norm = 8.3763e-02, time/batch = 0.1980s	
832/2700 (epoch 15.407), train_loss = 1.33514120, grad/param norm = 8.0908e-02, time/batch = 0.1974s	
833/2700 (epoch 15.426), train_loss = 1.36164335, grad/param norm = 8.9354e-02, time/batch = 0.1981s	
834/2700 (epoch 15.444), train_loss = 1.26669815, grad/param norm = 7.9056e-02, time/batch = 0.1970s	
835/2700 (epoch 15.463), train_loss = 1.30858641, grad/param norm = 7.7650e-02, time/batch = 0.1978s	
836/2700 (epoch 15.481), train_loss = 1.29557270, grad/param norm = 8.0164e-02, time/batch = 0.1973s	
837/2700 (epoch 15.500), train_loss = 1.26424379, grad/param norm = 8.5951e-02, time/batch = 0.1978s	
838/2700 (epoch 15.519), train_loss = 1.34232109, grad/param norm = 8.0948e-02, time/batch = 0.1966s	
839/2700 (epoch 15.537), train_loss = 1.30460517, grad/param norm = 8.7498e-02, time/batch = 0.1974s	
840/2700 (epoch 15.556), train_loss = 1.24152438, grad/param norm = 7.1603e-02, time/batch = 0.1971s	
841/2700 (epoch 15.574), train_loss = 1.23117863, grad/param norm = 7.3713e-02, time/batch = 0.1987s	
842/2700 (epoch 15.593), train_loss = 1.28426459, grad/param norm = 8.7673e-02, time/batch = 0.1973s	
843/2700 (epoch 15.611), train_loss = 1.22809799, grad/param norm = 8.5619e-02, time/batch = 0.1973s	
844/2700 (epoch 15.630), train_loss = 1.21101213, grad/param norm = 6.7985e-02, time/batch = 0.1977s	
845/2700 (epoch 15.648), train_loss = 1.24979108, grad/param norm = 7.3577e-02, time/batch = 0.1980s	
846/2700 (epoch 15.667), train_loss = 1.25012586, grad/param norm = 7.3574e-02, time/batch = 0.1972s	
847/2700 (epoch 15.685), train_loss = 1.26107713, grad/param norm = 6.9948e-02, time/batch = 0.1972s	
848/2700 (epoch 15.704), train_loss = 1.29588699, grad/param norm = 8.2560e-02, time/batch = 0.1967s	
849/2700 (epoch 15.722), train_loss = 1.28950495, grad/param norm = 8.7299e-02, time/batch = 0.1971s	
850/2700 (epoch 15.741), train_loss = 1.23169734, grad/param norm = 8.2272e-02, time/batch = 0.1973s	
851/2700 (epoch 15.759), train_loss = 1.21317876, grad/param norm = 7.5435e-02, time/batch = 0.1978s	
852/2700 (epoch 15.778), train_loss = 1.29819880, grad/param norm = 7.7623e-02, time/batch = 0.1975s	
853/2700 (epoch 15.796), train_loss = 1.21860149, grad/param norm = 8.3909e-02, time/batch = 0.1974s	
854/2700 (epoch 15.815), train_loss = 1.28568518, grad/param norm = 9.7928e-02, time/batch = 0.1978s	
855/2700 (epoch 15.833), train_loss = 1.28632936, grad/param norm = 1.0989e-01, time/batch = 0.1984s	
856/2700 (epoch 15.852), train_loss = 1.24397468, grad/param norm = 8.7241e-02, time/batch = 0.1978s	
857/2700 (epoch 15.870), train_loss = 1.27260245, grad/param norm = 1.0985e-01, time/batch = 0.1977s	
858/2700 (epoch 15.889), train_loss = 1.28093438, grad/param norm = 9.2423e-02, time/batch = 0.1969s	
859/2700 (epoch 15.907), train_loss = 1.29704635, grad/param norm = 7.7892e-02, time/batch = 0.1975s	
860/2700 (epoch 15.926), train_loss = 1.26463868, grad/param norm = 7.6121e-02, time/batch = 0.1971s	
861/2700 (epoch 15.944), train_loss = 1.23532386, grad/param norm = 7.3965e-02, time/batch = 0.1980s	
862/2700 (epoch 15.963), train_loss = 1.25039617, grad/param norm = 7.9067e-02, time/batch = 0.1970s	
863/2700 (epoch 15.981), train_loss = 1.22531690, grad/param norm = 9.1217e-02, time/batch = 0.1973s	
decayed learning rate by a factor 0.97 to 0.0016159656895623	
864/2700 (epoch 16.000), train_loss = 1.28196559, grad/param norm = 9.8741e-02, time/batch = 0.1970s	
865/2700 (epoch 16.019), train_loss = 1.34205999, grad/param norm = 9.3706e-02, time/batch = 0.1976s	
866/2700 (epoch 16.037), train_loss = 1.27825719, grad/param norm = 8.1556e-02, time/batch = 0.1966s	
867/2700 (epoch 16.056), train_loss = 1.20839996, grad/param norm = 7.6050e-02, time/batch = 0.1968s	
868/2700 (epoch 16.074), train_loss = 1.23403157, grad/param norm = 8.6419e-02, time/batch = 0.1974s	
869/2700 (epoch 16.093), train_loss = 1.21380782, grad/param norm = 8.3839e-02, time/batch = 0.1981s	
870/2700 (epoch 16.111), train_loss = 1.19499762, grad/param norm = 8.0930e-02, time/batch = 0.1978s	
871/2700 (epoch 16.130), train_loss = 1.21950824, grad/param norm = 7.4661e-02, time/batch = 0.1982s	
872/2700 (epoch 16.148), train_loss = 1.16640135, grad/param norm = 6.9403e-02, time/batch = 0.1971s	
873/2700 (epoch 16.167), train_loss = 1.24839237, grad/param norm = 8.0424e-02, time/batch = 0.1967s	
874/2700 (epoch 16.185), train_loss = 1.19032701, grad/param norm = 7.6394e-02, time/batch = 0.1978s	
875/2700 (epoch 16.204), train_loss = 1.21569958, grad/param norm = 8.3115e-02, time/batch = 0.1978s	
876/2700 (epoch 16.222), train_loss = 1.18792704, grad/param norm = 8.4704e-02, time/batch = 0.1965s	
877/2700 (epoch 16.241), train_loss = 1.16448567, grad/param norm = 9.2695e-02, time/batch = 0.1969s	
878/2700 (epoch 16.259), train_loss = 1.16726786, grad/param norm = 6.6618e-02, time/batch = 0.1969s	
879/2700 (epoch 16.278), train_loss = 1.21692408, grad/param norm = 6.9805e-02, time/batch = 0.1969s	
880/2700 (epoch 16.296), train_loss = 1.22539377, grad/param norm = 8.1060e-02, time/batch = 0.1975s	
881/2700 (epoch 16.315), train_loss = 1.18891518, grad/param norm = 9.9787e-02, time/batch = 0.1986s	
882/2700 (epoch 16.333), train_loss = 1.22526888, grad/param norm = 1.1657e-01, time/batch = 0.1972s	
883/2700 (epoch 16.352), train_loss = 1.19148699, grad/param norm = 9.1949e-02, time/batch = 0.1971s	
884/2700 (epoch 16.370), train_loss = 1.16985606, grad/param norm = 7.4614e-02, time/batch = 0.1971s	
885/2700 (epoch 16.389), train_loss = 1.16000466, grad/param norm = 7.4055e-02, time/batch = 0.1974s	
886/2700 (epoch 16.407), train_loss = 1.23874886, grad/param norm = 7.1025e-02, time/batch = 0.1966s	
887/2700 (epoch 16.426), train_loss = 1.24614286, grad/param norm = 7.9871e-02, time/batch = 0.1974s	
888/2700 (epoch 16.444), train_loss = 1.19031608, grad/param norm = 8.4894e-02, time/batch = 0.1975s	
889/2700 (epoch 16.463), train_loss = 1.24436677, grad/param norm = 1.1341e-01, time/batch = 0.1976s	
890/2700 (epoch 16.481), train_loss = 1.21470793, grad/param norm = 8.9100e-02, time/batch = 0.1973s	
891/2700 (epoch 16.500), train_loss = 1.16077333, grad/param norm = 7.5721e-02, time/batch = 0.1979s	
892/2700 (epoch 16.519), train_loss = 1.23120054, grad/param norm = 7.3920e-02, time/batch = 0.1971s	
893/2700 (epoch 16.537), train_loss = 1.18467759, grad/param norm = 7.0376e-02, time/batch = 0.1973s	
894/2700 (epoch 16.556), train_loss = 1.15578678, grad/param norm = 7.8264e-02, time/batch = 0.1968s	
895/2700 (epoch 16.574), train_loss = 1.14895472, grad/param norm = 8.3575e-02, time/batch = 0.1975s	
896/2700 (epoch 16.593), train_loss = 1.20711439, grad/param norm = 9.2516e-02, time/batch = 0.1969s	
897/2700 (epoch 16.611), train_loss = 1.13411710, grad/param norm = 7.5950e-02, time/batch = 0.1974s	
898/2700 (epoch 16.630), train_loss = 1.13654141, grad/param norm = 7.7531e-02, time/batch = 0.1968s	
899/2700 (epoch 16.648), train_loss = 1.18343029, grad/param norm = 8.6558e-02, time/batch = 0.1970s	
900/2700 (epoch 16.667), train_loss = 1.16129930, grad/param norm = 8.0178e-02, time/batch = 0.1972s	
901/2700 (epoch 16.685), train_loss = 1.19168962, grad/param norm = 9.4212e-02, time/batch = 0.1977s	
902/2700 (epoch 16.704), train_loss = 1.22509694, grad/param norm = 1.0261e-01, time/batch = 0.1968s	
903/2700 (epoch 16.722), train_loss = 1.18472116, grad/param norm = 7.2748e-02, time/batch = 0.1969s	
904/2700 (epoch 16.741), train_loss = 1.13496577, grad/param norm = 7.0635e-02, time/batch = 0.1976s	
905/2700 (epoch 16.759), train_loss = 1.11651164, grad/param norm = 7.6781e-02, time/batch = 0.1979s	
906/2700 (epoch 16.778), train_loss = 1.20686927, grad/param norm = 1.0069e-01, time/batch = 0.1977s	
907/2700 (epoch 16.796), train_loss = 1.15082341, grad/param norm = 8.7934e-02, time/batch = 0.1971s	
908/2700 (epoch 16.815), train_loss = 1.18622380, grad/param norm = 8.0960e-02, time/batch = 0.1969s	
909/2700 (epoch 16.833), train_loss = 1.18498458, grad/param norm = 8.5048e-02, time/batch = 0.1969s	
910/2700 (epoch 16.852), train_loss = 1.17493158, grad/param norm = 1.0519e-01, time/batch = 0.1975s	
911/2700 (epoch 16.870), train_loss = 1.17643395, grad/param norm = 9.7404e-02, time/batch = 0.1977s	
912/2700 (epoch 16.889), train_loss = 1.17234399, grad/param norm = 9.8129e-02, time/batch = 0.1976s	
913/2700 (epoch 16.907), train_loss = 1.22666389, grad/param norm = 1.0727e-01, time/batch = 0.1968s	
914/2700 (epoch 16.926), train_loss = 1.22771969, grad/param norm = 1.2343e-01, time/batch = 0.1973s	
915/2700 (epoch 16.944), train_loss = 1.19056396, grad/param norm = 9.7840e-02, time/batch = 0.1975s	
916/2700 (epoch 16.963), train_loss = 1.17315552, grad/param norm = 8.4584e-02, time/batch = 0.1969s	
917/2700 (epoch 16.981), train_loss = 1.12440672, grad/param norm = 7.8237e-02, time/batch = 0.1970s	
decayed learning rate by a factor 0.97 to 0.0015674867188754	
918/2700 (epoch 17.000), train_loss = 1.17280380, grad/param norm = 8.6163e-02, time/batch = 0.1967s	
919/2700 (epoch 17.019), train_loss = 1.24538165, grad/param norm = 8.4698e-02, time/batch = 0.1976s	
920/2700 (epoch 17.037), train_loss = 1.18502490, grad/param norm = 8.2878e-02, time/batch = 0.1975s	
921/2700 (epoch 17.056), train_loss = 1.13028146, grad/param norm = 8.4024e-02, time/batch = 0.1978s	
922/2700 (epoch 17.074), train_loss = 1.12513040, grad/param norm = 7.7354e-02, time/batch = 0.1973s	
923/2700 (epoch 17.093), train_loss = 1.10435473, grad/param norm = 6.9206e-02, time/batch = 0.1972s	
924/2700 (epoch 17.111), train_loss = 1.10368680, grad/param norm = 7.0382e-02, time/batch = 0.1970s	
925/2700 (epoch 17.130), train_loss = 1.13020255, grad/param norm = 7.7640e-02, time/batch = 0.1976s	
926/2700 (epoch 17.148), train_loss = 1.08672678, grad/param norm = 6.9922e-02, time/batch = 0.1970s	
927/2700 (epoch 17.167), train_loss = 1.13453348, grad/param norm = 7.3435e-02, time/batch = 0.1969s	
928/2700 (epoch 17.185), train_loss = 1.10979430, grad/param norm = 8.8962e-02, time/batch = 0.1975s	
929/2700 (epoch 17.204), train_loss = 1.14059649, grad/param norm = 9.1127e-02, time/batch = 0.1984s	
930/2700 (epoch 17.222), train_loss = 1.10287243, grad/param norm = 8.6575e-02, time/batch = 0.1976s	
931/2700 (epoch 17.241), train_loss = 1.06722951, grad/param norm = 7.8490e-02, time/batch = 0.1987s	
932/2700 (epoch 17.259), train_loss = 1.09957526, grad/param norm = 9.3051e-02, time/batch = 0.1973s	
933/2700 (epoch 17.278), train_loss = 1.16146260, grad/param norm = 9.5217e-02, time/batch = 0.1976s	
934/2700 (epoch 17.296), train_loss = 1.12947620, grad/param norm = 8.5299e-02, time/batch = 0.1984s	
935/2700 (epoch 17.315), train_loss = 1.07202490, grad/param norm = 8.2962e-02, time/batch = 0.1984s	
936/2700 (epoch 17.333), train_loss = 1.11838201, grad/param norm = 8.9416e-02, time/batch = 0.1974s	
937/2700 (epoch 17.352), train_loss = 1.11654659, grad/param norm = 1.0220e-01, time/batch = 0.1975s	
938/2700 (epoch 17.370), train_loss = 1.14457263, grad/param norm = 1.1855e-01, time/batch = 0.1972s	
939/2700 (epoch 17.389), train_loss = 1.09827876, grad/param norm = 9.6864e-02, time/batch = 0.1976s	
940/2700 (epoch 17.407), train_loss = 1.15768488, grad/param norm = 7.7906e-02, time/batch = 0.1983s	
941/2700 (epoch 17.426), train_loss = 1.14261896, grad/param norm = 7.3504e-02, time/batch = 0.1980s	
942/2700 (epoch 17.444), train_loss = 1.08037251, grad/param norm = 7.3907e-02, time/batch = 0.1970s	
943/2700 (epoch 17.463), train_loss = 1.11545183, grad/param norm = 7.9746e-02, time/batch = 0.1970s	
944/2700 (epoch 17.481), train_loss = 1.09930027, grad/param norm = 8.5157e-02, time/batch = 0.1981s	
945/2700 (epoch 17.500), train_loss = 1.08387674, grad/param norm = 9.0969e-02, time/batch = 0.1976s	
946/2700 (epoch 17.519), train_loss = 1.14713378, grad/param norm = 8.9297e-02, time/batch = 0.1974s	
947/2700 (epoch 17.537), train_loss = 1.10570128, grad/param norm = 8.7410e-02, time/batch = 0.1978s	
948/2700 (epoch 17.556), train_loss = 1.07509944, grad/param norm = 8.7473e-02, time/batch = 0.1979s	
949/2700 (epoch 17.574), train_loss = 1.05000585, grad/param norm = 7.8029e-02, time/batch = 0.1979s	
950/2700 (epoch 17.593), train_loss = 1.08969546, grad/param norm = 7.8014e-02, time/batch = 0.1982s	
951/2700 (epoch 17.611), train_loss = 1.04494122, grad/param norm = 8.1496e-02, time/batch = 0.1988s	
952/2700 (epoch 17.630), train_loss = 1.06635663, grad/param norm = 9.3365e-02, time/batch = 0.1974s	
953/2700 (epoch 17.648), train_loss = 1.08028195, grad/param norm = 8.5905e-02, time/batch = 0.1979s	
954/2700 (epoch 17.667), train_loss = 1.08183772, grad/param norm = 8.2039e-02, time/batch = 0.1976s	
955/2700 (epoch 17.685), train_loss = 1.11636475, grad/param norm = 9.7336e-02, time/batch = 0.1985s	
956/2700 (epoch 17.704), train_loss = 1.11823173, grad/param norm = 8.4593e-02, time/batch = 0.1971s	
957/2700 (epoch 17.722), train_loss = 1.11373477, grad/param norm = 9.0527e-02, time/batch = 0.1980s	
958/2700 (epoch 17.741), train_loss = 1.04935224, grad/param norm = 7.7975e-02, time/batch = 0.1974s	
959/2700 (epoch 17.759), train_loss = 1.00019260, grad/param norm = 6.8452e-02, time/batch = 0.1972s	
960/2700 (epoch 17.778), train_loss = 1.07921674, grad/param norm = 7.6450e-02, time/batch = 0.1978s	
961/2700 (epoch 17.796), train_loss = 1.02967715, grad/param norm = 7.9872e-02, time/batch = 0.1982s	
962/2700 (epoch 17.815), train_loss = 1.08143467, grad/param norm = 9.0993e-02, time/batch = 0.1970s	
963/2700 (epoch 17.833), train_loss = 1.10796210, grad/param norm = 1.0398e-01, time/batch = 0.1973s	
964/2700 (epoch 17.852), train_loss = 1.07213258, grad/param norm = 8.9834e-02, time/batch = 0.1975s	
965/2700 (epoch 17.870), train_loss = 1.07583933, grad/param norm = 8.9067e-02, time/batch = 0.1973s	
966/2700 (epoch 17.889), train_loss = 1.09288050, grad/param norm = 9.3692e-02, time/batch = 0.1971s	
967/2700 (epoch 17.907), train_loss = 1.11199496, grad/param norm = 9.2414e-02, time/batch = 0.1975s	
968/2700 (epoch 17.926), train_loss = 1.09397726, grad/param norm = 9.8090e-02, time/batch = 0.1968s	
969/2700 (epoch 17.944), train_loss = 1.09325394, grad/param norm = 1.1778e-01, time/batch = 0.1974s	
970/2700 (epoch 17.963), train_loss = 1.09253145, grad/param norm = 9.6696e-02, time/batch = 0.1974s	
971/2700 (epoch 17.981), train_loss = 1.04321933, grad/param norm = 8.3414e-02, time/batch = 0.1982s	
decayed learning rate by a factor 0.97 to 0.0015204621173091	
972/2700 (epoch 18.000), train_loss = 1.06515931, grad/param norm = 8.3376e-02, time/batch = 0.1971s	
973/2700 (epoch 18.019), train_loss = 1.13706311, grad/param norm = 8.3414e-02, time/batch = 0.1968s	
974/2700 (epoch 18.037), train_loss = 1.09476155, grad/param norm = 8.7886e-02, time/batch = 0.1977s	
975/2700 (epoch 18.056), train_loss = 1.04224014, grad/param norm = 8.7374e-02, time/batch = 0.1978s	
976/2700 (epoch 18.074), train_loss = 1.04681412, grad/param norm = 8.9081e-02, time/batch = 0.1973s	
977/2700 (epoch 18.093), train_loss = 1.03775314, grad/param norm = 9.3041e-02, time/batch = 0.1971s	
978/2700 (epoch 18.111), train_loss = 1.01433147, grad/param norm = 7.7538e-02, time/batch = 0.1972s	
979/2700 (epoch 18.130), train_loss = 1.02754429, grad/param norm = 7.6233e-02, time/batch = 0.1976s	
980/2700 (epoch 18.148), train_loss = 1.01059529, grad/param norm = 7.9717e-02, time/batch = 0.1973s	
981/2700 (epoch 18.167), train_loss = 1.05902354, grad/param norm = 8.8708e-02, time/batch = 0.1987s	
982/2700 (epoch 18.185), train_loss = 1.03419725, grad/param norm = 8.5885e-02, time/batch = 0.1971s	
983/2700 (epoch 18.204), train_loss = 1.04993122, grad/param norm = 9.3350e-02, time/batch = 0.1974s	
984/2700 (epoch 18.222), train_loss = 1.03788031, grad/param norm = 1.0899e-01, time/batch = 0.1971s	
985/2700 (epoch 18.241), train_loss = 0.99823953, grad/param norm = 8.8887e-02, time/batch = 0.1977s	
986/2700 (epoch 18.259), train_loss = 0.99787528, grad/param norm = 7.6370e-02, time/batch = 0.1970s	
987/2700 (epoch 18.278), train_loss = 1.03317696, grad/param norm = 7.4610e-02, time/batch = 0.1976s	
988/2700 (epoch 18.296), train_loss = 1.02142889, grad/param norm = 8.0103e-02, time/batch = 0.1968s	
989/2700 (epoch 18.315), train_loss = 0.96663065, grad/param norm = 8.0469e-02, time/batch = 0.1971s	
990/2700 (epoch 18.333), train_loss = 1.00615468, grad/param norm = 8.7993e-02, time/batch = 0.1972s	
991/2700 (epoch 18.352), train_loss = 0.99593170, grad/param norm = 9.4091e-02, time/batch = 0.1983s	
992/2700 (epoch 18.370), train_loss = 1.00181714, grad/param norm = 8.5550e-02, time/batch = 0.1972s	
993/2700 (epoch 18.389), train_loss = 0.99176780, grad/param norm = 8.3163e-02, time/batch = 0.1971s	
994/2700 (epoch 18.407), train_loss = 1.07920830, grad/param norm = 1.0653e-01, time/batch = 0.1982s	
995/2700 (epoch 18.426), train_loss = 1.08540618, grad/param norm = 9.0835e-02, time/batch = 0.1977s	
996/2700 (epoch 18.444), train_loss = 1.00303067, grad/param norm = 8.3835e-02, time/batch = 0.1965s	
997/2700 (epoch 18.463), train_loss = 1.04918007, grad/param norm = 9.8646e-02, time/batch = 0.1973s	
998/2700 (epoch 18.481), train_loss = 1.03741808, grad/param norm = 1.1325e-01, time/batch = 0.1980s	
999/2700 (epoch 18.500), train_loss = 0.98542596, grad/param norm = 9.5573e-02, time/batch = 0.1973s	
evaluating loss over split index 2	
1/3...	
2/3...	
3/3...	
saving checkpoint to cv/lm_lstm_epoch18.52_1.7220.t7	
1000/2700 (epoch 18.519), train_loss = 1.03463430, grad/param norm = 8.0108e-02, time/batch = 0.1975s	
1001/2700 (epoch 18.537), train_loss = 1.27805684, grad/param norm = 9.1608e-02, time/batch = 0.1988s	
1002/2700 (epoch 18.556), train_loss = 0.98468770, grad/param norm = 8.0728e-02, time/batch = 0.1966s	
1003/2700 (epoch 18.574), train_loss = 0.96561850, grad/param norm = 8.7367e-02, time/batch = 0.1973s	
1004/2700 (epoch 18.593), train_loss = 1.01704607, grad/param norm = 8.7916e-02, time/batch = 0.1972s	
1005/2700 (epoch 18.611), train_loss = 0.96141707, grad/param norm = 8.0551e-02, time/batch = 0.1978s	
1006/2700 (epoch 18.630), train_loss = 0.96263480, grad/param norm = 8.0315e-02, time/batch = 0.1970s	
1007/2700 (epoch 18.648), train_loss = 0.99549592, grad/param norm = 8.0708e-02, time/batch = 0.1972s	
1008/2700 (epoch 18.667), train_loss = 0.98955742, grad/param norm = 9.3482e-02, time/batch = 0.1969s	
1009/2700 (epoch 18.685), train_loss = 1.02793655, grad/param norm = 9.3841e-02, time/batch = 0.1979s	
1010/2700 (epoch 18.704), train_loss = 1.05462077, grad/param norm = 1.0752e-01, time/batch = 0.1972s	
1011/2700 (epoch 18.722), train_loss = 1.03338324, grad/param norm = 8.8863e-02, time/batch = 0.1977s	
1012/2700 (epoch 18.741), train_loss = 0.97400448, grad/param norm = 7.8422e-02, time/batch = 0.1972s	
1013/2700 (epoch 18.759), train_loss = 0.91604828, grad/param norm = 7.7895e-02, time/batch = 0.1970s	
1014/2700 (epoch 18.778), train_loss = 0.98537038, grad/param norm = 8.0951e-02, time/batch = 0.1982s	
1015/2700 (epoch 18.796), train_loss = 0.93272053, grad/param norm = 1.0065e-01, time/batch = 0.1980s	
1016/2700 (epoch 18.815), train_loss = 0.99892691, grad/param norm = 1.0500e-01, time/batch = 0.1973s	
1017/2700 (epoch 18.833), train_loss = 1.02411494, grad/param norm = 1.0246e-01, time/batch = 0.1974s	
1018/2700 (epoch 18.852), train_loss = 0.98800161, grad/param norm = 9.6319e-02, time/batch = 0.1972s	
1019/2700 (epoch 18.870), train_loss = 0.98707136, grad/param norm = 8.5496e-02, time/batch = 0.1975s	
1020/2700 (epoch 18.889), train_loss = 0.97818788, grad/param norm = 9.2468e-02, time/batch = 0.1973s	
1021/2700 (epoch 18.907), train_loss = 1.00524396, grad/param norm = 9.3172e-02, time/batch = 0.1982s	
1022/2700 (epoch 18.926), train_loss = 0.96866708, grad/param norm = 8.0685e-02, time/batch = 0.1969s	
1023/2700 (epoch 18.944), train_loss = 0.96885983, grad/param norm = 8.9176e-02, time/batch = 0.1971s	
1024/2700 (epoch 18.963), train_loss = 0.99530671, grad/param norm = 9.5020e-02, time/batch = 0.1971s	
1025/2700 (epoch 18.981), train_loss = 0.96938738, grad/param norm = 9.5886e-02, time/batch = 0.1975s	
decayed learning rate by a factor 0.97 to 0.0014748482537899	
1026/2700 (epoch 19.000), train_loss = 0.96622977, grad/param norm = 8.7834e-02, time/batch = 0.1967s	
1027/2700 (epoch 19.019), train_loss = 1.04424763, grad/param norm = 8.6515e-02, time/batch = 0.1975s	
1028/2700 (epoch 19.037), train_loss = 0.99962999, grad/param norm = 9.4161e-02, time/batch = 0.1966s	
1029/2700 (epoch 19.056), train_loss = 0.95646629, grad/param norm = 9.2062e-02, time/batch = 0.1973s	
1030/2700 (epoch 19.074), train_loss = 0.95148395, grad/param norm = 8.4625e-02, time/batch = 0.1975s	
1031/2700 (epoch 19.093), train_loss = 0.94113989, grad/param norm = 8.6156e-02, time/batch = 0.1977s	
1032/2700 (epoch 19.111), train_loss = 0.95202841, grad/param norm = 9.3043e-02, time/batch = 0.1970s	
1033/2700 (epoch 19.130), train_loss = 0.97342749, grad/param norm = 1.1521e-01, time/batch = 0.1974s	
1034/2700 (epoch 19.148), train_loss = 0.93356928, grad/param norm = 9.7765e-02, time/batch = 0.1976s	
1035/2700 (epoch 19.167), train_loss = 0.95257242, grad/param norm = 8.5521e-02, time/batch = 0.1976s	
1036/2700 (epoch 19.185), train_loss = 0.91538793, grad/param norm = 7.6982e-02, time/batch = 0.1970s	
1037/2700 (epoch 19.204), train_loss = 0.94430321, grad/param norm = 8.8086e-02, time/batch = 0.1971s	
1038/2700 (epoch 19.222), train_loss = 0.92237857, grad/param norm = 8.1815e-02, time/batch = 0.1969s	
1039/2700 (epoch 19.241), train_loss = 0.90515871, grad/param norm = 8.9075e-02, time/batch = 0.1970s	
1040/2700 (epoch 19.259), train_loss = 0.93174620, grad/param norm = 1.0164e-01, time/batch = 0.1981s	
1041/2700 (epoch 19.278), train_loss = 0.95758730, grad/param norm = 8.8964e-02, time/batch = 0.1980s	
1042/2700 (epoch 19.296), train_loss = 0.93718625, grad/param norm = 8.4708e-02, time/batch = 0.1970s	
1043/2700 (epoch 19.315), train_loss = 0.86791093, grad/param norm = 7.9918e-02, time/batch = 0.1968s	
1044/2700 (epoch 19.333), train_loss = 0.90480295, grad/param norm = 8.7400e-02, time/batch = 0.1974s	
1045/2700 (epoch 19.352), train_loss = 0.90714322, grad/param norm = 9.0458e-02, time/batch = 0.1977s	
1046/2700 (epoch 19.370), train_loss = 0.88966493, grad/param norm = 8.8820e-02, time/batch = 0.1975s	
1047/2700 (epoch 19.389), train_loss = 0.89878287, grad/param norm = 9.4725e-02, time/batch = 0.1970s	
1048/2700 (epoch 19.407), train_loss = 0.94446156, grad/param norm = 8.2326e-02, time/batch = 0.1973s	
1049/2700 (epoch 19.426), train_loss = 0.95459190, grad/param norm = 8.7564e-02, time/batch = 0.1976s	
1050/2700 (epoch 19.444), train_loss = 0.92223645, grad/param norm = 8.3642e-02, time/batch = 0.1979s	
1051/2700 (epoch 19.463), train_loss = 0.93248636, grad/param norm = 9.0669e-02, time/batch = 0.1984s	
1052/2700 (epoch 19.481), train_loss = 0.92601212, grad/param norm = 9.1337e-02, time/batch = 0.1970s	
1053/2700 (epoch 19.500), train_loss = 0.89134224, grad/param norm = 9.2613e-02, time/batch = 0.1970s	
1054/2700 (epoch 19.519), train_loss = 0.95728232, grad/param norm = 9.2572e-02, time/batch = 0.1971s	
1055/2700 (epoch 19.537), train_loss = 0.94284523, grad/param norm = 8.6156e-02, time/batch = 0.1977s	
1056/2700 (epoch 19.556), train_loss = 0.88502242, grad/param norm = 8.7419e-02, time/batch = 0.1969s	
1057/2700 (epoch 19.574), train_loss = 0.85266343, grad/param norm = 7.7255e-02, time/batch = 0.1972s	
1058/2700 (epoch 19.593), train_loss = 0.91454309, grad/param norm = 8.6073e-02, time/batch = 0.1968s	
1059/2700 (epoch 19.611), train_loss = 0.89703557, grad/param norm = 1.0486e-01, time/batch = 0.1973s	
1060/2700 (epoch 19.630), train_loss = 0.92120831, grad/param norm = 1.1862e-01, time/batch = 0.1978s	
1061/2700 (epoch 19.648), train_loss = 0.93802328, grad/param norm = 1.0358e-01, time/batch = 0.1981s	
1062/2700 (epoch 19.667), train_loss = 0.88562115, grad/param norm = 8.8523e-02, time/batch = 0.1971s	
1063/2700 (epoch 19.685), train_loss = 0.91163797, grad/param norm = 8.4543e-02, time/batch = 0.1968s	
1064/2700 (epoch 19.704), train_loss = 0.94159702, grad/param norm = 9.5673e-02, time/batch = 0.1974s	
1065/2700 (epoch 19.722), train_loss = 0.94221485, grad/param norm = 9.6434e-02, time/batch = 0.1983s	
1066/2700 (epoch 19.741), train_loss = 0.90149564, grad/param norm = 8.6487e-02, time/batch = 0.1978s	
1067/2700 (epoch 19.759), train_loss = 0.84606738, grad/param norm = 8.7871e-02, time/batch = 0.1974s	
1068/2700 (epoch 19.778), train_loss = 0.90288287, grad/param norm = 8.9256e-02, time/batch = 0.1979s	
1069/2700 (epoch 19.796), train_loss = 0.83431896, grad/param norm = 8.8887e-02, time/batch = 0.1973s	
1070/2700 (epoch 19.815), train_loss = 0.90150665, grad/param norm = 1.1321e-01, time/batch = 0.1974s	
1071/2700 (epoch 19.833), train_loss = 0.94393090, grad/param norm = 1.2281e-01, time/batch = 0.1982s	
1072/2700 (epoch 19.852), train_loss = 0.92112904, grad/param norm = 1.0806e-01, time/batch = 0.1971s	
1073/2700 (epoch 19.870), train_loss = 0.89823309, grad/param norm = 9.0073e-02, time/batch = 0.1969s	
1074/2700 (epoch 19.889), train_loss = 0.87243220, grad/param norm = 8.2709e-02, time/batch = 0.1980s	
1075/2700 (epoch 19.907), train_loss = 0.87604230, grad/param norm = 8.6152e-02, time/batch = 0.1983s	
1076/2700 (epoch 19.926), train_loss = 0.87538038, grad/param norm = 8.7926e-02, time/batch = 0.1972s	
1077/2700 (epoch 19.944), train_loss = 0.88097888, grad/param norm = 1.0491e-01, time/batch = 0.1978s	
1078/2700 (epoch 19.963), train_loss = 0.87402751, grad/param norm = 8.3818e-02, time/batch = 0.1976s	
1079/2700 (epoch 19.981), train_loss = 0.84827680, grad/param norm = 7.8166e-02, time/batch = 0.1977s	
decayed learning rate by a factor 0.97 to 0.0014306028061762	
1080/2700 (epoch 20.000), train_loss = 0.84377008, grad/param norm = 7.3902e-02, time/batch = 0.1978s	
1081/2700 (epoch 20.019), train_loss = 0.94389438, grad/param norm = 8.3835e-02, time/batch = 0.1988s	
1082/2700 (epoch 20.037), train_loss = 0.88330574, grad/param norm = 8.8279e-02, time/batch = 0.1977s	
1083/2700 (epoch 20.056), train_loss = 0.88307675, grad/param norm = 1.0838e-01, time/batch = 0.1979s	
1084/2700 (epoch 20.074), train_loss = 0.88276291, grad/param norm = 9.8736e-02, time/batch = 0.1977s	
1085/2700 (epoch 20.093), train_loss = 0.83485872, grad/param norm = 8.4333e-02, time/batch = 0.1983s	
1086/2700 (epoch 20.111), train_loss = 0.86189551, grad/param norm = 9.9228e-02, time/batch = 0.1971s	
1087/2700 (epoch 20.130), train_loss = 0.85727889, grad/param norm = 7.9386e-02, time/batch = 0.1977s	
1088/2700 (epoch 20.148), train_loss = 0.82459665, grad/param norm = 7.4329e-02, time/batch = 0.1974s	
1089/2700 (epoch 20.167), train_loss = 0.85445741, grad/param norm = 8.5139e-02, time/batch = 0.1979s	
1090/2700 (epoch 20.185), train_loss = 0.84633066, grad/param norm = 9.0733e-02, time/batch = 0.1985s	
1091/2700 (epoch 20.204), train_loss = 0.84096874, grad/param norm = 8.7700e-02, time/batch = 0.1980s	
1092/2700 (epoch 20.222), train_loss = 0.83646974, grad/param norm = 9.3578e-02, time/batch = 0.1977s	
1093/2700 (epoch 20.241), train_loss = 0.83618629, grad/param norm = 1.1145e-01, time/batch = 0.1977s	
1094/2700 (epoch 20.259), train_loss = 0.84474441, grad/param norm = 9.7146e-02, time/batch = 0.1980s	
1095/2700 (epoch 20.278), train_loss = 0.88248895, grad/param norm = 9.4542e-02, time/batch = 0.1982s	
1096/2700 (epoch 20.296), train_loss = 0.87845876, grad/param norm = 1.0507e-01, time/batch = 0.1975s	
1097/2700 (epoch 20.315), train_loss = 0.79872302, grad/param norm = 9.2397e-02, time/batch = 0.1977s	
1098/2700 (epoch 20.333), train_loss = 0.81996825, grad/param norm = 9.4244e-02, time/batch = 0.1967s	
1099/2700 (epoch 20.352), train_loss = 0.81645540, grad/param norm = 8.8502e-02, time/batch = 0.1979s	
1100/2700 (epoch 20.370), train_loss = 0.80241337, grad/param norm = 9.4999e-02, time/batch = 0.1983s	
1101/2700 (epoch 20.389), train_loss = 0.82411439, grad/param norm = 1.0046e-01, time/batch = 0.1986s	
1102/2700 (epoch 20.407), train_loss = 0.87955670, grad/param norm = 9.7585e-02, time/batch = 0.1973s	
1103/2700 (epoch 20.426), train_loss = 0.87689355, grad/param norm = 1.0791e-01, time/batch = 0.1969s	
1104/2700 (epoch 20.444), train_loss = 0.84574808, grad/param norm = 9.0060e-02, time/batch = 0.1970s	
1105/2700 (epoch 20.463), train_loss = 0.82721812, grad/param norm = 8.9159e-02, time/batch = 0.1983s	
1106/2700 (epoch 20.481), train_loss = 0.83617157, grad/param norm = 1.1882e-01, time/batch = 0.1972s	
1107/2700 (epoch 20.500), train_loss = 0.79446944, grad/param norm = 8.9222e-02, time/batch = 0.1974s	
1108/2700 (epoch 20.519), train_loss = 0.82406511, grad/param norm = 7.9562e-02, time/batch = 0.1980s	
1109/2700 (epoch 20.537), train_loss = 0.85189313, grad/param norm = 8.2753e-02, time/batch = 0.1979s	
1110/2700 (epoch 20.556), train_loss = 0.79549829, grad/param norm = 8.6297e-02, time/batch = 0.1978s	
1111/2700 (epoch 20.574), train_loss = 0.78784231, grad/param norm = 9.3812e-02, time/batch = 0.1988s	
1112/2700 (epoch 20.593), train_loss = 0.82211596, grad/param norm = 9.8866e-02, time/batch = 0.1974s	
1113/2700 (epoch 20.611), train_loss = 0.78074882, grad/param norm = 8.9596e-02, time/batch = 0.1972s	
1114/2700 (epoch 20.630), train_loss = 0.80281291, grad/param norm = 8.8581e-02, time/batch = 0.1978s	
1115/2700 (epoch 20.648), train_loss = 0.86699943, grad/param norm = 1.1446e-01, time/batch = 0.1982s	
1116/2700 (epoch 20.667), train_loss = 0.87194005, grad/param norm = 1.2351e-01, time/batch = 0.1969s	
1117/2700 (epoch 20.685), train_loss = 0.86355596, grad/param norm = 1.2294e-01, time/batch = 0.1982s	
1118/2700 (epoch 20.704), train_loss = 0.86486095, grad/param norm = 1.0572e-01, time/batch = 0.1971s	
1119/2700 (epoch 20.722), train_loss = 0.83064574, grad/param norm = 8.4993e-02, time/batch = 0.1975s	
1120/2700 (epoch 20.741), train_loss = 0.81658206, grad/param norm = 8.5578e-02, time/batch = 0.1981s	
1121/2700 (epoch 20.759), train_loss = 0.74997217, grad/param norm = 7.7567e-02, time/batch = 0.1980s	
1122/2700 (epoch 20.778), train_loss = 0.78898087, grad/param norm = 7.8930e-02, time/batch = 0.1976s	
1123/2700 (epoch 20.796), train_loss = 0.76556548, grad/param norm = 1.0193e-01, time/batch = 0.1981s	
1124/2700 (epoch 20.815), train_loss = 0.80671902, grad/param norm = 9.7665e-02, time/batch = 0.1972s	
1125/2700 (epoch 20.833), train_loss = 0.82109950, grad/param norm = 9.0612e-02, time/batch = 0.1980s	
1126/2700 (epoch 20.852), train_loss = 0.78702491, grad/param norm = 8.6093e-02, time/batch = 0.1976s	
1127/2700 (epoch 20.870), train_loss = 0.78641033, grad/param norm = 8.1969e-02, time/batch = 0.1979s	
1128/2700 (epoch 20.889), train_loss = 0.78031316, grad/param norm = 9.2337e-02, time/batch = 0.1974s	
1129/2700 (epoch 20.907), train_loss = 0.80034326, grad/param norm = 9.9043e-02, time/batch = 0.1972s	
1130/2700 (epoch 20.926), train_loss = 0.78280206, grad/param norm = 9.3567e-02, time/batch = 0.1975s	
1131/2700 (epoch 20.944), train_loss = 0.76213039, grad/param norm = 8.3811e-02, time/batch = 0.1982s	
1132/2700 (epoch 20.963), train_loss = 0.80264170, grad/param norm = 9.6685e-02, time/batch = 0.1967s	
1133/2700 (epoch 20.981), train_loss = 0.78341420, grad/param norm = 8.9716e-02, time/batch = 0.1973s	
decayed learning rate by a factor 0.97 to 0.0013876847219909	
1134/2700 (epoch 21.000), train_loss = 0.76167731, grad/param norm = 8.9803e-02, time/batch = 0.1977s	
1135/2700 (epoch 21.019), train_loss = 0.84985382, grad/param norm = 8.5501e-02, time/batch = 0.1979s	
1136/2700 (epoch 21.037), train_loss = 0.76045269, grad/param norm = 7.2563e-02, time/batch = 0.1973s	
1137/2700 (epoch 21.056), train_loss = 0.76123589, grad/param norm = 8.2668e-02, time/batch = 0.1977s	
1138/2700 (epoch 21.074), train_loss = 0.76325644, grad/param norm = 8.7412e-02, time/batch = 0.1973s	
1139/2700 (epoch 21.093), train_loss = 0.76147791, grad/param norm = 9.6270e-02, time/batch = 0.1976s	
1140/2700 (epoch 21.111), train_loss = 0.76503907, grad/param norm = 8.7572e-02, time/batch = 0.1978s	
1141/2700 (epoch 21.130), train_loss = 0.78280256, grad/param norm = 1.0242e-01, time/batch = 0.1982s	
1142/2700 (epoch 21.148), train_loss = 0.76181154, grad/param norm = 9.3369e-02, time/batch = 0.1971s	
1143/2700 (epoch 21.167), train_loss = 0.77061305, grad/param norm = 1.0337e-01, time/batch = 0.1969s	
1144/2700 (epoch 21.185), train_loss = 0.75553941, grad/param norm = 8.7296e-02, time/batch = 0.1977s	
1145/2700 (epoch 21.204), train_loss = 0.74673405, grad/param norm = 8.2033e-02, time/batch = 0.1980s	
1146/2700 (epoch 21.222), train_loss = 0.74301046, grad/param norm = 9.3084e-02, time/batch = 0.1975s	
1147/2700 (epoch 21.241), train_loss = 0.73043036, grad/param norm = 8.8183e-02, time/batch = 0.1977s	
1148/2700 (epoch 21.259), train_loss = 0.75693250, grad/param norm = 9.7696e-02, time/batch = 0.1971s	
1149/2700 (epoch 21.278), train_loss = 0.79015397, grad/param norm = 1.0946e-01, time/batch = 0.1976s	
1150/2700 (epoch 21.296), train_loss = 0.76466687, grad/param norm = 9.6694e-02, time/batch = 0.1975s	
1151/2700 (epoch 21.315), train_loss = 0.72414032, grad/param norm = 9.2411e-02, time/batch = 0.1976s	
1152/2700 (epoch 21.333), train_loss = 0.72115657, grad/param norm = 8.5941e-02, time/batch = 0.1973s	
1153/2700 (epoch 21.352), train_loss = 0.71970363, grad/param norm = 8.9505e-02, time/batch = 0.1975s	
1154/2700 (epoch 21.370), train_loss = 0.71870418, grad/param norm = 9.4887e-02, time/batch = 0.1982s	
1155/2700 (epoch 21.389), train_loss = 0.72402746, grad/param norm = 8.9647e-02, time/batch = 0.1980s	
1156/2700 (epoch 21.407), train_loss = 0.77077997, grad/param norm = 9.0172e-02, time/batch = 0.1974s	
1157/2700 (epoch 21.426), train_loss = 0.74943676, grad/param norm = 9.3420e-02, time/batch = 0.1974s	
1158/2700 (epoch 21.444), train_loss = 0.77050597, grad/param norm = 1.0820e-01, time/batch = 0.1971s	
1159/2700 (epoch 21.463), train_loss = 0.79464433, grad/param norm = 1.0986e-01, time/batch = 0.1976s	
1160/2700 (epoch 21.481), train_loss = 0.76322319, grad/param norm = 1.1283e-01, time/batch = 0.1977s	
1161/2700 (epoch 21.500), train_loss = 0.72428350, grad/param norm = 9.1018e-02, time/batch = 0.1984s	
1162/2700 (epoch 21.519), train_loss = 0.75467958, grad/param norm = 9.4218e-02, time/batch = 0.1972s	
1163/2700 (epoch 21.537), train_loss = 0.76316444, grad/param norm = 9.1285e-02, time/batch = 0.1972s	
1164/2700 (epoch 21.556), train_loss = 0.69413500, grad/param norm = 7.7438e-02, time/batch = 0.1974s	
1165/2700 (epoch 21.574), train_loss = 0.70391764, grad/param norm = 1.0025e-01, time/batch = 0.1977s	
1166/2700 (epoch 21.593), train_loss = 0.76401537, grad/param norm = 9.8466e-02, time/batch = 0.1970s	
1167/2700 (epoch 21.611), train_loss = 0.71709739, grad/param norm = 1.0035e-01, time/batch = 0.1971s	
1168/2700 (epoch 21.630), train_loss = 0.71862577, grad/param norm = 8.9254e-02, time/batch = 0.1971s	
1169/2700 (epoch 21.648), train_loss = 0.71728621, grad/param norm = 7.7609e-02, time/batch = 0.1983s	
1170/2700 (epoch 21.667), train_loss = 0.68937301, grad/param norm = 7.7587e-02, time/batch = 0.1977s	
1171/2700 (epoch 21.685), train_loss = 0.73147048, grad/param norm = 9.5201e-02, time/batch = 0.1979s	
1172/2700 (epoch 21.704), train_loss = 0.78059218, grad/param norm = 1.2603e-01, time/batch = 0.1968s	
1173/2700 (epoch 21.722), train_loss = 0.77384327, grad/param norm = 9.8525e-02, time/batch = 0.1969s	
1174/2700 (epoch 21.741), train_loss = 0.75257477, grad/param norm = 1.0071e-01, time/batch = 0.1979s	
1175/2700 (epoch 21.759), train_loss = 0.67599756, grad/param norm = 8.2297e-02, time/batch = 0.1979s	
1176/2700 (epoch 21.778), train_loss = 0.72018985, grad/param norm = 9.6702e-02, time/batch = 0.1972s	
1177/2700 (epoch 21.796), train_loss = 0.66395252, grad/param norm = 8.9990e-02, time/batch = 0.1970s	
1178/2700 (epoch 21.815), train_loss = 0.70885546, grad/param norm = 9.6210e-02, time/batch = 0.1975s	
1179/2700 (epoch 21.833), train_loss = 0.73987605, grad/param norm = 1.1005e-01, time/batch = 0.1977s	
1180/2700 (epoch 21.852), train_loss = 0.74526101, grad/param norm = 1.0316e-01, time/batch = 0.1977s	
1181/2700 (epoch 21.870), train_loss = 0.70690751, grad/param norm = 8.5146e-02, time/batch = 0.1978s	
1182/2700 (epoch 21.889), train_loss = 0.69317106, grad/param norm = 8.6676e-02, time/batch = 0.1969s	
1183/2700 (epoch 21.907), train_loss = 0.68254469, grad/param norm = 7.9744e-02, time/batch = 0.1971s	
1184/2700 (epoch 21.926), train_loss = 0.66399324, grad/param norm = 8.2884e-02, time/batch = 0.1972s	
1185/2700 (epoch 21.944), train_loss = 0.66216346, grad/param norm = 9.5114e-02, time/batch = 0.1976s	
1186/2700 (epoch 21.963), train_loss = 0.69769161, grad/param norm = 9.4878e-02, time/batch = 0.1972s	
1187/2700 (epoch 21.981), train_loss = 0.68178242, grad/param norm = 8.6985e-02, time/batch = 0.1973s	
decayed learning rate by a factor 0.97 to 0.0013460541803311	
1188/2700 (epoch 22.000), train_loss = 0.66898973, grad/param norm = 8.6283e-02, time/batch = 0.1972s	
1189/2700 (epoch 22.019), train_loss = 0.75374185, grad/param norm = 9.3917e-02, time/batch = 0.1977s	
1190/2700 (epoch 22.037), train_loss = 0.69828535, grad/param norm = 9.3374e-02, time/batch = 0.1977s	
1191/2700 (epoch 22.056), train_loss = 0.69426742, grad/param norm = 1.0300e-01, time/batch = 0.1979s	
1192/2700 (epoch 22.074), train_loss = 0.69008695, grad/param norm = 9.2450e-02, time/batch = 0.1973s	
1193/2700 (epoch 22.093), train_loss = 0.64428376, grad/param norm = 7.8060e-02, time/batch = 0.1968s	
1194/2700 (epoch 22.111), train_loss = 0.65965538, grad/param norm = 7.8378e-02, time/batch = 0.1975s	
1195/2700 (epoch 22.130), train_loss = 0.66871309, grad/param norm = 9.1552e-02, time/batch = 0.1980s	
1196/2700 (epoch 22.148), train_loss = 0.67498349, grad/param norm = 9.4020e-02, time/batch = 0.1973s	
1197/2700 (epoch 22.167), train_loss = 0.68761502, grad/param norm = 1.1386e-01, time/batch = 0.1976s	
1198/2700 (epoch 22.185), train_loss = 0.69123202, grad/param norm = 1.1681e-01, time/batch = 0.1965s	
1199/2700 (epoch 22.204), train_loss = 0.72376323, grad/param norm = 1.1603e-01, time/batch = 0.1969s	
1200/2700 (epoch 22.222), train_loss = 0.68099952, grad/param norm = 1.0250e-01, time/batch = 0.1974s	
1201/2700 (epoch 22.241), train_loss = 0.66050301, grad/param norm = 9.5427e-02, time/batch = 0.1975s	
1202/2700 (epoch 22.259), train_loss = 0.67323628, grad/param norm = 9.5835e-02, time/batch = 0.1969s	
1203/2700 (epoch 22.278), train_loss = 0.67640487, grad/param norm = 8.5731e-02, time/batch = 0.1969s	
1204/2700 (epoch 22.296), train_loss = 0.66206081, grad/param norm = 8.2758e-02, time/batch = 0.1970s	
1205/2700 (epoch 22.315), train_loss = 0.61710906, grad/param norm = 9.3182e-02, time/batch = 0.1978s	
1206/2700 (epoch 22.333), train_loss = 0.65811069, grad/param norm = 1.1192e-01, time/batch = 0.1976s	
1207/2700 (epoch 22.352), train_loss = 0.63428064, grad/param norm = 9.6411e-02, time/batch = 0.1981s	
1208/2700 (epoch 22.370), train_loss = 0.62178616, grad/param norm = 9.1139e-02, time/batch = 0.1969s	
1209/2700 (epoch 22.389), train_loss = 0.61922524, grad/param norm = 8.2847e-02, time/batch = 0.1975s	
1210/2700 (epoch 22.407), train_loss = 0.68203075, grad/param norm = 8.9137e-02, time/batch = 0.1983s	
1211/2700 (epoch 22.426), train_loss = 0.68644226, grad/param norm = 9.8952e-02, time/batch = 0.1977s	
1212/2700 (epoch 22.444), train_loss = 0.70807725, grad/param norm = 1.2884e-01, time/batch = 0.1981s	
1213/2700 (epoch 22.463), train_loss = 0.69802811, grad/param norm = 1.1367e-01, time/batch = 0.1973s	
1214/2700 (epoch 22.481), train_loss = 0.65694611, grad/param norm = 9.6270e-02, time/batch = 0.1975s	
1215/2700 (epoch 22.500), train_loss = 0.64273686, grad/param norm = 1.0205e-01, time/batch = 0.1984s	
1216/2700 (epoch 22.519), train_loss = 0.65915267, grad/param norm = 8.4717e-02, time/batch = 0.1974s	
1217/2700 (epoch 22.537), train_loss = 0.64490580, grad/param norm = 8.0993e-02, time/batch = 0.1973s	
1218/2700 (epoch 22.556), train_loss = 0.62600613, grad/param norm = 9.3936e-02, time/batch = 0.1970s	
1219/2700 (epoch 22.574), train_loss = 0.62382459, grad/param norm = 9.7533e-02, time/batch = 0.1980s	
1220/2700 (epoch 22.593), train_loss = 0.65814029, grad/param norm = 9.7593e-02, time/batch = 0.1972s	
1221/2700 (epoch 22.611), train_loss = 0.64513267, grad/param norm = 9.8412e-02, time/batch = 0.1983s	
1222/2700 (epoch 22.630), train_loss = 0.66285574, grad/param norm = 1.1797e-01, time/batch = 0.1970s	
1223/2700 (epoch 22.648), train_loss = 0.71255393, grad/param norm = 1.0795e-01, time/batch = 0.1969s	
1224/2700 (epoch 22.667), train_loss = 0.62232542, grad/param norm = 7.9538e-02, time/batch = 0.1970s	
1225/2700 (epoch 22.685), train_loss = 0.63025104, grad/param norm = 8.0223e-02, time/batch = 0.1978s	
1226/2700 (epoch 22.704), train_loss = 0.66537032, grad/param norm = 1.0036e-01, time/batch = 0.1974s	
1227/2700 (epoch 22.722), train_loss = 0.67645929, grad/param norm = 1.0982e-01, time/batch = 0.1976s	
1228/2700 (epoch 22.741), train_loss = 0.64187581, grad/param norm = 8.1814e-02, time/batch = 0.1966s	
1229/2700 (epoch 22.759), train_loss = 0.59265483, grad/param norm = 8.3502e-02, time/batch = 0.1971s	
1230/2700 (epoch 22.778), train_loss = 0.62803291, grad/param norm = 9.0668e-02, time/batch = 0.1974s	
1231/2700 (epoch 22.796), train_loss = 0.56991333, grad/param norm = 9.0698e-02, time/batch = 0.1979s	
1232/2700 (epoch 22.815), train_loss = 0.66105648, grad/param norm = 1.0802e-01, time/batch = 0.1972s	
1233/2700 (epoch 22.833), train_loss = 0.64905075, grad/param norm = 9.9755e-02, time/batch = 0.1969s	
1234/2700 (epoch 22.852), train_loss = 0.63266740, grad/param norm = 9.8665e-02, time/batch = 0.1976s	
1235/2700 (epoch 22.870), train_loss = 0.64118708, grad/param norm = 9.1518e-02, time/batch = 0.1980s	
1236/2700 (epoch 22.889), train_loss = 0.60673165, grad/param norm = 8.8518e-02, time/batch = 0.1966s	
1237/2700 (epoch 22.907), train_loss = 0.60845993, grad/param norm = 9.0199e-02, time/batch = 0.1975s	
1238/2700 (epoch 22.926), train_loss = 0.57723801, grad/param norm = 8.7142e-02, time/batch = 0.1973s	
1239/2700 (epoch 22.944), train_loss = 0.56787084, grad/param norm = 8.5202e-02, time/batch = 0.1972s	
1240/2700 (epoch 22.963), train_loss = 0.60630958, grad/param norm = 1.0253e-01, time/batch = 0.1973s	
1241/2700 (epoch 22.981), train_loss = 0.59214543, grad/param norm = 8.6361e-02, time/batch = 0.1978s	
decayed learning rate by a factor 0.97 to 0.0013056725549212	
1242/2700 (epoch 23.000), train_loss = 0.59236068, grad/param norm = 9.5974e-02, time/batch = 0.1967s	
1243/2700 (epoch 23.019), train_loss = 0.68078054, grad/param norm = 9.8593e-02, time/batch = 0.1973s	
1244/2700 (epoch 23.037), train_loss = 0.60942672, grad/param norm = 9.6440e-02, time/batch = 0.1974s	
1245/2700 (epoch 23.056), train_loss = 0.60040935, grad/param norm = 9.0161e-02, time/batch = 0.1976s	
1246/2700 (epoch 23.074), train_loss = 0.60535724, grad/param norm = 9.9992e-02, time/batch = 0.1965s	
1247/2700 (epoch 23.093), train_loss = 0.59761499, grad/param norm = 9.1664e-02, time/batch = 0.1973s	
1248/2700 (epoch 23.111), train_loss = 0.58889402, grad/param norm = 8.5574e-02, time/batch = 0.1970s	
1249/2700 (epoch 23.130), train_loss = 0.57702181, grad/param norm = 7.8757e-02, time/batch = 0.1972s	
1250/2700 (epoch 23.148), train_loss = 0.56393428, grad/param norm = 7.2144e-02, time/batch = 0.1972s	
1251/2700 (epoch 23.167), train_loss = 0.57191017, grad/param norm = 8.3143e-02, time/batch = 0.1982s	
1252/2700 (epoch 23.185), train_loss = 0.60973014, grad/param norm = 1.0924e-01, time/batch = 0.1972s	
1253/2700 (epoch 23.204), train_loss = 0.60479144, grad/param norm = 9.5671e-02, time/batch = 0.1970s	
1254/2700 (epoch 23.222), train_loss = 0.56835661, grad/param norm = 9.4300e-02, time/batch = 0.1976s	
1255/2700 (epoch 23.241), train_loss = 0.57902877, grad/param norm = 1.0643e-01, time/batch = 0.1984s	
1256/2700 (epoch 23.259), train_loss = 0.59506298, grad/param norm = 1.0246e-01, time/batch = 0.1970s	
1257/2700 (epoch 23.278), train_loss = 0.61853931, grad/param norm = 9.5855e-02, time/batch = 0.1978s	
1258/2700 (epoch 23.296), train_loss = 0.59975698, grad/param norm = 9.3914e-02, time/batch = 0.1972s	
1259/2700 (epoch 23.315), train_loss = 0.54386189, grad/param norm = 8.7767e-02, time/batch = 0.1974s	
1260/2700 (epoch 23.333), train_loss = 0.55365276, grad/param norm = 8.9144e-02, time/batch = 0.1974s	
1261/2700 (epoch 23.352), train_loss = 0.54501096, grad/param norm = 9.3568e-02, time/batch = 0.1985s	
1262/2700 (epoch 23.370), train_loss = 0.54542650, grad/param norm = 9.8283e-02, time/batch = 0.1970s	
1263/2700 (epoch 23.389), train_loss = 0.56637553, grad/param norm = 1.0370e-01, time/batch = 0.1969s	
1264/2700 (epoch 23.407), train_loss = 0.61039027, grad/param norm = 1.0129e-01, time/batch = 0.1976s	
1265/2700 (epoch 23.426), train_loss = 0.57184231, grad/param norm = 9.4043e-02, time/batch = 0.1977s	
1266/2700 (epoch 23.444), train_loss = 0.59538785, grad/param norm = 9.6906e-02, time/batch = 0.1969s	
1267/2700 (epoch 23.463), train_loss = 0.66040587, grad/param norm = 1.3575e-01, time/batch = 0.1978s	
1268/2700 (epoch 23.481), train_loss = 0.65356702, grad/param norm = 1.3305e-01, time/batch = 0.1970s	
1269/2700 (epoch 23.500), train_loss = 0.56153235, grad/param norm = 9.3111e-02, time/batch = 0.1974s	
1270/2700 (epoch 23.519), train_loss = 0.59334791, grad/param norm = 1.0012e-01, time/batch = 0.1978s	
1271/2700 (epoch 23.537), train_loss = 0.57983061, grad/param norm = 9.5799e-02, time/batch = 0.1978s	
1272/2700 (epoch 23.556), train_loss = 0.52360248, grad/param norm = 7.3198e-02, time/batch = 0.1972s	
1273/2700 (epoch 23.574), train_loss = 0.52369052, grad/param norm = 7.8246e-02, time/batch = 0.1971s	
1274/2700 (epoch 23.593), train_loss = 0.55436250, grad/param norm = 9.5121e-02, time/batch = 0.1975s	
1275/2700 (epoch 23.611), train_loss = 0.55246184, grad/param norm = 9.6787e-02, time/batch = 0.1980s	
1276/2700 (epoch 23.630), train_loss = 0.55561714, grad/param norm = 9.6708e-02, time/batch = 0.1976s	
1277/2700 (epoch 23.648), train_loss = 0.58049063, grad/param norm = 9.4593e-02, time/batch = 0.1970s	
1278/2700 (epoch 23.667), train_loss = 0.58493151, grad/param norm = 1.1069e-01, time/batch = 0.1972s	
1279/2700 (epoch 23.685), train_loss = 0.56951508, grad/param norm = 9.7140e-02, time/batch = 0.1977s	
1280/2700 (epoch 23.704), train_loss = 0.56392766, grad/param norm = 8.6682e-02, time/batch = 0.1973s	
1281/2700 (epoch 23.722), train_loss = 0.59211436, grad/param norm = 1.0036e-01, time/batch = 0.1983s	
1282/2700 (epoch 23.741), train_loss = 0.58512872, grad/param norm = 1.0226e-01, time/batch = 0.1969s	
1283/2700 (epoch 23.759), train_loss = 0.53612924, grad/param norm = 1.0277e-01, time/batch = 0.1971s	
1284/2700 (epoch 23.778), train_loss = 0.56063728, grad/param norm = 9.1408e-02, time/batch = 0.1971s	
1285/2700 (epoch 23.796), train_loss = 0.50706527, grad/param norm = 9.2049e-02, time/batch = 0.1981s	
1286/2700 (epoch 23.815), train_loss = 0.54071251, grad/param norm = 8.8302e-02, time/batch = 0.1973s	
1287/2700 (epoch 23.833), train_loss = 0.55720244, grad/param norm = 8.8041e-02, time/batch = 0.1976s	
1288/2700 (epoch 23.852), train_loss = 0.56099431, grad/param norm = 1.1519e-01, time/batch = 0.1974s	
1289/2700 (epoch 23.870), train_loss = 0.56178822, grad/param norm = 8.9616e-02, time/batch = 0.1974s	
1290/2700 (epoch 23.889), train_loss = 0.52431823, grad/param norm = 8.4289e-02, time/batch = 0.1975s	
1291/2700 (epoch 23.907), train_loss = 0.53374267, grad/param norm = 1.0080e-01, time/batch = 0.1982s	
1292/2700 (epoch 23.926), train_loss = 0.52777024, grad/param norm = 9.8596e-02, time/batch = 0.1972s	
1293/2700 (epoch 23.944), train_loss = 0.50488146, grad/param norm = 9.0098e-02, time/batch = 0.2149s	
1294/2700 (epoch 23.963), train_loss = 0.53554767, grad/param norm = 1.0743e-01, time/batch = 0.2138s	
1295/2700 (epoch 23.981), train_loss = 0.53607756, grad/param norm = 1.0814e-01, time/batch = 0.2099s	
decayed learning rate by a factor 0.97 to 0.0012665023782736	
1296/2700 (epoch 24.000), train_loss = 0.51184575, grad/param norm = 9.1769e-02, time/batch = 0.2167s	
1297/2700 (epoch 24.019), train_loss = 0.57803673, grad/param norm = 8.2507e-02, time/batch = 0.2154s	
1298/2700 (epoch 24.037), train_loss = 0.52830368, grad/param norm = 9.4753e-02, time/batch = 0.2196s	
1299/2700 (epoch 24.056), train_loss = 0.50983190, grad/param norm = 8.9639e-02, time/batch = 0.2193s	
1300/2700 (epoch 24.074), train_loss = 0.50561075, grad/param norm = 9.7636e-02, time/batch = 0.2241s	
1301/2700 (epoch 24.093), train_loss = 0.51423975, grad/param norm = 9.8945e-02, time/batch = 0.2241s	
1302/2700 (epoch 24.111), train_loss = 0.51746071, grad/param norm = 9.0443e-02, time/batch = 0.2233s	
1303/2700 (epoch 24.130), train_loss = 0.51036785, grad/param norm = 8.4635e-02, time/batch = 0.2238s	
1304/2700 (epoch 24.148), train_loss = 0.50457437, grad/param norm = 8.6041e-02, time/batch = 0.2233s	
1305/2700 (epoch 24.167), train_loss = 0.49698424, grad/param norm = 8.0830e-02, time/batch = 0.2239s	
1306/2700 (epoch 24.185), train_loss = 0.48576526, grad/param norm = 7.3873e-02, time/batch = 0.2227s	
1307/2700 (epoch 24.204), train_loss = 0.50428556, grad/param norm = 8.0390e-02, time/batch = 0.2240s	
1308/2700 (epoch 24.222), train_loss = 0.50572069, grad/param norm = 9.2960e-02, time/batch = 0.2234s	
1309/2700 (epoch 24.241), train_loss = 0.50197132, grad/param norm = 9.4057e-02, time/batch = 0.2236s	
1310/2700 (epoch 24.259), train_loss = 0.51325726, grad/param norm = 9.7544e-02, time/batch = 0.2239s	
1311/2700 (epoch 24.278), train_loss = 0.52568825, grad/param norm = 9.6347e-02, time/batch = 0.2239s	
1312/2700 (epoch 24.296), train_loss = 0.51673854, grad/param norm = 9.1080e-02, time/batch = 0.2235s	
1313/2700 (epoch 24.315), train_loss = 0.46098386, grad/param norm = 8.2596e-02, time/batch = 0.2235s	
1314/2700 (epoch 24.333), train_loss = 0.48881258, grad/param norm = 8.9622e-02, time/batch = 0.2238s	
1315/2700 (epoch 24.352), train_loss = 0.47215775, grad/param norm = 9.6338e-02, time/batch = 0.2243s	
1316/2700 (epoch 24.370), train_loss = 0.46317717, grad/param norm = 8.3845e-02, time/batch = 0.2231s	
1317/2700 (epoch 24.389), train_loss = 0.48401873, grad/param norm = 9.1907e-02, time/batch = 0.2239s	
1318/2700 (epoch 24.407), train_loss = 0.51482465, grad/param norm = 9.4112e-02, time/batch = 0.2236s	
1319/2700 (epoch 24.426), train_loss = 0.49747046, grad/param norm = 1.0343e-01, time/batch = 0.2244s	
1320/2700 (epoch 24.444), train_loss = 0.50231660, grad/param norm = 1.0208e-01, time/batch = 0.2239s	
1321/2700 (epoch 24.463), train_loss = 0.53194236, grad/param norm = 1.0054e-01, time/batch = 0.2255s	
1322/2700 (epoch 24.481), train_loss = 0.51041575, grad/param norm = 1.0146e-01, time/batch = 0.2239s	
1323/2700 (epoch 24.500), train_loss = 0.50581412, grad/param norm = 1.0395e-01, time/batch = 0.2219s	
1324/2700 (epoch 24.519), train_loss = 0.51876584, grad/param norm = 1.0628e-01, time/batch = 0.2210s	
1325/2700 (epoch 24.537), train_loss = 0.51595816, grad/param norm = 1.0062e-01, time/batch = 0.2214s	
1326/2700 (epoch 24.556), train_loss = 0.48669405, grad/param norm = 1.1142e-01, time/batch = 0.2210s	
1327/2700 (epoch 24.574), train_loss = 0.49351232, grad/param norm = 1.0547e-01, time/batch = 0.2205s	
1328/2700 (epoch 24.593), train_loss = 0.50385337, grad/param norm = 9.4202e-02, time/batch = 0.2206s	
1329/2700 (epoch 24.611), train_loss = 0.47552345, grad/param norm = 8.3635e-02, time/batch = 0.2212s	
1330/2700 (epoch 24.630), train_loss = 0.48850491, grad/param norm = 9.0090e-02, time/batch = 0.2212s	
1331/2700 (epoch 24.648), train_loss = 0.51718389, grad/param norm = 1.0362e-01, time/batch = 0.2226s	
1332/2700 (epoch 24.667), train_loss = 0.49272767, grad/param norm = 1.0937e-01, time/batch = 0.2212s	
1333/2700 (epoch 24.685), train_loss = 0.50203433, grad/param norm = 1.0141e-01, time/batch = 0.2202s	
1334/2700 (epoch 24.704), train_loss = 0.50172768, grad/param norm = 1.0084e-01, time/batch = 0.2212s	
1335/2700 (epoch 24.722), train_loss = 0.50713512, grad/param norm = 9.4886e-02, time/batch = 0.2212s	
1336/2700 (epoch 24.741), train_loss = 0.48685294, grad/param norm = 8.3387e-02, time/batch = 0.2213s	
1337/2700 (epoch 24.759), train_loss = 0.46753978, grad/param norm = 9.3434e-02, time/batch = 0.2216s	
1338/2700 (epoch 24.778), train_loss = 0.50346678, grad/param norm = 1.0481e-01, time/batch = 0.2184s	
1339/2700 (epoch 24.796), train_loss = 0.45143585, grad/param norm = 9.1951e-02, time/batch = 0.2175s	
1340/2700 (epoch 24.815), train_loss = 0.47348450, grad/param norm = 8.8087e-02, time/batch = 0.2241s	
1341/2700 (epoch 24.833), train_loss = 0.47461553, grad/param norm = 8.5574e-02, time/batch = 0.2244s	
1342/2700 (epoch 24.852), train_loss = 0.46457269, grad/param norm = 8.6595e-02, time/batch = 0.2236s	
1343/2700 (epoch 24.870), train_loss = 0.49779449, grad/param norm = 9.7945e-02, time/batch = 0.2233s	
1344/2700 (epoch 24.889), train_loss = 0.49017442, grad/param norm = 9.7863e-02, time/batch = 0.2237s	
1345/2700 (epoch 24.907), train_loss = 0.46017273, grad/param norm = 8.6893e-02, time/batch = 0.2241s	
1346/2700 (epoch 24.926), train_loss = 0.44156835, grad/param norm = 9.4445e-02, time/batch = 0.2233s	
1347/2700 (epoch 24.944), train_loss = 0.43985075, grad/param norm = 1.0264e-01, time/batch = 0.2237s	
1348/2700 (epoch 24.963), train_loss = 0.48412851, grad/param norm = 1.1632e-01, time/batch = 0.2231s	
1349/2700 (epoch 24.981), train_loss = 0.45219163, grad/param norm = 8.6257e-02, time/batch = 0.2236s	
decayed learning rate by a factor 0.97 to 0.0012285073069254	
1350/2700 (epoch 25.000), train_loss = 0.44193304, grad/param norm = 9.1317e-02, time/batch = 0.2237s	
1351/2700 (epoch 25.019), train_loss = 0.51318155, grad/param norm = 9.0926e-02, time/batch = 0.2249s	
1352/2700 (epoch 25.037), train_loss = 0.43821446, grad/param norm = 7.9984e-02, time/batch = 0.2235s	
1353/2700 (epoch 25.056), train_loss = 0.42813244, grad/param norm = 7.3820e-02, time/batch = 0.2236s	
1354/2700 (epoch 25.074), train_loss = 0.41570256, grad/param norm = 7.7823e-02, time/batch = 0.2238s	
1355/2700 (epoch 25.093), train_loss = 0.42701478, grad/param norm = 8.0128e-02, time/batch = 0.2243s	
1356/2700 (epoch 25.111), train_loss = 0.44250123, grad/param norm = 8.8346e-02, time/batch = 0.2235s	
1357/2700 (epoch 25.130), train_loss = 0.44977435, grad/param norm = 1.0132e-01, time/batch = 0.2238s	
1358/2700 (epoch 25.148), train_loss = 0.44842366, grad/param norm = 9.1441e-02, time/batch = 0.2237s	
1359/2700 (epoch 25.167), train_loss = 0.45889564, grad/param norm = 1.0174e-01, time/batch = 0.2242s	
1360/2700 (epoch 25.185), train_loss = 0.45105464, grad/param norm = 9.9730e-02, time/batch = 0.2239s	
1361/2700 (epoch 25.204), train_loss = 0.44306119, grad/param norm = 8.8340e-02, time/batch = 0.2243s	
1362/2700 (epoch 25.222), train_loss = 0.42671805, grad/param norm = 9.4623e-02, time/batch = 0.2233s	
1363/2700 (epoch 25.241), train_loss = 0.41549952, grad/param norm = 8.8493e-02, time/batch = 0.2236s	
1364/2700 (epoch 25.259), train_loss = 0.44349205, grad/param norm = 8.8620e-02, time/batch = 0.2241s	
1365/2700 (epoch 25.278), train_loss = 0.47663671, grad/param norm = 1.0393e-01, time/batch = 0.2241s	
1366/2700 (epoch 25.296), train_loss = 0.45910205, grad/param norm = 9.8143e-02, time/batch = 0.2238s	
1367/2700 (epoch 25.315), train_loss = 0.40574892, grad/param norm = 7.8111e-02, time/batch = 0.2240s	
1368/2700 (epoch 25.333), train_loss = 0.40137015, grad/param norm = 7.4133e-02, time/batch = 0.2235s	
1369/2700 (epoch 25.352), train_loss = 0.38547177, grad/param norm = 7.3712e-02, time/batch = 0.2238s	
1370/2700 (epoch 25.370), train_loss = 0.40062710, grad/param norm = 9.4049e-02, time/batch = 0.2239s	
1371/2700 (epoch 25.389), train_loss = 0.41673009, grad/param norm = 8.5307e-02, time/batch = 0.2210s	
1372/2700 (epoch 25.407), train_loss = 0.43270909, grad/param norm = 8.4767e-02, time/batch = 0.2207s	
1373/2700 (epoch 25.426), train_loss = 0.43619053, grad/param norm = 9.2735e-02, time/batch = 0.2208s	
1374/2700 (epoch 25.444), train_loss = 0.44532730, grad/param norm = 1.0164e-01, time/batch = 0.2208s	
1375/2700 (epoch 25.463), train_loss = 0.43935000, grad/param norm = 1.0077e-01, time/batch = 0.2217s	
1376/2700 (epoch 25.481), train_loss = 0.44590239, grad/param norm = 9.5746e-02, time/batch = 0.2201s	
1377/2700 (epoch 25.500), train_loss = 0.41206955, grad/param norm = 9.1591e-02, time/batch = 0.2208s	
1378/2700 (epoch 25.519), train_loss = 0.43036582, grad/param norm = 8.7470e-02, time/batch = 0.2203s	
1379/2700 (epoch 25.537), train_loss = 0.43558798, grad/param norm = 9.9221e-02, time/batch = 0.2207s	
1380/2700 (epoch 25.556), train_loss = 0.42722696, grad/param norm = 9.2278e-02, time/batch = 0.2207s	
1381/2700 (epoch 25.574), train_loss = 0.43226603, grad/param norm = 9.9405e-02, time/batch = 0.2214s	
1382/2700 (epoch 25.593), train_loss = 0.43964555, grad/param norm = 9.7799e-02, time/batch = 0.2204s	
1383/2700 (epoch 25.611), train_loss = 0.42438075, grad/param norm = 1.0842e-01, time/batch = 0.2206s	
1384/2700 (epoch 25.630), train_loss = 0.43475989, grad/param norm = 1.0027e-01, time/batch = 0.2208s	
1385/2700 (epoch 25.648), train_loss = 0.46506595, grad/param norm = 9.8643e-02, time/batch = 0.2205s	
1386/2700 (epoch 25.667), train_loss = 0.43024056, grad/param norm = 8.8045e-02, time/batch = 0.2205s	
1387/2700 (epoch 25.685), train_loss = 0.42401501, grad/param norm = 9.1639e-02, time/batch = 0.2211s	
1388/2700 (epoch 25.704), train_loss = 0.45031771, grad/param norm = 1.1507e-01, time/batch = 0.2211s	
1389/2700 (epoch 25.722), train_loss = 0.45616539, grad/param norm = 1.0820e-01, time/batch = 0.2205s	
1390/2700 (epoch 25.741), train_loss = 0.42092907, grad/param norm = 7.9121e-02, time/batch = 0.2204s	
1391/2700 (epoch 25.759), train_loss = 0.36998930, grad/param norm = 7.0486e-02, time/batch = 0.2236s	
1392/2700 (epoch 25.778), train_loss = 0.39883583, grad/param norm = 7.6713e-02, time/batch = 0.2237s	
1393/2700 (epoch 25.796), train_loss = 0.36930946, grad/param norm = 8.3178e-02, time/batch = 0.2237s	
1394/2700 (epoch 25.815), train_loss = 0.42393806, grad/param norm = 9.7114e-02, time/batch = 0.2237s	
1395/2700 (epoch 25.833), train_loss = 0.41795405, grad/param norm = 1.0676e-01, time/batch = 0.2241s	
1396/2700 (epoch 25.852), train_loss = 0.41473787, grad/param norm = 9.6963e-02, time/batch = 0.2238s	
1397/2700 (epoch 25.870), train_loss = 0.40575318, grad/param norm = 8.9965e-02, time/batch = 0.2239s	
1398/2700 (epoch 25.889), train_loss = 0.39180559, grad/param norm = 7.9578e-02, time/batch = 0.2236s	
1399/2700 (epoch 25.907), train_loss = 0.40382865, grad/param norm = 9.2022e-02, time/batch = 0.2237s	
1400/2700 (epoch 25.926), train_loss = 0.38338163, grad/param norm = 8.3030e-02, time/batch = 0.2236s	
1401/2700 (epoch 25.944), train_loss = 0.36411558, grad/param norm = 7.9114e-02, time/batch = 0.2242s	
1402/2700 (epoch 25.963), train_loss = 0.37679721, grad/param norm = 8.4376e-02, time/batch = 0.2233s	
1403/2700 (epoch 25.981), train_loss = 0.38811204, grad/param norm = 9.2952e-02, time/batch = 0.2234s	
decayed learning rate by a factor 0.97 to 0.0011916520877176	
1404/2700 (epoch 26.000), train_loss = 0.37012075, grad/param norm = 9.3839e-02, time/batch = 0.2236s	
1405/2700 (epoch 26.019), train_loss = 0.45380692, grad/param norm = 8.7998e-02, time/batch = 0.2240s	
1406/2700 (epoch 26.037), train_loss = 0.39212595, grad/param norm = 8.9773e-02, time/batch = 0.2224s	
1407/2700 (epoch 26.056), train_loss = 0.37688364, grad/param norm = 8.0103e-02, time/batch = 0.2241s	
1408/2700 (epoch 26.074), train_loss = 0.36026590, grad/param norm = 8.0805e-02, time/batch = 0.2230s	
1409/2700 (epoch 26.093), train_loss = 0.37173286, grad/param norm = 9.0973e-02, time/batch = 0.2237s	
1410/2700 (epoch 26.111), train_loss = 0.38951511, grad/param norm = 9.0981e-02, time/batch = 0.2238s	
1411/2700 (epoch 26.130), train_loss = 0.36932763, grad/param norm = 8.0737e-02, time/batch = 0.2254s	
1412/2700 (epoch 26.148), train_loss = 0.37783154, grad/param norm = 8.0595e-02, time/batch = 0.2238s	
1413/2700 (epoch 26.167), train_loss = 0.39063195, grad/param norm = 9.8316e-02, time/batch = 0.2241s	
1414/2700 (epoch 26.185), train_loss = 0.39328082, grad/param norm = 9.8371e-02, time/batch = 0.2246s	
1415/2700 (epoch 26.204), train_loss = 0.41899506, grad/param norm = 1.0784e-01, time/batch = 0.2247s	
1416/2700 (epoch 26.222), train_loss = 0.37525348, grad/param norm = 8.7432e-02, time/batch = 0.2238s	
1417/2700 (epoch 26.241), train_loss = 0.35994944, grad/param norm = 8.0552e-02, time/batch = 0.2241s	
1418/2700 (epoch 26.259), train_loss = 0.36830837, grad/param norm = 8.7563e-02, time/batch = 0.2238s	
1419/2700 (epoch 26.278), train_loss = 0.37528353, grad/param norm = 7.2723e-02, time/batch = 0.2244s	
1420/2700 (epoch 26.296), train_loss = 0.36397384, grad/param norm = 8.0498e-02, time/batch = 0.2243s	
1421/2700 (epoch 26.315), train_loss = 0.34793810, grad/param norm = 8.6662e-02, time/batch = 0.2246s	
1422/2700 (epoch 26.333), train_loss = 0.35292952, grad/param norm = 8.1937e-02, time/batch = 0.2241s	
1423/2700 (epoch 26.352), train_loss = 0.31456813, grad/param norm = 6.7354e-02, time/batch = 0.2213s	
1424/2700 (epoch 26.370), train_loss = 0.30797223, grad/param norm = 6.5158e-02, time/batch = 0.2208s	
1425/2700 (epoch 26.389), train_loss = 0.32583529, grad/param norm = 7.2268e-02, time/batch = 0.2214s	
1426/2700 (epoch 26.407), train_loss = 0.35564469, grad/param norm = 7.4713e-02, time/batch = 0.2206s	
1427/2700 (epoch 26.426), train_loss = 0.34458229, grad/param norm = 8.1632e-02, time/batch = 0.2206s	
1428/2700 (epoch 26.444), train_loss = 0.34913045, grad/param norm = 8.3820e-02, time/batch = 0.2201s	
1429/2700 (epoch 26.463), train_loss = 0.37856541, grad/param norm = 1.0214e-01, time/batch = 0.2210s	
1430/2700 (epoch 26.481), train_loss = 0.38208456, grad/param norm = 9.2067e-02, time/batch = 0.2207s	
1431/2700 (epoch 26.500), train_loss = 0.36014093, grad/param norm = 9.0836e-02, time/batch = 0.2217s	
1432/2700 (epoch 26.519), train_loss = 0.37061211, grad/param norm = 9.0548e-02, time/batch = 0.2206s	
1433/2700 (epoch 26.537), train_loss = 0.36096458, grad/param norm = 8.1868e-02, time/batch = 0.2211s	
1434/2700 (epoch 26.556), train_loss = 0.34037518, grad/param norm = 8.3297e-02, time/batch = 0.2214s	
1435/2700 (epoch 26.574), train_loss = 0.34214393, grad/param norm = 7.8318e-02, time/batch = 0.2218s	
1436/2700 (epoch 26.593), train_loss = 0.34249543, grad/param norm = 7.6591e-02, time/batch = 0.2210s	
1437/2700 (epoch 26.611), train_loss = 0.35169257, grad/param norm = 8.4777e-02, time/batch = 0.2212s	
1438/2700 (epoch 26.630), train_loss = 0.37671028, grad/param norm = 1.0727e-01, time/batch = 0.2206s	
1439/2700 (epoch 26.648), train_loss = 0.40947479, grad/param norm = 1.0763e-01, time/batch = 0.2210s	
1440/2700 (epoch 26.667), train_loss = 0.39390305, grad/param norm = 1.0704e-01, time/batch = 0.2210s	
1441/2700 (epoch 26.685), train_loss = 0.39687135, grad/param norm = 1.0442e-01, time/batch = 0.2194s	
1442/2700 (epoch 26.704), train_loss = 0.37321222, grad/param norm = 8.5332e-02, time/batch = 0.2138s	
1443/2700 (epoch 26.722), train_loss = 0.38120341, grad/param norm = 9.3088e-02, time/batch = 0.2216s	
1444/2700 (epoch 26.741), train_loss = 0.37591193, grad/param norm = 9.5598e-02, time/batch = 0.2237s	
1445/2700 (epoch 26.759), train_loss = 0.32493230, grad/param norm = 7.6426e-02, time/batch = 0.2243s	
1446/2700 (epoch 26.778), train_loss = 0.33772221, grad/param norm = 7.3062e-02, time/batch = 0.2234s	
1447/2700 (epoch 26.796), train_loss = 0.30391805, grad/param norm = 7.2471e-02, time/batch = 0.2239s	
1448/2700 (epoch 26.815), train_loss = 0.32641258, grad/param norm = 6.9454e-02, time/batch = 0.2238s	
1449/2700 (epoch 26.833), train_loss = 0.33984208, grad/param norm = 8.0311e-02, time/batch = 0.2236s	
1450/2700 (epoch 26.852), train_loss = 0.35383377, grad/param norm = 1.0356e-01, time/batch = 0.2240s	
1451/2700 (epoch 26.870), train_loss = 0.38590533, grad/param norm = 1.0650e-01, time/batch = 0.2250s	
1452/2700 (epoch 26.889), train_loss = 0.35432125, grad/param norm = 9.0331e-02, time/batch = 0.2235s	
1453/2700 (epoch 26.907), train_loss = 0.34736968, grad/param norm = 9.1257e-02, time/batch = 0.2239s	
1454/2700 (epoch 26.926), train_loss = 0.33238491, grad/param norm = 8.3564e-02, time/batch = 0.2231s	
1455/2700 (epoch 26.944), train_loss = 0.31190154, grad/param norm = 7.7645e-02, time/batch = 0.2241s	
1456/2700 (epoch 26.963), train_loss = 0.32445607, grad/param norm = 7.0512e-02, time/batch = 0.2234s	
1457/2700 (epoch 26.981), train_loss = 0.31747556, grad/param norm = 8.4832e-02, time/batch = 0.2241s	
decayed learning rate by a factor 0.97 to 0.0011559025250861	
1458/2700 (epoch 27.000), train_loss = 0.28629910, grad/param norm = 6.6095e-02, time/batch = 0.2224s	
1459/2700 (epoch 27.019), train_loss = 0.37290697, grad/param norm = 9.0111e-02, time/batch = 0.2238s	
1460/2700 (epoch 27.037), train_loss = 0.32997175, grad/param norm = 8.5742e-02, time/batch = 0.2231s	
1461/2700 (epoch 27.056), train_loss = 0.33050608, grad/param norm = 8.9151e-02, time/batch = 0.2243s	
1462/2700 (epoch 27.074), train_loss = 0.32939119, grad/param norm = 8.9022e-02, time/batch = 0.2232s	
1463/2700 (epoch 27.093), train_loss = 0.33158665, grad/param norm = 8.5183e-02, time/batch = 0.2235s	
1464/2700 (epoch 27.111), train_loss = 0.33393623, grad/param norm = 8.3284e-02, time/batch = 0.2239s	
1465/2700 (epoch 27.130), train_loss = 0.31817121, grad/param norm = 7.7659e-02, time/batch = 0.2242s	
1466/2700 (epoch 27.148), train_loss = 0.31408704, grad/param norm = 7.9293e-02, time/batch = 0.2231s	
1467/2700 (epoch 27.167), train_loss = 0.31645965, grad/param norm = 7.5922e-02, time/batch = 0.2238s	
1468/2700 (epoch 27.185), train_loss = 0.29798735, grad/param norm = 7.0045e-02, time/batch = 0.2236s	
1469/2700 (epoch 27.204), train_loss = 0.30804888, grad/param norm = 7.1199e-02, time/batch = 0.2239s	
1470/2700 (epoch 27.222), train_loss = 0.29222314, grad/param norm = 7.8581e-02, time/batch = 0.2236s	
1471/2700 (epoch 27.241), train_loss = 0.30650334, grad/param norm = 8.0012e-02, time/batch = 0.2250s	
1472/2700 (epoch 27.259), train_loss = 0.31388459, grad/param norm = 8.1237e-02, time/batch = 0.2237s	
1473/2700 (epoch 27.278), train_loss = 0.32472510, grad/param norm = 8.2173e-02, time/batch = 0.2238s	
1474/2700 (epoch 27.296), train_loss = 0.30901077, grad/param norm = 6.8922e-02, time/batch = 0.2241s	
1475/2700 (epoch 27.315), train_loss = 0.28208168, grad/param norm = 7.1514e-02, time/batch = 0.2224s	
1476/2700 (epoch 27.333), train_loss = 0.29404541, grad/param norm = 8.0494e-02, time/batch = 0.2204s	
1477/2700 (epoch 27.352), train_loss = 0.28383056, grad/param norm = 7.2483e-02, time/batch = 0.2204s	
1478/2700 (epoch 27.370), train_loss = 0.27117781, grad/param norm = 7.4419e-02, time/batch = 0.2208s	
1479/2700 (epoch 27.389), train_loss = 0.28145568, grad/param norm = 6.9014e-02, time/batch = 0.2211s	
1480/2700 (epoch 27.407), train_loss = 0.29247956, grad/param norm = 6.5849e-02, time/batch = 0.2210s	
1481/2700 (epoch 27.426), train_loss = 0.28153324, grad/param norm = 6.9081e-02, time/batch = 0.2217s	
1482/2700 (epoch 27.444), train_loss = 0.29067587, grad/param norm = 7.4631e-02, time/batch = 0.2204s	
1483/2700 (epoch 27.463), train_loss = 0.32261695, grad/param norm = 1.0458e-01, time/batch = 0.2207s	
1484/2700 (epoch 27.481), train_loss = 0.33316325, grad/param norm = 9.6679e-02, time/batch = 0.2213s	
1485/2700 (epoch 27.500), train_loss = 0.30274459, grad/param norm = 8.1563e-02, time/batch = 0.2217s	
1486/2700 (epoch 27.519), train_loss = 0.31472192, grad/param norm = 8.2870e-02, time/batch = 0.2209s	
1487/2700 (epoch 27.537), train_loss = 0.30507844, grad/param norm = 8.1188e-02, time/batch = 0.2210s	
1488/2700 (epoch 27.556), train_loss = 0.29617944, grad/param norm = 7.9581e-02, time/batch = 0.2207s	
1489/2700 (epoch 27.574), train_loss = 0.29728134, grad/param norm = 7.9408e-02, time/batch = 0.2212s	
1490/2700 (epoch 27.593), train_loss = 0.30293813, grad/param norm = 7.9648e-02, time/batch = 0.2177s	
1491/2700 (epoch 27.611), train_loss = 0.29486326, grad/param norm = 8.4888e-02, time/batch = 0.1989s	
1492/2700 (epoch 27.630), train_loss = 0.28309474, grad/param norm = 7.3116e-02, time/batch = 0.1973s	
1493/2700 (epoch 27.648), train_loss = 0.31362301, grad/param norm = 7.7294e-02, time/batch = 0.1970s	
1494/2700 (epoch 27.667), train_loss = 0.30603412, grad/param norm = 8.3068e-02, time/batch = 0.1976s	
1495/2700 (epoch 27.685), train_loss = 0.29735466, grad/param norm = 8.0522e-02, time/batch = 0.1982s	
1496/2700 (epoch 27.704), train_loss = 0.33372779, grad/param norm = 9.8654e-02, time/batch = 0.1972s	
1497/2700 (epoch 27.722), train_loss = 0.36015023, grad/param norm = 1.2541e-01, time/batch = 0.1975s	
1498/2700 (epoch 27.741), train_loss = 0.34500386, grad/param norm = 9.9358e-02, time/batch = 0.1968s	
1499/2700 (epoch 27.759), train_loss = 0.28407903, grad/param norm = 7.6029e-02, time/batch = 0.1975s	
1500/2700 (epoch 27.778), train_loss = 0.31596914, grad/param norm = 8.4457e-02, time/batch = 0.1972s	
1501/2700 (epoch 27.796), train_loss = 0.25935944, grad/param norm = 6.8222e-02, time/batch = 0.1987s	
1502/2700 (epoch 27.815), train_loss = 0.28062911, grad/param norm = 7.0959e-02, time/batch = 0.1974s	
1503/2700 (epoch 27.833), train_loss = 0.27493283, grad/param norm = 6.8597e-02, time/batch = 0.1973s	
1504/2700 (epoch 27.852), train_loss = 0.27097947, grad/param norm = 7.0489e-02, time/batch = 0.1981s	
1505/2700 (epoch 27.870), train_loss = 0.27963743, grad/param norm = 7.7174e-02, time/batch = 0.1984s	
1506/2700 (epoch 27.889), train_loss = 0.29249142, grad/param norm = 7.9992e-02, time/batch = 0.1977s	
1507/2700 (epoch 27.907), train_loss = 0.28109340, grad/param norm = 8.3572e-02, time/batch = 0.1978s	
1508/2700 (epoch 27.926), train_loss = 0.27891438, grad/param norm = 7.4792e-02, time/batch = 0.1977s	
1509/2700 (epoch 27.944), train_loss = 0.26534593, grad/param norm = 7.4879e-02, time/batch = 0.1982s	
1510/2700 (epoch 27.963), train_loss = 0.28619182, grad/param norm = 7.5435e-02, time/batch = 0.1983s	
1511/2700 (epoch 27.981), train_loss = 0.26274527, grad/param norm = 7.0866e-02, time/batch = 0.1981s	
decayed learning rate by a factor 0.97 to 0.0011212254493335	
1512/2700 (epoch 28.000), train_loss = 0.24663453, grad/param norm = 6.9272e-02, time/batch = 0.1977s	
1513/2700 (epoch 28.019), train_loss = 0.31559021, grad/param norm = 7.0912e-02, time/batch = 0.1981s	
1514/2700 (epoch 28.037), train_loss = 0.26005116, grad/param norm = 6.2986e-02, time/batch = 0.1982s	
1515/2700 (epoch 28.056), train_loss = 0.25541277, grad/param norm = 6.8888e-02, time/batch = 0.1984s	
1516/2700 (epoch 28.074), train_loss = 0.25127766, grad/param norm = 7.6323e-02, time/batch = 0.1969s	
1517/2700 (epoch 28.093), train_loss = 0.28377124, grad/param norm = 8.9779e-02, time/batch = 0.1973s	
1518/2700 (epoch 28.111), train_loss = 0.28611269, grad/param norm = 8.9428e-02, time/batch = 0.1971s	
1519/2700 (epoch 28.130), train_loss = 0.29019980, grad/param norm = 8.6731e-02, time/batch = 0.1982s	
1520/2700 (epoch 28.148), train_loss = 0.27883150, grad/param norm = 7.4677e-02, time/batch = 0.1981s	
1521/2700 (epoch 28.167), train_loss = 0.28105488, grad/param norm = 8.0689e-02, time/batch = 0.1985s	
1522/2700 (epoch 28.185), train_loss = 0.27493307, grad/param norm = 9.0209e-02, time/batch = 0.1979s	
1523/2700 (epoch 28.204), train_loss = 0.26597641, grad/param norm = 7.7835e-02, time/batch = 0.1976s	
1524/2700 (epoch 28.222), train_loss = 0.24397154, grad/param norm = 7.1764e-02, time/batch = 0.1971s	
1525/2700 (epoch 28.241), train_loss = 0.24245576, grad/param norm = 7.2436e-02, time/batch = 0.1981s	
1526/2700 (epoch 28.259), train_loss = 0.25373004, grad/param norm = 6.8122e-02, time/batch = 0.1977s	
1527/2700 (epoch 28.278), train_loss = 0.27382356, grad/param norm = 8.0537e-02, time/batch = 0.1979s	
1528/2700 (epoch 28.296), train_loss = 0.28107712, grad/param norm = 8.3500e-02, time/batch = 0.1974s	
1529/2700 (epoch 28.315), train_loss = 0.24449093, grad/param norm = 7.0752e-02, time/batch = 0.1980s	
1530/2700 (epoch 28.333), train_loss = 0.23709374, grad/param norm = 6.1642e-02, time/batch = 0.1980s	
1531/2700 (epoch 28.352), train_loss = 0.23222765, grad/param norm = 6.4243e-02, time/batch = 0.1987s	
1532/2700 (epoch 28.370), train_loss = 0.22533272, grad/param norm = 6.9785e-02, time/batch = 0.1976s	
1533/2700 (epoch 28.389), train_loss = 0.23815127, grad/param norm = 7.2437e-02, time/batch = 0.1975s	
1534/2700 (epoch 28.407), train_loss = 0.25755249, grad/param norm = 7.0312e-02, time/batch = 0.1975s	
1535/2700 (epoch 28.426), train_loss = 0.24071066, grad/param norm = 6.8445e-02, time/batch = 0.1982s	
1536/2700 (epoch 28.444), train_loss = 0.24224687, grad/param norm = 6.9103e-02, time/batch = 0.1970s	
1537/2700 (epoch 28.463), train_loss = 0.25607420, grad/param norm = 7.4003e-02, time/batch = 0.1972s	
1538/2700 (epoch 28.481), train_loss = 0.27037566, grad/param norm = 8.8262e-02, time/batch = 0.1970s	
1539/2700 (epoch 28.500), train_loss = 0.27277227, grad/param norm = 9.0530e-02, time/batch = 0.1973s	
1540/2700 (epoch 28.519), train_loss = 0.27558061, grad/param norm = 8.9450e-02, time/batch = 0.1971s	
1541/2700 (epoch 28.537), train_loss = 0.26163099, grad/param norm = 7.5538e-02, time/batch = 0.1982s	
1542/2700 (epoch 28.556), train_loss = 0.24869028, grad/param norm = 7.2701e-02, time/batch = 0.1970s	
1543/2700 (epoch 28.574), train_loss = 0.25961718, grad/param norm = 7.7849e-02, time/batch = 0.1968s	
1544/2700 (epoch 28.593), train_loss = 0.26159482, grad/param norm = 8.2421e-02, time/batch = 0.1976s	
1545/2700 (epoch 28.611), train_loss = 0.24595925, grad/param norm = 6.7736e-02, time/batch = 0.1976s	
1546/2700 (epoch 28.630), train_loss = 0.24598692, grad/param norm = 7.4164e-02, time/batch = 0.1972s	
1547/2700 (epoch 28.648), train_loss = 0.28951083, grad/param norm = 9.3685e-02, time/batch = 0.1977s	
1548/2700 (epoch 28.667), train_loss = 0.26303703, grad/param norm = 8.2106e-02, time/batch = 0.1971s	
1549/2700 (epoch 28.685), train_loss = 0.26089815, grad/param norm = 7.8223e-02, time/batch = 0.1970s	
1550/2700 (epoch 28.704), train_loss = 0.24431183, grad/param norm = 6.8672e-02, time/batch = 0.1973s	
1551/2700 (epoch 28.722), train_loss = 0.26157377, grad/param norm = 7.1584e-02, time/batch = 0.1981s	
1552/2700 (epoch 28.741), train_loss = 0.27240392, grad/param norm = 8.1650e-02, time/batch = 0.1971s	
1553/2700 (epoch 28.759), train_loss = 0.25100225, grad/param norm = 8.4529e-02, time/batch = 0.1971s	
1554/2700 (epoch 28.778), train_loss = 0.26126994, grad/param norm = 7.6583e-02, time/batch = 0.1973s	
1555/2700 (epoch 28.796), train_loss = 0.22626579, grad/param norm = 6.7725e-02, time/batch = 0.1978s	
1556/2700 (epoch 28.815), train_loss = 0.24085821, grad/param norm = 6.8474e-02, time/batch = 0.1972s	
1557/2700 (epoch 28.833), train_loss = 0.22567263, grad/param norm = 6.0751e-02, time/batch = 0.1984s	
1558/2700 (epoch 28.852), train_loss = 0.22910088, grad/param norm = 7.4328e-02, time/batch = 0.1972s	
1559/2700 (epoch 28.870), train_loss = 0.24194961, grad/param norm = 7.1013e-02, time/batch = 0.1984s	
1560/2700 (epoch 28.889), train_loss = 0.24223557, grad/param norm = 7.3239e-02, time/batch = 0.1978s	
1561/2700 (epoch 28.907), train_loss = 0.22428016, grad/param norm = 6.5981e-02, time/batch = 0.1979s	
1562/2700 (epoch 28.926), train_loss = 0.23430492, grad/param norm = 7.9476e-02, time/batch = 0.1973s	
1563/2700 (epoch 28.944), train_loss = 0.22819269, grad/param norm = 8.0074e-02, time/batch = 0.1973s	
1564/2700 (epoch 28.963), train_loss = 0.24771993, grad/param norm = 7.5994e-02, time/batch = 0.1976s	
1565/2700 (epoch 28.981), train_loss = 0.23147583, grad/param norm = 7.4018e-02, time/batch = 0.1976s	
decayed learning rate by a factor 0.97 to 0.0010875886858535	
1566/2700 (epoch 29.000), train_loss = 0.21918501, grad/param norm = 7.7578e-02, time/batch = 0.1969s	
1567/2700 (epoch 29.019), train_loss = 0.26515633, grad/param norm = 7.0967e-02, time/batch = 0.1972s	
1568/2700 (epoch 29.037), train_loss = 0.22786710, grad/param norm = 6.5425e-02, time/batch = 0.1970s	
1569/2700 (epoch 29.056), train_loss = 0.21634068, grad/param norm = 5.7755e-02, time/batch = 0.1975s	
1570/2700 (epoch 29.074), train_loss = 0.20204360, grad/param norm = 5.9157e-02, time/batch = 0.1983s	
1571/2700 (epoch 29.093), train_loss = 0.21367004, grad/param norm = 6.0621e-02, time/batch = 0.1985s	
1572/2700 (epoch 29.111), train_loss = 0.20941929, grad/param norm = 6.4275e-02, time/batch = 0.1972s	
1573/2700 (epoch 29.130), train_loss = 0.22475895, grad/param norm = 8.2239e-02, time/batch = 0.1975s	
1574/2700 (epoch 29.148), train_loss = 0.23840637, grad/param norm = 7.3860e-02, time/batch = 0.1974s	
1575/2700 (epoch 29.167), train_loss = 0.23782249, grad/param norm = 7.5015e-02, time/batch = 0.1979s	
1576/2700 (epoch 29.185), train_loss = 0.23589968, grad/param norm = 8.1143e-02, time/batch = 0.1975s	
1577/2700 (epoch 29.204), train_loss = 0.24168990, grad/param norm = 8.1411e-02, time/batch = 0.1975s	
1578/2700 (epoch 29.222), train_loss = 0.22499749, grad/param norm = 7.9752e-02, time/batch = 0.1972s	
1579/2700 (epoch 29.241), train_loss = 0.22238078, grad/param norm = 7.3838e-02, time/batch = 0.1975s	
1580/2700 (epoch 29.259), train_loss = 0.22967384, grad/param norm = 7.9783e-02, time/batch = 0.1982s	
1581/2700 (epoch 29.278), train_loss = 0.23483045, grad/param norm = 7.6498e-02, time/batch = 0.1983s	
1582/2700 (epoch 29.296), train_loss = 0.22685706, grad/param norm = 6.9529e-02, time/batch = 0.1973s	
1583/2700 (epoch 29.315), train_loss = 0.22301714, grad/param norm = 7.6638e-02, time/batch = 0.1973s	
1584/2700 (epoch 29.333), train_loss = 0.22207798, grad/param norm = 7.6671e-02, time/batch = 0.1974s	
1585/2700 (epoch 29.352), train_loss = 0.19765532, grad/param norm = 6.0222e-02, time/batch = 0.1980s	
1586/2700 (epoch 29.370), train_loss = 0.18097316, grad/param norm = 5.6154e-02, time/batch = 0.1972s	
1587/2700 (epoch 29.389), train_loss = 0.19708124, grad/param norm = 6.0405e-02, time/batch = 0.1972s	
1588/2700 (epoch 29.407), train_loss = 0.20334451, grad/param norm = 5.6694e-02, time/batch = 0.1966s	
1589/2700 (epoch 29.426), train_loss = 0.20555538, grad/param norm = 6.3895e-02, time/batch = 0.1973s	
1590/2700 (epoch 29.444), train_loss = 0.20108233, grad/param norm = 6.9218e-02, time/batch = 0.1973s	
1591/2700 (epoch 29.463), train_loss = 0.22014436, grad/param norm = 7.0110e-02, time/batch = 0.1979s	
1592/2700 (epoch 29.481), train_loss = 0.21150704, grad/param norm = 7.1753e-02, time/batch = 0.1966s	
1593/2700 (epoch 29.500), train_loss = 0.20773246, grad/param norm = 6.4536e-02, time/batch = 0.1976s	
1594/2700 (epoch 29.519), train_loss = 0.23288678, grad/param norm = 8.1442e-02, time/batch = 0.1984s	
1595/2700 (epoch 29.537), train_loss = 0.23438914, grad/param norm = 7.8747e-02, time/batch = 0.1985s	
1596/2700 (epoch 29.556), train_loss = 0.23978721, grad/param norm = 8.2264e-02, time/batch = 0.1971s	
1597/2700 (epoch 29.574), train_loss = 0.22489236, grad/param norm = 7.4351e-02, time/batch = 0.1974s	
1598/2700 (epoch 29.593), train_loss = 0.22548051, grad/param norm = 7.1984e-02, time/batch = 0.1972s	
1599/2700 (epoch 29.611), train_loss = 0.22455717, grad/param norm = 8.1369e-02, time/batch = 0.1976s	
1600/2700 (epoch 29.630), train_loss = 0.22475074, grad/param norm = 8.0395e-02, time/batch = 0.1971s	
1601/2700 (epoch 29.648), train_loss = 0.23104851, grad/param norm = 7.1404e-02, time/batch = 0.1980s	
1602/2700 (epoch 29.667), train_loss = 0.21535492, grad/param norm = 6.4703e-02, time/batch = 0.1969s	
1603/2700 (epoch 29.685), train_loss = 0.21188247, grad/param norm = 7.0405e-02, time/batch = 0.1968s	
1604/2700 (epoch 29.704), train_loss = 0.20428054, grad/param norm = 6.3384e-02, time/batch = 0.1975s	
1605/2700 (epoch 29.722), train_loss = 0.21373272, grad/param norm = 6.5525e-02, time/batch = 0.1974s	
1606/2700 (epoch 29.741), train_loss = 0.20879620, grad/param norm = 5.8972e-02, time/batch = 0.1970s	
1607/2700 (epoch 29.759), train_loss = 0.18496248, grad/param norm = 5.7565e-02, time/batch = 0.1971s	
1608/2700 (epoch 29.778), train_loss = 0.19743699, grad/param norm = 5.8636e-02, time/batch = 0.1969s	
1609/2700 (epoch 29.796), train_loss = 0.18010362, grad/param norm = 6.8745e-02, time/batch = 0.1972s	
1610/2700 (epoch 29.815), train_loss = 0.19839701, grad/param norm = 6.7382e-02, time/batch = 0.1974s	
1611/2700 (epoch 29.833), train_loss = 0.20130172, grad/param norm = 7.0908e-02, time/batch = 0.1982s	
1612/2700 (epoch 29.852), train_loss = 0.19172850, grad/param norm = 6.4388e-02, time/batch = 0.1968s	
1613/2700 (epoch 29.870), train_loss = 0.20531637, grad/param norm = 7.1143e-02, time/batch = 0.1972s	
1614/2700 (epoch 29.889), train_loss = 0.21283309, grad/param norm = 7.6356e-02, time/batch = 0.1974s	
1615/2700 (epoch 29.907), train_loss = 0.19080275, grad/param norm = 6.9529e-02, time/batch = 0.1976s	
1616/2700 (epoch 29.926), train_loss = 0.19222862, grad/param norm = 6.2630e-02, time/batch = 0.1969s	
1617/2700 (epoch 29.944), train_loss = 0.17385880, grad/param norm = 5.5789e-02, time/batch = 0.1974s	
1618/2700 (epoch 29.963), train_loss = 0.19630282, grad/param norm = 6.2745e-02, time/batch = 0.1969s	
1619/2700 (epoch 29.981), train_loss = 0.18503132, grad/param norm = 6.2487e-02, time/batch = 0.1975s	
decayed learning rate by a factor 0.97 to 0.0010549610252779	
1620/2700 (epoch 30.000), train_loss = 0.17513598, grad/param norm = 6.4646e-02, time/batch = 0.1974s	
1621/2700 (epoch 30.019), train_loss = 0.22800540, grad/param norm = 6.6527e-02, time/batch = 0.1982s	
1622/2700 (epoch 30.037), train_loss = 0.20306700, grad/param norm = 7.4256e-02, time/batch = 0.1974s	
1623/2700 (epoch 30.056), train_loss = 0.19287116, grad/param norm = 6.8340e-02, time/batch = 0.1968s	
1624/2700 (epoch 30.074), train_loss = 0.17694782, grad/param norm = 6.1102e-02, time/batch = 0.1973s	
1625/2700 (epoch 30.093), train_loss = 0.18150307, grad/param norm = 5.7836e-02, time/batch = 0.1974s	
1626/2700 (epoch 30.111), train_loss = 0.17488645, grad/param norm = 5.4498e-02, time/batch = 0.1968s	
1627/2700 (epoch 30.130), train_loss = 0.17633804, grad/param norm = 5.8051e-02, time/batch = 0.1972s	
1628/2700 (epoch 30.148), train_loss = 0.18592666, grad/param norm = 5.7720e-02, time/batch = 0.1968s	
1629/2700 (epoch 30.167), train_loss = 0.18665935, grad/param norm = 5.6934e-02, time/batch = 0.1972s	
1630/2700 (epoch 30.185), train_loss = 0.16597046, grad/param norm = 5.4794e-02, time/batch = 0.1976s	
1631/2700 (epoch 30.204), train_loss = 0.18138248, grad/param norm = 6.9161e-02, time/batch = 0.1991s	
1632/2700 (epoch 30.222), train_loss = 0.17544797, grad/param norm = 7.7479e-02, time/batch = 0.1973s	
1633/2700 (epoch 30.241), train_loss = 0.18839972, grad/param norm = 7.4277e-02, time/batch = 0.1974s	
1634/2700 (epoch 30.259), train_loss = 0.18745081, grad/param norm = 6.8915e-02, time/batch = 0.1976s	
1635/2700 (epoch 30.278), train_loss = 0.19870957, grad/param norm = 6.9639e-02, time/batch = 0.1981s	
1636/2700 (epoch 30.296), train_loss = 0.19448976, grad/param norm = 6.3648e-02, time/batch = 0.1969s	
1637/2700 (epoch 30.315), train_loss = 0.16815687, grad/param norm = 5.7187e-02, time/batch = 0.1974s	
1638/2700 (epoch 30.333), train_loss = 0.17855151, grad/param norm = 6.2856e-02, time/batch = 0.1970s	
1639/2700 (epoch 30.352), train_loss = 0.17168401, grad/param norm = 6.5989e-02, time/batch = 0.1971s	
1640/2700 (epoch 30.370), train_loss = 0.15988695, grad/param norm = 6.0985e-02, time/batch = 0.1973s	
1641/2700 (epoch 30.389), train_loss = 0.16791300, grad/param norm = 5.8611e-02, time/batch = 0.1979s	
1642/2700 (epoch 30.407), train_loss = 0.17033801, grad/param norm = 5.4720e-02, time/batch = 0.1974s	
1643/2700 (epoch 30.426), train_loss = 0.16349409, grad/param norm = 4.9528e-02, time/batch = 0.1976s	
1644/2700 (epoch 30.444), train_loss = 0.15850840, grad/param norm = 4.8585e-02, time/batch = 0.1969s	
1645/2700 (epoch 30.463), train_loss = 0.16535285, grad/param norm = 5.0211e-02, time/batch = 0.1975s	
1646/2700 (epoch 30.481), train_loss = 0.15688302, grad/param norm = 4.6830e-02, time/batch = 0.2120s	
1647/2700 (epoch 30.500), train_loss = 0.16910921, grad/param norm = 5.9990e-02, time/batch = 0.2136s	
1648/2700 (epoch 30.519), train_loss = 0.18352356, grad/param norm = 7.9356e-02, time/batch = 0.2152s	
1649/2700 (epoch 30.537), train_loss = 0.16542771, grad/param norm = 5.3020e-02, time/batch = 0.2185s	
1650/2700 (epoch 30.556), train_loss = 0.16282366, grad/param norm = 5.2356e-02, time/batch = 0.2171s	
1651/2700 (epoch 30.574), train_loss = 0.17587151, grad/param norm = 6.3147e-02, time/batch = 0.2170s	
1652/2700 (epoch 30.593), train_loss = 0.18385867, grad/param norm = 7.0533e-02, time/batch = 0.2200s	
1653/2700 (epoch 30.611), train_loss = 0.18964015, grad/param norm = 7.2077e-02, time/batch = 0.2239s	
1654/2700 (epoch 30.630), train_loss = 0.17717492, grad/param norm = 6.6000e-02, time/batch = 0.2238s	
1655/2700 (epoch 30.648), train_loss = 0.20793957, grad/param norm = 7.3816e-02, time/batch = 0.2241s	
1656/2700 (epoch 30.667), train_loss = 0.20743834, grad/param norm = 7.8171e-02, time/batch = 0.2232s	
1657/2700 (epoch 30.685), train_loss = 0.18474504, grad/param norm = 7.0590e-02, time/batch = 0.2238s	
1658/2700 (epoch 30.704), train_loss = 0.17918533, grad/param norm = 6.6968e-02, time/batch = 0.2234s	
1659/2700 (epoch 30.722), train_loss = 0.18569310, grad/param norm = 6.9765e-02, time/batch = 0.2237s	
1660/2700 (epoch 30.741), train_loss = 0.17739213, grad/param norm = 6.0427e-02, time/batch = 0.2239s	
1661/2700 (epoch 30.759), train_loss = 0.15829491, grad/param norm = 5.7321e-02, time/batch = 0.2244s	
1662/2700 (epoch 30.778), train_loss = 0.16913394, grad/param norm = 5.1214e-02, time/batch = 0.2231s	
1663/2700 (epoch 30.796), train_loss = 0.14374206, grad/param norm = 4.6939e-02, time/batch = 0.2238s	
1664/2700 (epoch 30.815), train_loss = 0.16449453, grad/param norm = 6.0670e-02, time/batch = 0.2237s	
1665/2700 (epoch 30.833), train_loss = 0.17077618, grad/param norm = 6.5760e-02, time/batch = 0.2241s	
1666/2700 (epoch 30.852), train_loss = 0.17737788, grad/param norm = 7.9745e-02, time/batch = 0.2234s	
1667/2700 (epoch 30.870), train_loss = 0.17587362, grad/param norm = 6.9857e-02, time/batch = 0.2238s	
1668/2700 (epoch 30.889), train_loss = 0.18379923, grad/param norm = 6.8352e-02, time/batch = 0.2227s	
1669/2700 (epoch 30.907), train_loss = 0.16547010, grad/param norm = 6.3390e-02, time/batch = 0.2239s	
1670/2700 (epoch 30.926), train_loss = 0.17342924, grad/param norm = 6.4647e-02, time/batch = 0.2241s	
1671/2700 (epoch 30.944), train_loss = 0.14366129, grad/param norm = 5.3506e-02, time/batch = 0.2244s	
1672/2700 (epoch 30.963), train_loss = 0.15759555, grad/param norm = 5.1416e-02, time/batch = 0.2236s	
1673/2700 (epoch 30.981), train_loss = 0.14215412, grad/param norm = 4.7796e-02, time/batch = 0.2236s	
decayed learning rate by a factor 0.97 to 0.0010233121945196	
1674/2700 (epoch 31.000), train_loss = 0.13923551, grad/param norm = 5.3551e-02, time/batch = 0.2241s	
1675/2700 (epoch 31.019), train_loss = 0.19482382, grad/param norm = 5.6039e-02, time/batch = 0.2241s	
1676/2700 (epoch 31.037), train_loss = 0.16145840, grad/param norm = 5.6668e-02, time/batch = 0.2234s	
1677/2700 (epoch 31.056), train_loss = 0.15895915, grad/param norm = 5.6843e-02, time/batch = 0.2224s	
1678/2700 (epoch 31.074), train_loss = 0.15237450, grad/param norm = 5.7978e-02, time/batch = 0.2206s	
1679/2700 (epoch 31.093), train_loss = 0.15051873, grad/param norm = 5.2526e-02, time/batch = 0.2212s	
1680/2700 (epoch 31.111), train_loss = 0.14340013, grad/param norm = 5.2548e-02, time/batch = 0.2211s	
1681/2700 (epoch 31.130), train_loss = 0.14420624, grad/param norm = 5.1455e-02, time/batch = 0.2217s	
1682/2700 (epoch 31.148), train_loss = 0.15192118, grad/param norm = 5.2870e-02, time/batch = 0.2208s	
1683/2700 (epoch 31.167), train_loss = 0.16020556, grad/param norm = 5.3245e-02, time/batch = 0.2210s	
1684/2700 (epoch 31.185), train_loss = 0.13996165, grad/param norm = 5.2800e-02, time/batch = 0.2209s	
1685/2700 (epoch 31.204), train_loss = 0.13988029, grad/param norm = 4.9594e-02, time/batch = 0.2217s	
1686/2700 (epoch 31.222), train_loss = 0.13805435, grad/param norm = 6.0461e-02, time/batch = 0.2200s	
1687/2700 (epoch 31.241), train_loss = 0.14872324, grad/param norm = 6.7097e-02, time/batch = 0.2210s	
1688/2700 (epoch 31.259), train_loss = 0.17130117, grad/param norm = 8.0273e-02, time/batch = 0.2207s	
1689/2700 (epoch 31.278), train_loss = 0.21047935, grad/param norm = 9.6963e-02, time/batch = 0.2211s	
1690/2700 (epoch 31.296), train_loss = 0.19378294, grad/param norm = 8.5070e-02, time/batch = 0.2214s	
1691/2700 (epoch 31.315), train_loss = 0.16235551, grad/param norm = 6.4545e-02, time/batch = 0.2162s	
1692/2700 (epoch 31.333), train_loss = 0.15048209, grad/param norm = 5.7844e-02, time/batch = 0.2208s	
1693/2700 (epoch 31.352), train_loss = 0.13255539, grad/param norm = 5.1487e-02, time/batch = 0.2235s	
1694/2700 (epoch 31.370), train_loss = 0.13178591, grad/param norm = 5.1720e-02, time/batch = 0.2240s	
1695/2700 (epoch 31.389), train_loss = 0.14182335, grad/param norm = 4.7094e-02, time/batch = 0.2242s	
1696/2700 (epoch 31.407), train_loss = 0.14704053, grad/param norm = 5.7581e-02, time/batch = 0.2234s	
1697/2700 (epoch 31.426), train_loss = 0.15812171, grad/param norm = 6.3533e-02, time/batch = 0.2243s	
1698/2700 (epoch 31.444), train_loss = 0.14683452, grad/param norm = 5.5521e-02, time/batch = 0.2235s	
1699/2700 (epoch 31.463), train_loss = 0.14951937, grad/param norm = 4.9819e-02, time/batch = 0.2240s	
1700/2700 (epoch 31.481), train_loss = 0.12982404, grad/param norm = 4.2994e-02, time/batch = 0.2233s	
1701/2700 (epoch 31.500), train_loss = 0.12832611, grad/param norm = 4.1984e-02, time/batch = 0.2245s	
1702/2700 (epoch 31.519), train_loss = 0.13612543, grad/param norm = 4.8286e-02, time/batch = 0.2229s	
1703/2700 (epoch 31.537), train_loss = 0.14727573, grad/param norm = 6.9640e-02, time/batch = 0.2238s	
1704/2700 (epoch 31.556), train_loss = 0.13976870, grad/param norm = 5.6078e-02, time/batch = 0.2237s	
1705/2700 (epoch 31.574), train_loss = 0.13673589, grad/param norm = 4.9289e-02, time/batch = 0.2243s	
1706/2700 (epoch 31.593), train_loss = 0.13446881, grad/param norm = 4.7084e-02, time/batch = 0.2229s	
1707/2700 (epoch 31.611), train_loss = 0.14536389, grad/param norm = 6.0492e-02, time/batch = 0.2238s	
1708/2700 (epoch 31.630), train_loss = 0.15516545, grad/param norm = 6.5238e-02, time/batch = 0.2230s	
1709/2700 (epoch 31.648), train_loss = 0.17802175, grad/param norm = 7.1235e-02, time/batch = 0.2228s	
1710/2700 (epoch 31.667), train_loss = 0.19656940, grad/param norm = 8.7970e-02, time/batch = 0.2239s	
1711/2700 (epoch 31.685), train_loss = 0.19567495, grad/param norm = 8.3836e-02, time/batch = 0.2243s	
1712/2700 (epoch 31.704), train_loss = 0.17999288, grad/param norm = 7.5612e-02, time/batch = 0.2234s	
1713/2700 (epoch 31.722), train_loss = 0.16897174, grad/param norm = 6.8017e-02, time/batch = 0.2234s	
1714/2700 (epoch 31.741), train_loss = 0.17658422, grad/param norm = 7.5903e-02, time/batch = 0.2236s	
1715/2700 (epoch 31.759), train_loss = 0.15484138, grad/param norm = 6.4055e-02, time/batch = 0.2241s	
1716/2700 (epoch 31.778), train_loss = 0.14932046, grad/param norm = 5.1039e-02, time/batch = 0.2218s	
1717/2700 (epoch 31.796), train_loss = 0.12471063, grad/param norm = 4.5671e-02, time/batch = 0.2237s	
1718/2700 (epoch 31.815), train_loss = 0.12861210, grad/param norm = 4.2177e-02, time/batch = 0.2237s	
1719/2700 (epoch 31.833), train_loss = 0.13079856, grad/param norm = 4.5425e-02, time/batch = 0.2238s	
1720/2700 (epoch 31.852), train_loss = 0.12705526, grad/param norm = 4.6460e-02, time/batch = 0.2242s	
1721/2700 (epoch 31.870), train_loss = 0.13969013, grad/param norm = 5.7358e-02, time/batch = 0.2246s	
1722/2700 (epoch 31.889), train_loss = 0.14921311, grad/param norm = 6.1953e-02, time/batch = 0.2232s	
1723/2700 (epoch 31.907), train_loss = 0.12582508, grad/param norm = 4.9818e-02, time/batch = 0.2236s	
1724/2700 (epoch 31.926), train_loss = 0.13520018, grad/param norm = 4.9137e-02, time/batch = 0.2232s	
1725/2700 (epoch 31.944), train_loss = 0.12719198, grad/param norm = 5.4197e-02, time/batch = 0.2211s	
1726/2700 (epoch 31.963), train_loss = 0.14498154, grad/param norm = 5.1508e-02, time/batch = 0.2203s	
1727/2700 (epoch 31.981), train_loss = 0.12251540, grad/param norm = 4.6532e-02, time/batch = 0.2210s	
decayed learning rate by a factor 0.97 to 0.00099261282868397	
1728/2700 (epoch 32.000), train_loss = 0.10961066, grad/param norm = 4.4870e-02, time/batch = 0.2203s	
1729/2700 (epoch 32.019), train_loss = 0.16166464, grad/param norm = 4.7563e-02, time/batch = 0.2207s	
1730/2700 (epoch 32.037), train_loss = 0.13167534, grad/param norm = 4.5284e-02, time/batch = 0.2211s	
1731/2700 (epoch 32.056), train_loss = 0.13032044, grad/param norm = 5.2992e-02, time/batch = 0.2242s	
1732/2700 (epoch 32.074), train_loss = 0.12081885, grad/param norm = 4.6977e-02, time/batch = 0.2208s	
1733/2700 (epoch 32.093), train_loss = 0.12655116, grad/param norm = 5.0153e-02, time/batch = 0.2211s	
1734/2700 (epoch 32.111), train_loss = 0.12578701, grad/param norm = 5.2796e-02, time/batch = 0.2210s	
1735/2700 (epoch 32.130), train_loss = 0.12568002, grad/param norm = 5.0443e-02, time/batch = 0.2214s	
1736/2700 (epoch 32.148), train_loss = 0.12539832, grad/param norm = 4.7182e-02, time/batch = 0.2207s	
1737/2700 (epoch 32.167), train_loss = 0.13246278, grad/param norm = 4.6919e-02, time/batch = 0.2214s	
1738/2700 (epoch 32.185), train_loss = 0.11051115, grad/param norm = 4.3995e-02, time/batch = 0.2207s	
1739/2700 (epoch 32.204), train_loss = 0.12604219, grad/param norm = 5.7653e-02, time/batch = 0.2214s	
1740/2700 (epoch 32.222), train_loss = 0.12030255, grad/param norm = 5.4739e-02, time/batch = 0.2212s	
1741/2700 (epoch 32.241), train_loss = 0.11759877, grad/param norm = 5.1459e-02, time/batch = 0.2218s	
1742/2700 (epoch 32.259), train_loss = 0.12152772, grad/param norm = 4.7518e-02, time/batch = 0.2213s	
1743/2700 (epoch 32.278), train_loss = 0.13170054, grad/param norm = 4.9763e-02, time/batch = 0.2178s	
1744/2700 (epoch 32.296), train_loss = 0.14694032, grad/param norm = 6.7657e-02, time/batch = 0.2155s	
1745/2700 (epoch 32.315), train_loss = 0.14101162, grad/param norm = 6.7815e-02, time/batch = 0.2228s	
1746/2700 (epoch 32.333), train_loss = 0.13706599, grad/param norm = 6.3130e-02, time/batch = 0.2236s	
1747/2700 (epoch 32.352), train_loss = 0.12329380, grad/param norm = 5.8075e-02, time/batch = 0.2241s	
1748/2700 (epoch 32.370), train_loss = 0.11625847, grad/param norm = 5.2480e-02, time/batch = 0.2234s	
1749/2700 (epoch 32.389), train_loss = 0.12458706, grad/param norm = 4.7217e-02, time/batch = 0.2239s	
1750/2700 (epoch 32.407), train_loss = 0.12262163, grad/param norm = 4.9095e-02, time/batch = 0.2239s	
1751/2700 (epoch 32.426), train_loss = 0.12289330, grad/param norm = 4.6187e-02, time/batch = 0.2237s	
1752/2700 (epoch 32.444), train_loss = 0.12325181, grad/param norm = 5.0525e-02, time/batch = 0.2235s	
1753/2700 (epoch 32.463), train_loss = 0.13516982, grad/param norm = 5.4510e-02, time/batch = 0.2235s	
1754/2700 (epoch 32.481), train_loss = 0.11878492, grad/param norm = 4.7960e-02, time/batch = 0.2237s	
1755/2700 (epoch 32.500), train_loss = 0.10863622, grad/param norm = 4.0491e-02, time/batch = 0.2243s	
1756/2700 (epoch 32.519), train_loss = 0.10545555, grad/param norm = 3.8536e-02, time/batch = 0.2232s	
1757/2700 (epoch 32.537), train_loss = 0.10907393, grad/param norm = 3.8338e-02, time/batch = 0.2238s	
1758/2700 (epoch 32.556), train_loss = 0.10674779, grad/param norm = 4.0744e-02, time/batch = 0.2235s	
1759/2700 (epoch 32.574), train_loss = 0.11641728, grad/param norm = 4.4699e-02, time/batch = 0.2241s	
1760/2700 (epoch 32.593), train_loss = 0.11196710, grad/param norm = 4.3897e-02, time/batch = 0.2232s	
1761/2700 (epoch 32.611), train_loss = 0.11449416, grad/param norm = 4.8019e-02, time/batch = 0.2247s	
1762/2700 (epoch 32.630), train_loss = 0.10817805, grad/param norm = 4.7601e-02, time/batch = 0.2234s	
1763/2700 (epoch 32.648), train_loss = 0.13434340, grad/param norm = 5.4013e-02, time/batch = 0.2237s	
1764/2700 (epoch 32.667), train_loss = 0.13667798, grad/param norm = 5.2678e-02, time/batch = 0.2237s	
1765/2700 (epoch 32.685), train_loss = 0.13441890, grad/param norm = 6.4748e-02, time/batch = 0.2243s	
1766/2700 (epoch 32.704), train_loss = 0.14125612, grad/param norm = 6.3088e-02, time/batch = 0.2232s	
1767/2700 (epoch 32.722), train_loss = 0.14348261, grad/param norm = 6.3343e-02, time/batch = 0.2241s	
1768/2700 (epoch 32.741), train_loss = 0.14141624, grad/param norm = 5.3674e-02, time/batch = 0.2232s	
1769/2700 (epoch 32.759), train_loss = 0.12031137, grad/param norm = 5.1943e-02, time/batch = 0.2240s	
1770/2700 (epoch 32.778), train_loss = 0.13129852, grad/param norm = 5.4391e-02, time/batch = 0.2240s	
1771/2700 (epoch 32.796), train_loss = 0.11638165, grad/param norm = 4.6442e-02, time/batch = 0.2252s	
1772/2700 (epoch 32.815), train_loss = 0.12521196, grad/param norm = 5.0999e-02, time/batch = 0.2235s	
1773/2700 (epoch 32.833), train_loss = 0.11573957, grad/param norm = 5.1000e-02, time/batch = 0.2237s	
1774/2700 (epoch 32.852), train_loss = 0.10488239, grad/param norm = 4.0969e-02, time/batch = 0.2243s	
1775/2700 (epoch 32.870), train_loss = 0.10725163, grad/param norm = 4.3133e-02, time/batch = 0.2247s	
1776/2700 (epoch 32.889), train_loss = 0.11590118, grad/param norm = 4.3279e-02, time/batch = 0.2230s	
1777/2700 (epoch 32.907), train_loss = 0.11297019, grad/param norm = 5.2598e-02, time/batch = 0.2209s	
1778/2700 (epoch 32.926), train_loss = 0.11598312, grad/param norm = 4.9583e-02, time/batch = 0.2204s	
1779/2700 (epoch 32.944), train_loss = 0.09569061, grad/param norm = 3.9070e-02, time/batch = 0.2204s	
1780/2700 (epoch 32.963), train_loss = 0.10830767, grad/param norm = 4.1579e-02, time/batch = 0.2208s	
1781/2700 (epoch 32.981), train_loss = 0.09680768, grad/param norm = 3.8893e-02, time/batch = 0.2218s	
decayed learning rate by a factor 0.97 to 0.00096283444382345	
1782/2700 (epoch 33.000), train_loss = 0.09928788, grad/param norm = 4.1587e-02, time/batch = 0.2205s	
1783/2700 (epoch 33.019), train_loss = 0.13631303, grad/param norm = 4.3339e-02, time/batch = 0.2206s	
1784/2700 (epoch 33.037), train_loss = 0.10956138, grad/param norm = 3.7896e-02, time/batch = 0.2211s	
1785/2700 (epoch 33.056), train_loss = 0.10222352, grad/param norm = 4.5265e-02, time/batch = 0.2215s	
1786/2700 (epoch 33.074), train_loss = 0.10613928, grad/param norm = 4.6287e-02, time/batch = 0.2204s	
1787/2700 (epoch 33.093), train_loss = 0.10151584, grad/param norm = 3.9570e-02, time/batch = 0.2209s	
1788/2700 (epoch 33.111), train_loss = 0.10456411, grad/param norm = 4.4511e-02, time/batch = 0.2203s	
1789/2700 (epoch 33.130), train_loss = 0.10390896, grad/param norm = 4.7462e-02, time/batch = 0.2208s	
1790/2700 (epoch 33.148), train_loss = 0.11352512, grad/param norm = 4.7976e-02, time/batch = 0.2208s	
1791/2700 (epoch 33.167), train_loss = 0.11670507, grad/param norm = 4.6448e-02, time/batch = 0.2216s	
1792/2700 (epoch 33.185), train_loss = 0.09531077, grad/param norm = 4.6604e-02, time/batch = 0.2204s	
1793/2700 (epoch 33.204), train_loss = 0.10164292, grad/param norm = 4.7395e-02, time/batch = 0.2209s	
1794/2700 (epoch 33.222), train_loss = 0.09005996, grad/param norm = 4.8214e-02, time/batch = 0.2210s	
1795/2700 (epoch 33.241), train_loss = 0.10320213, grad/param norm = 4.9239e-02, time/batch = 0.2214s	
1796/2700 (epoch 33.259), train_loss = 0.11159891, grad/param norm = 4.9404e-02, time/batch = 0.2134s	
1797/2700 (epoch 33.278), train_loss = 0.12048373, grad/param norm = 4.9198e-02, time/batch = 0.2203s	
1798/2700 (epoch 33.296), train_loss = 0.10960306, grad/param norm = 4.8575e-02, time/batch = 0.2229s	
1799/2700 (epoch 33.315), train_loss = 0.09971812, grad/param norm = 4.2498e-02, time/batch = 0.2228s	
1800/2700 (epoch 33.333), train_loss = 0.10976604, grad/param norm = 5.2132e-02, time/batch = 0.2234s	
1801/2700 (epoch 33.352), train_loss = 0.09849055, grad/param norm = 4.5053e-02, time/batch = 0.2248s	
1802/2700 (epoch 33.370), train_loss = 0.09709673, grad/param norm = 4.7842e-02, time/batch = 0.2234s	
1803/2700 (epoch 33.389), train_loss = 0.10972479, grad/param norm = 4.5401e-02, time/batch = 0.2235s	
1804/2700 (epoch 33.407), train_loss = 0.11402539, grad/param norm = 4.9438e-02, time/batch = 0.2236s	
1805/2700 (epoch 33.426), train_loss = 0.10519770, grad/param norm = 4.4494e-02, time/batch = 0.2241s	
1806/2700 (epoch 33.444), train_loss = 0.10730928, grad/param norm = 4.2515e-02, time/batch = 0.2230s	
1807/2700 (epoch 33.463), train_loss = 0.11141411, grad/param norm = 4.5204e-02, time/batch = 0.2236s	
1808/2700 (epoch 33.481), train_loss = 0.10043953, grad/param norm = 4.1735e-02, time/batch = 0.2235s	
1809/2700 (epoch 33.500), train_loss = 0.09791915, grad/param norm = 3.8719e-02, time/batch = 0.2238s	
1810/2700 (epoch 33.519), train_loss = 0.09719581, grad/param norm = 4.2709e-02, time/batch = 0.2236s	
1811/2700 (epoch 33.537), train_loss = 0.09216170, grad/param norm = 3.8208e-02, time/batch = 0.2244s	
1812/2700 (epoch 33.556), train_loss = 0.09393347, grad/param norm = 3.6971e-02, time/batch = 0.2228s	
1813/2700 (epoch 33.574), train_loss = 0.09826966, grad/param norm = 3.7285e-02, time/batch = 0.2230s	
1814/2700 (epoch 33.593), train_loss = 0.09378978, grad/param norm = 3.7941e-02, time/batch = 0.2240s	
1815/2700 (epoch 33.611), train_loss = 0.09407606, grad/param norm = 3.6958e-02, time/batch = 0.2239s	
1816/2700 (epoch 33.630), train_loss = 0.08622674, grad/param norm = 4.1741e-02, time/batch = 0.2231s	
1817/2700 (epoch 33.648), train_loss = 0.11493749, grad/param norm = 4.7772e-02, time/batch = 0.2243s	
1818/2700 (epoch 33.667), train_loss = 0.11321740, grad/param norm = 4.6938e-02, time/batch = 0.2240s	
1819/2700 (epoch 33.685), train_loss = 0.10604252, grad/param norm = 4.5655e-02, time/batch = 0.2238s	
1820/2700 (epoch 33.704), train_loss = 0.11035963, grad/param norm = 4.7682e-02, time/batch = 0.2238s	
1821/2700 (epoch 33.722), train_loss = 0.10791303, grad/param norm = 5.4277e-02, time/batch = 0.2242s	
1822/2700 (epoch 33.741), train_loss = 0.12143365, grad/param norm = 5.4808e-02, time/batch = 0.2238s	
1823/2700 (epoch 33.759), train_loss = 0.11198767, grad/param norm = 6.1146e-02, time/batch = 0.2237s	
1824/2700 (epoch 33.778), train_loss = 0.12648552, grad/param norm = 5.4692e-02, time/batch = 0.2239s	
1825/2700 (epoch 33.796), train_loss = 0.10029676, grad/param norm = 4.4944e-02, time/batch = 0.2243s	
1826/2700 (epoch 33.815), train_loss = 0.10323877, grad/param norm = 4.6257e-02, time/batch = 0.2234s	
1827/2700 (epoch 33.833), train_loss = 0.10220028, grad/param norm = 4.2005e-02, time/batch = 0.2239s	
1828/2700 (epoch 33.852), train_loss = 0.09362133, grad/param norm = 4.4092e-02, time/batch = 0.2236s	
1829/2700 (epoch 33.870), train_loss = 0.09150967, grad/param norm = 3.9675e-02, time/batch = 0.2229s	
1830/2700 (epoch 33.889), train_loss = 0.09609408, grad/param norm = 3.6458e-02, time/batch = 0.2204s	
1831/2700 (epoch 33.907), train_loss = 0.08444334, grad/param norm = 3.4476e-02, time/batch = 0.2212s	
1832/2700 (epoch 33.926), train_loss = 0.09058000, grad/param norm = 3.8621e-02, time/batch = 0.2203s	
1833/2700 (epoch 33.944), train_loss = 0.08667582, grad/param norm = 4.5849e-02, time/batch = 0.2208s	
1834/2700 (epoch 33.963), train_loss = 0.09688313, grad/param norm = 4.0781e-02, time/batch = 0.2213s	
1835/2700 (epoch 33.981), train_loss = 0.08459769, grad/param norm = 3.8887e-02, time/batch = 0.2214s	
decayed learning rate by a factor 0.97 to 0.00093394941050874	
1836/2700 (epoch 34.000), train_loss = 0.07552512, grad/param norm = 3.1945e-02, time/batch = 0.2202s	
1837/2700 (epoch 34.019), train_loss = 0.12033739, grad/param norm = 3.9965e-02, time/batch = 0.2207s	
1838/2700 (epoch 34.037), train_loss = 0.09720207, grad/param norm = 4.1468e-02, time/batch = 0.2214s	
1839/2700 (epoch 34.056), train_loss = 0.08622208, grad/param norm = 3.8877e-02, time/batch = 0.2214s	
1840/2700 (epoch 34.074), train_loss = 0.08645555, grad/param norm = 3.5859e-02, time/batch = 0.2210s	
1841/2700 (epoch 34.093), train_loss = 0.08777074, grad/param norm = 3.8879e-02, time/batch = 0.2217s	
1842/2700 (epoch 34.111), train_loss = 0.08500568, grad/param norm = 3.8020e-02, time/batch = 0.2206s	
1843/2700 (epoch 34.130), train_loss = 0.08243666, grad/param norm = 3.5760e-02, time/batch = 0.2213s	
1844/2700 (epoch 34.148), train_loss = 0.08652306, grad/param norm = 3.4985e-02, time/batch = 0.2023s	
1845/2700 (epoch 34.167), train_loss = 0.09258719, grad/param norm = 3.7136e-02, time/batch = 0.1978s	
1846/2700 (epoch 34.185), train_loss = 0.07682830, grad/param norm = 3.8031e-02, time/batch = 0.1977s	
1847/2700 (epoch 34.204), train_loss = 0.08695821, grad/param norm = 4.2842e-02, time/batch = 0.1975s	
1848/2700 (epoch 34.222), train_loss = 0.08869991, grad/param norm = 5.7018e-02, time/batch = 0.1974s	
1849/2700 (epoch 34.241), train_loss = 0.08884244, grad/param norm = 4.6438e-02, time/batch = 0.1976s	
1850/2700 (epoch 34.259), train_loss = 0.09757517, grad/param norm = 4.5710e-02, time/batch = 0.1979s	
1851/2700 (epoch 34.278), train_loss = 0.10202875, grad/param norm = 4.7217e-02, time/batch = 0.1977s	
1852/2700 (epoch 34.296), train_loss = 0.09647947, grad/param norm = 4.7772e-02, time/batch = 0.1975s	
1853/2700 (epoch 34.315), train_loss = 0.08375769, grad/param norm = 3.8793e-02, time/batch = 0.1976s	
1854/2700 (epoch 34.333), train_loss = 0.09287538, grad/param norm = 4.7444e-02, time/batch = 0.1978s	
1855/2700 (epoch 34.352), train_loss = 0.08460904, grad/param norm = 4.1204e-02, time/batch = 0.1977s	
1856/2700 (epoch 34.370), train_loss = 0.07603470, grad/param norm = 4.0875e-02, time/batch = 0.1972s	
1857/2700 (epoch 34.389), train_loss = 0.09397413, grad/param norm = 4.3245e-02, time/batch = 0.1980s	
1858/2700 (epoch 34.407), train_loss = 0.08981600, grad/param norm = 3.9573e-02, time/batch = 0.1970s	
1859/2700 (epoch 34.426), train_loss = 0.09318058, grad/param norm = 4.2018e-02, time/batch = 0.1978s	
1860/2700 (epoch 34.444), train_loss = 0.08948520, grad/param norm = 4.3708e-02, time/batch = 0.1977s	
1861/2700 (epoch 34.463), train_loss = 0.09475227, grad/param norm = 4.5324e-02, time/batch = 0.1987s	
1862/2700 (epoch 34.481), train_loss = 0.09286833, grad/param norm = 4.3333e-02, time/batch = 0.1971s	
1863/2700 (epoch 34.500), train_loss = 0.09074363, grad/param norm = 4.5602e-02, time/batch = 0.1968s	
1864/2700 (epoch 34.519), train_loss = 0.09177868, grad/param norm = 5.0861e-02, time/batch = 0.1967s	
1865/2700 (epoch 34.537), train_loss = 0.09243528, grad/param norm = 4.5466e-02, time/batch = 0.1971s	
1866/2700 (epoch 34.556), train_loss = 0.08423873, grad/param norm = 4.0823e-02, time/batch = 0.1965s	
1867/2700 (epoch 34.574), train_loss = 0.08573450, grad/param norm = 3.7138e-02, time/batch = 0.1972s	
1868/2700 (epoch 34.593), train_loss = 0.08405248, grad/param norm = 3.7460e-02, time/batch = 0.1968s	
1869/2700 (epoch 34.611), train_loss = 0.08347877, grad/param norm = 3.6144e-02, time/batch = 0.1967s	
1870/2700 (epoch 34.630), train_loss = 0.07669930, grad/param norm = 4.0004e-02, time/batch = 0.1968s	
1871/2700 (epoch 34.648), train_loss = 0.09392307, grad/param norm = 3.9205e-02, time/batch = 0.1974s	
1872/2700 (epoch 34.667), train_loss = 0.09075943, grad/param norm = 3.5795e-02, time/batch = 0.1964s	
1873/2700 (epoch 34.685), train_loss = 0.07995034, grad/param norm = 3.5635e-02, time/batch = 0.1967s	
1874/2700 (epoch 34.704), train_loss = 0.08342552, grad/param norm = 3.6841e-02, time/batch = 0.1976s	
1875/2700 (epoch 34.722), train_loss = 0.08403464, grad/param norm = 3.9561e-02, time/batch = 0.1976s	
1876/2700 (epoch 34.741), train_loss = 0.08890649, grad/param norm = 3.4252e-02, time/batch = 0.1970s	
1877/2700 (epoch 34.759), train_loss = 0.07765980, grad/param norm = 3.8144e-02, time/batch = 0.1972s	
1878/2700 (epoch 34.778), train_loss = 0.09765571, grad/param norm = 4.3537e-02, time/batch = 0.1966s	
1879/2700 (epoch 34.796), train_loss = 0.10413411, grad/param norm = 5.9004e-02, time/batch = 0.1972s	
1880/2700 (epoch 34.815), train_loss = 0.11027687, grad/param norm = 6.0582e-02, time/batch = 0.1968s	
1881/2700 (epoch 34.833), train_loss = 0.10811985, grad/param norm = 6.1880e-02, time/batch = 0.1977s	
1882/2700 (epoch 34.852), train_loss = 0.09894561, grad/param norm = 5.5632e-02, time/batch = 0.1963s	
1883/2700 (epoch 34.870), train_loss = 0.09508576, grad/param norm = 4.8327e-02, time/batch = 0.1971s	
1884/2700 (epoch 34.889), train_loss = 0.09101359, grad/param norm = 4.4039e-02, time/batch = 0.1967s	
1885/2700 (epoch 34.907), train_loss = 0.07762230, grad/param norm = 3.8726e-02, time/batch = 0.1975s	
1886/2700 (epoch 34.926), train_loss = 0.07981781, grad/param norm = 3.3362e-02, time/batch = 0.1966s	
1887/2700 (epoch 34.944), train_loss = 0.06797391, grad/param norm = 3.2317e-02, time/batch = 0.1968s	
1888/2700 (epoch 34.963), train_loss = 0.07981995, grad/param norm = 3.3683e-02, time/batch = 0.1966s	
1889/2700 (epoch 34.981), train_loss = 0.07320435, grad/param norm = 3.5996e-02, time/batch = 0.1967s	
decayed learning rate by a factor 0.97 to 0.00090593092819348	
1890/2700 (epoch 35.000), train_loss = 0.07439136, grad/param norm = 4.0311e-02, time/batch = 0.1969s	
1891/2700 (epoch 35.019), train_loss = 0.10361528, grad/param norm = 3.4932e-02, time/batch = 0.1978s	
1892/2700 (epoch 35.037), train_loss = 0.08107494, grad/param norm = 3.3464e-02, time/batch = 0.1971s	
1893/2700 (epoch 35.056), train_loss = 0.07696501, grad/param norm = 3.2832e-02, time/batch = 0.1968s	
1894/2700 (epoch 35.074), train_loss = 0.07837040, grad/param norm = 3.4885e-02, time/batch = 0.1975s	
1895/2700 (epoch 35.093), train_loss = 0.07063956, grad/param norm = 3.3887e-02, time/batch = 0.1974s	
1896/2700 (epoch 35.111), train_loss = 0.06898536, grad/param norm = 2.9420e-02, time/batch = 0.1972s	
1897/2700 (epoch 35.130), train_loss = 0.06847348, grad/param norm = 3.4350e-02, time/batch = 0.1972s	
1898/2700 (epoch 35.148), train_loss = 0.07615413, grad/param norm = 3.5189e-02, time/batch = 0.1964s	
1899/2700 (epoch 35.167), train_loss = 0.07662733, grad/param norm = 3.1832e-02, time/batch = 0.1971s	
1900/2700 (epoch 35.185), train_loss = 0.06635405, grad/param norm = 3.3315e-02, time/batch = 0.1971s	
1901/2700 (epoch 35.204), train_loss = 0.06888173, grad/param norm = 3.3058e-02, time/batch = 0.1976s	
1902/2700 (epoch 35.222), train_loss = 0.06132726, grad/param norm = 3.2958e-02, time/batch = 0.1967s	
1903/2700 (epoch 35.241), train_loss = 0.06519641, grad/param norm = 3.5122e-02, time/batch = 0.1968s	
1904/2700 (epoch 35.259), train_loss = 0.08154966, grad/param norm = 4.1354e-02, time/batch = 0.1968s	
1905/2700 (epoch 35.278), train_loss = 0.09395518, grad/param norm = 4.2672e-02, time/batch = 0.1977s	
1906/2700 (epoch 35.296), train_loss = 0.07917856, grad/param norm = 3.5826e-02, time/batch = 0.1965s	
1907/2700 (epoch 35.315), train_loss = 0.06888773, grad/param norm = 2.9368e-02, time/batch = 0.1971s	
1908/2700 (epoch 35.333), train_loss = 0.07442155, grad/param norm = 3.7501e-02, time/batch = 0.1961s	
1909/2700 (epoch 35.352), train_loss = 0.07013326, grad/param norm = 3.3234e-02, time/batch = 0.1969s	
1910/2700 (epoch 35.370), train_loss = 0.06109596, grad/param norm = 3.0342e-02, time/batch = 0.1969s	
1911/2700 (epoch 35.389), train_loss = 0.08288887, grad/param norm = 3.7848e-02, time/batch = 0.1977s	
1912/2700 (epoch 35.407), train_loss = 0.07672274, grad/param norm = 3.5422e-02, time/batch = 0.1965s	
1913/2700 (epoch 35.426), train_loss = 0.07996132, grad/param norm = 4.2680e-02, time/batch = 0.1970s	
1914/2700 (epoch 35.444), train_loss = 0.08239842, grad/param norm = 4.3992e-02, time/batch = 0.1966s	
1915/2700 (epoch 35.463), train_loss = 0.10269612, grad/param norm = 5.7325e-02, time/batch = 0.1976s	
1916/2700 (epoch 35.481), train_loss = 0.11768620, grad/param norm = 7.3188e-02, time/batch = 0.1972s	
1917/2700 (epoch 35.500), train_loss = 0.11752162, grad/param norm = 7.6418e-02, time/batch = 0.1974s	
1918/2700 (epoch 35.519), train_loss = 0.11482009, grad/param norm = 7.5439e-02, time/batch = 0.1970s	
1919/2700 (epoch 35.537), train_loss = 0.11083814, grad/param norm = 6.4017e-02, time/batch = 0.1970s	
1920/2700 (epoch 35.556), train_loss = 0.08978009, grad/param norm = 4.6802e-02, time/batch = 0.1969s	
1921/2700 (epoch 35.574), train_loss = 0.08304225, grad/param norm = 4.0753e-02, time/batch = 0.1982s	
1922/2700 (epoch 35.593), train_loss = 0.07877482, grad/param norm = 3.6549e-02, time/batch = 0.1971s	
1923/2700 (epoch 35.611), train_loss = 0.08181380, grad/param norm = 3.8820e-02, time/batch = 0.1970s	
1924/2700 (epoch 35.630), train_loss = 0.06834954, grad/param norm = 3.6265e-02, time/batch = 0.1969s	
1925/2700 (epoch 35.648), train_loss = 0.08086128, grad/param norm = 3.4665e-02, time/batch = 0.1974s	
1926/2700 (epoch 35.667), train_loss = 0.07952402, grad/param norm = 3.0727e-02, time/batch = 0.1974s	
1927/2700 (epoch 35.685), train_loss = 0.07285670, grad/param norm = 3.1908e-02, time/batch = 0.1972s	
1928/2700 (epoch 35.704), train_loss = 0.07614496, grad/param norm = 3.9010e-02, time/batch = 0.1966s	
1929/2700 (epoch 35.722), train_loss = 0.07178078, grad/param norm = 3.8451e-02, time/batch = 0.1969s	
1930/2700 (epoch 35.741), train_loss = 0.07437112, grad/param norm = 3.1389e-02, time/batch = 0.1973s	
1931/2700 (epoch 35.759), train_loss = 0.07166252, grad/param norm = 4.1249e-02, time/batch = 0.1973s	
1932/2700 (epoch 35.778), train_loss = 0.08074670, grad/param norm = 3.3152e-02, time/batch = 0.1969s	
1933/2700 (epoch 35.796), train_loss = 0.07450601, grad/param norm = 3.8560e-02, time/batch = 0.1969s	
1934/2700 (epoch 35.815), train_loss = 0.08253892, grad/param norm = 4.0833e-02, time/batch = 0.1973s	
1935/2700 (epoch 35.833), train_loss = 0.07757558, grad/param norm = 4.5328e-02, time/batch = 0.2179s	
1936/2700 (epoch 35.852), train_loss = 0.08670174, grad/param norm = 5.4093e-02, time/batch = 0.2210s	
1937/2700 (epoch 35.870), train_loss = 0.09837917, grad/param norm = 5.8120e-02, time/batch = 0.2121s	
1938/2700 (epoch 35.889), train_loss = 0.08395891, grad/param norm = 4.9618e-02, time/batch = 0.2127s	
1939/2700 (epoch 35.907), train_loss = 0.08546799, grad/param norm = 5.4920e-02, time/batch = 0.2158s	
1940/2700 (epoch 35.926), train_loss = 0.07854015, grad/param norm = 4.2746e-02, time/batch = 0.2166s	
1941/2700 (epoch 35.944), train_loss = 0.06595479, grad/param norm = 3.3946e-02, time/batch = 0.2247s	
1942/2700 (epoch 35.963), train_loss = 0.07304954, grad/param norm = 3.2170e-02, time/batch = 0.2232s	
1943/2700 (epoch 35.981), train_loss = 0.07021923, grad/param norm = 3.3903e-02, time/batch = 0.2235s	
decayed learning rate by a factor 0.97 to 0.00087875300034768	
1944/2700 (epoch 36.000), train_loss = 0.06314167, grad/param norm = 3.7169e-02, time/batch = 0.2237s	
1945/2700 (epoch 36.019), train_loss = 0.09807806, grad/param norm = 3.7470e-02, time/batch = 0.2240s	
1946/2700 (epoch 36.037), train_loss = 0.07546001, grad/param norm = 3.4720e-02, time/batch = 0.2231s	
1947/2700 (epoch 36.056), train_loss = 0.06513036, grad/param norm = 3.0328e-02, time/batch = 0.2236s	
1948/2700 (epoch 36.074), train_loss = 0.06866864, grad/param norm = 3.3853e-02, time/batch = 0.2239s	
1949/2700 (epoch 36.093), train_loss = 0.06340099, grad/param norm = 3.6446e-02, time/batch = 0.2236s	
1950/2700 (epoch 36.111), train_loss = 0.06255030, grad/param norm = 3.1211e-02, time/batch = 0.2241s	
1951/2700 (epoch 36.130), train_loss = 0.05709691, grad/param norm = 3.1721e-02, time/batch = 0.2245s	
1952/2700 (epoch 36.148), train_loss = 0.06436914, grad/param norm = 2.7963e-02, time/batch = 0.2236s	
1953/2700 (epoch 36.167), train_loss = 0.06591807, grad/param norm = 2.9363e-02, time/batch = 0.2233s	
1954/2700 (epoch 36.185), train_loss = 0.05034052, grad/param norm = 2.4535e-02, time/batch = 0.2238s	
1955/2700 (epoch 36.204), train_loss = 0.05636397, grad/param norm = 3.1047e-02, time/batch = 0.2240s	
1956/2700 (epoch 36.222), train_loss = 0.05111812, grad/param norm = 2.7395e-02, time/batch = 0.2231s	
1957/2700 (epoch 36.241), train_loss = 0.05387497, grad/param norm = 3.4877e-02, time/batch = 0.2235s	
1958/2700 (epoch 36.259), train_loss = 0.06704873, grad/param norm = 3.5363e-02, time/batch = 0.2232s	
1959/2700 (epoch 36.278), train_loss = 0.07121328, grad/param norm = 3.3732e-02, time/batch = 0.2238s	
1960/2700 (epoch 36.296), train_loss = 0.06386564, grad/param norm = 2.9642e-02, time/batch = 0.2238s	
1961/2700 (epoch 36.315), train_loss = 0.05879546, grad/param norm = 3.0632e-02, time/batch = 0.2242s	
1962/2700 (epoch 36.333), train_loss = 0.05857539, grad/param norm = 3.4571e-02, time/batch = 0.2235s	
1963/2700 (epoch 36.352), train_loss = 0.05497822, grad/param norm = 2.8332e-02, time/batch = 0.2233s	
1964/2700 (epoch 36.370), train_loss = 0.05033018, grad/param norm = 2.8465e-02, time/batch = 0.2238s	
1965/2700 (epoch 36.389), train_loss = 0.06500852, grad/param norm = 3.0335e-02, time/batch = 0.2233s	
1966/2700 (epoch 36.407), train_loss = 0.06227463, grad/param norm = 3.3590e-02, time/batch = 0.2199s	
1967/2700 (epoch 36.426), train_loss = 0.06322124, grad/param norm = 3.1141e-02, time/batch = 0.2205s	
1968/2700 (epoch 36.444), train_loss = 0.05891942, grad/param norm = 2.8560e-02, time/batch = 0.2204s	
1969/2700 (epoch 36.463), train_loss = 0.07221018, grad/param norm = 3.9242e-02, time/batch = 0.2206s	
1970/2700 (epoch 36.481), train_loss = 0.06961568, grad/param norm = 4.0566e-02, time/batch = 0.2205s	
1971/2700 (epoch 36.500), train_loss = 0.07228685, grad/param norm = 4.3583e-02, time/batch = 0.2211s	
1972/2700 (epoch 36.519), train_loss = 0.08089832, grad/param norm = 5.4684e-02, time/batch = 0.2212s	
1973/2700 (epoch 36.537), train_loss = 0.08568611, grad/param norm = 5.0483e-02, time/batch = 0.2200s	
1974/2700 (epoch 36.556), train_loss = 0.08250458, grad/param norm = 5.0799e-02, time/batch = 0.2211s	
1975/2700 (epoch 36.574), train_loss = 0.08027038, grad/param norm = 4.5503e-02, time/batch = 0.2209s	
1976/2700 (epoch 36.593), train_loss = 0.07187249, grad/param norm = 4.0894e-02, time/batch = 0.2205s	
1977/2700 (epoch 36.611), train_loss = 0.07279780, grad/param norm = 4.0270e-02, time/batch = 0.2207s	
1978/2700 (epoch 36.630), train_loss = 0.06463405, grad/param norm = 3.5487e-02, time/batch = 0.2207s	
1979/2700 (epoch 36.648), train_loss = 0.07547441, grad/param norm = 3.4777e-02, time/batch = 0.2208s	
1980/2700 (epoch 36.667), train_loss = 0.07003658, grad/param norm = 2.8718e-02, time/batch = 0.2217s	
1981/2700 (epoch 36.685), train_loss = 0.06477909, grad/param norm = 2.9912e-02, time/batch = 0.2246s	
1982/2700 (epoch 36.704), train_loss = 0.06093380, grad/param norm = 2.9940e-02, time/batch = 0.2231s	
1983/2700 (epoch 36.722), train_loss = 0.05890943, grad/param norm = 3.5549e-02, time/batch = 0.2235s	
1984/2700 (epoch 36.741), train_loss = 0.06496759, grad/param norm = 2.6249e-02, time/batch = 0.2233s	
1985/2700 (epoch 36.759), train_loss = 0.05996759, grad/param norm = 3.1360e-02, time/batch = 0.2242s	
1986/2700 (epoch 36.778), train_loss = 0.06721062, grad/param norm = 3.2628e-02, time/batch = 0.2232s	
1987/2700 (epoch 36.796), train_loss = 0.06477526, grad/param norm = 3.1035e-02, time/batch = 0.2236s	
1988/2700 (epoch 36.815), train_loss = 0.06057553, grad/param norm = 2.8628e-02, time/batch = 0.2234s	
1989/2700 (epoch 36.833), train_loss = 0.06504393, grad/param norm = 3.4864e-02, time/batch = 0.2238s	
1990/2700 (epoch 36.852), train_loss = 0.06182753, grad/param norm = 4.2780e-02, time/batch = 0.2239s	
1991/2700 (epoch 36.870), train_loss = 0.07069014, grad/param norm = 4.3697e-02, time/batch = 0.2235s	
1992/2700 (epoch 36.889), train_loss = 0.06983844, grad/param norm = 4.2216e-02, time/batch = 0.2231s	
1993/2700 (epoch 36.907), train_loss = 0.07126467, grad/param norm = 4.2525e-02, time/batch = 0.2236s	
1994/2700 (epoch 36.926), train_loss = 0.06713921, grad/param norm = 3.6913e-02, time/batch = 0.2236s	
1995/2700 (epoch 36.944), train_loss = 0.05797759, grad/param norm = 3.5662e-02, time/batch = 0.2236s	
1996/2700 (epoch 36.963), train_loss = 0.06630722, grad/param norm = 3.5954e-02, time/batch = 0.2230s	
1997/2700 (epoch 36.981), train_loss = 0.05609444, grad/param norm = 3.0578e-02, time/batch = 0.2238s	
decayed learning rate by a factor 0.97 to 0.00085239041033725	
1998/2700 (epoch 37.000), train_loss = 0.04988822, grad/param norm = 2.5934e-02, time/batch = 0.2229s	
1999/2700 (epoch 37.019), train_loss = 0.08388006, grad/param norm = 3.0300e-02, time/batch = 0.2236s	
evaluating loss over split index 2	
1/3...	
2/3...	
3/3...	
saving checkpoint to cv/lm_lstm_epoch37.04_2.6966.t7	
2000/2700 (epoch 37.037), train_loss = 0.07089083, grad/param norm = 3.7492e-02, time/batch = 0.2237s	
2001/2700 (epoch 37.056), train_loss = 0.91816732, grad/param norm = 1.6595e-01, time/batch = 0.2234s	
2002/2700 (epoch 37.074), train_loss = 0.17624196, grad/param norm = 1.1301e-01, time/batch = 0.2228s	
2003/2700 (epoch 37.093), train_loss = 0.12646196, grad/param norm = 8.4036e-02, time/batch = 0.2234s	
2004/2700 (epoch 37.111), train_loss = 0.09452425, grad/param norm = 5.6429e-02, time/batch = 0.2236s	
2005/2700 (epoch 37.130), train_loss = 0.08154658, grad/param norm = 4.7726e-02, time/batch = 0.2238s	
2006/2700 (epoch 37.148), train_loss = 0.07769131, grad/param norm = 4.1510e-02, time/batch = 0.2235s	
2007/2700 (epoch 37.167), train_loss = 0.06992958, grad/param norm = 3.7757e-02, time/batch = 0.2236s	
2008/2700 (epoch 37.185), train_loss = 0.06020726, grad/param norm = 3.4025e-02, time/batch = 0.2235s	
2009/2700 (epoch 37.204), train_loss = 0.06554831, grad/param norm = 3.7991e-02, time/batch = 0.2240s	
2010/2700 (epoch 37.222), train_loss = 0.05429701, grad/param norm = 3.5944e-02, time/batch = 0.2234s	
2011/2700 (epoch 37.241), train_loss = 0.05843718, grad/param norm = 3.4173e-02, time/batch = 0.2213s	
2012/2700 (epoch 37.259), train_loss = 0.06536983, grad/param norm = 3.4261e-02, time/batch = 0.2207s	
2013/2700 (epoch 37.278), train_loss = 0.06740892, grad/param norm = 3.2762e-02, time/batch = 0.2201s	
2014/2700 (epoch 37.296), train_loss = 0.06214786, grad/param norm = 3.3928e-02, time/batch = 0.2206s	
2015/2700 (epoch 37.315), train_loss = 0.06167489, grad/param norm = 3.0932e-02, time/batch = 0.2210s	
2016/2700 (epoch 37.333), train_loss = 0.05877472, grad/param norm = 3.0759e-02, time/batch = 0.2209s	
2017/2700 (epoch 37.352), train_loss = 0.05159534, grad/param norm = 2.5940e-02, time/batch = 0.2216s	
2018/2700 (epoch 37.370), train_loss = 0.04646054, grad/param norm = 2.7755e-02, time/batch = 0.2208s	
2019/2700 (epoch 37.389), train_loss = 0.06344555, grad/param norm = 3.6003e-02, time/batch = 0.2212s	
2020/2700 (epoch 37.407), train_loss = 0.05499769, grad/param norm = 2.5979e-02, time/batch = 0.2215s	
2021/2700 (epoch 37.426), train_loss = 0.05773672, grad/param norm = 3.0797e-02, time/batch = 0.2218s	
2022/2700 (epoch 37.444), train_loss = 0.05590457, grad/param norm = 2.9291e-02, time/batch = 0.2210s	
2023/2700 (epoch 37.463), train_loss = 0.05935778, grad/param norm = 3.0651e-02, time/batch = 0.2208s	
2024/2700 (epoch 37.481), train_loss = 0.05526251, grad/param norm = 2.5231e-02, time/batch = 0.2209s	
2025/2700 (epoch 37.500), train_loss = 0.05356356, grad/param norm = 2.8167e-02, time/batch = 0.2216s	
2026/2700 (epoch 37.519), train_loss = 0.06086698, grad/param norm = 3.4385e-02, time/batch = 0.2205s	
2027/2700 (epoch 37.537), train_loss = 0.06584860, grad/param norm = 3.3234e-02, time/batch = 0.2208s	
2028/2700 (epoch 37.556), train_loss = 0.06115179, grad/param norm = 3.1446e-02, time/batch = 0.2207s	
2029/2700 (epoch 37.574), train_loss = 0.05793317, grad/param norm = 3.2744e-02, time/batch = 0.2169s	
2030/2700 (epoch 37.593), train_loss = 0.05877017, grad/param norm = 3.3715e-02, time/batch = 0.2138s	
2031/2700 (epoch 37.611), train_loss = 0.06887000, grad/param norm = 3.5269e-02, time/batch = 0.2246s	
2032/2700 (epoch 37.630), train_loss = 0.05072969, grad/param norm = 3.3975e-02, time/batch = 0.2217s	
2033/2700 (epoch 37.648), train_loss = 0.06776244, grad/param norm = 3.5101e-02, time/batch = 0.2236s	
2034/2700 (epoch 37.667), train_loss = 0.06652081, grad/param norm = 2.7844e-02, time/batch = 0.2228s	
2035/2700 (epoch 37.685), train_loss = 0.05197379, grad/param norm = 2.4688e-02, time/batch = 0.2240s	
2036/2700 (epoch 37.704), train_loss = 0.05544300, grad/param norm = 2.7383e-02, time/batch = 0.2235s	
2037/2700 (epoch 37.722), train_loss = 0.05076283, grad/param norm = 2.8009e-02, time/batch = 0.2241s	
2038/2700 (epoch 37.741), train_loss = 0.05529320, grad/param norm = 2.6182e-02, time/batch = 0.2236s	
2039/2700 (epoch 37.759), train_loss = 0.05267952, grad/param norm = 3.8153e-02, time/batch = 0.2238s	
2040/2700 (epoch 37.778), train_loss = 0.05523337, grad/param norm = 2.9782e-02, time/batch = 0.2247s	
2041/2700 (epoch 37.796), train_loss = 0.05501488, grad/param norm = 3.2300e-02, time/batch = 0.2244s	
2042/2700 (epoch 37.815), train_loss = 0.05588570, grad/param norm = 2.7923e-02, time/batch = 0.2238s	
2043/2700 (epoch 37.833), train_loss = 0.05436464, grad/param norm = 2.9324e-02, time/batch = 0.2237s	
2044/2700 (epoch 37.852), train_loss = 0.05199853, grad/param norm = 2.9826e-02, time/batch = 0.2237s	
2045/2700 (epoch 37.870), train_loss = 0.05958130, grad/param norm = 3.3152e-02, time/batch = 0.2239s	
2046/2700 (epoch 37.889), train_loss = 0.05650095, grad/param norm = 3.5045e-02, time/batch = 0.2236s	
2047/2700 (epoch 37.907), train_loss = 0.05677380, grad/param norm = 4.0314e-02, time/batch = 0.2236s	
2048/2700 (epoch 37.926), train_loss = 0.06050942, grad/param norm = 3.7313e-02, time/batch = 0.2238s	
2049/2700 (epoch 37.944), train_loss = 0.04723183, grad/param norm = 3.0934e-02, time/batch = 0.2235s	
2050/2700 (epoch 37.963), train_loss = 0.05898259, grad/param norm = 3.3286e-02, time/batch = 0.2237s	
2051/2700 (epoch 37.981), train_loss = 0.05177093, grad/param norm = 2.8174e-02, time/batch = 0.2246s	
decayed learning rate by a factor 0.97 to 0.00082681869802713	
2052/2700 (epoch 38.000), train_loss = 0.04505713, grad/param norm = 2.9924e-02, time/batch = 0.2238s	
2053/2700 (epoch 38.019), train_loss = 0.07758916, grad/param norm = 3.2644e-02, time/batch = 0.2239s	
2054/2700 (epoch 38.037), train_loss = 0.06182528, grad/param norm = 3.3824e-02, time/batch = 0.2242s	
2055/2700 (epoch 38.056), train_loss = 0.06147640, grad/param norm = 3.4587e-02, time/batch = 0.2239s	
2056/2700 (epoch 38.074), train_loss = 0.06683171, grad/param norm = 4.2932e-02, time/batch = 0.2230s	
2057/2700 (epoch 38.093), train_loss = 0.06981966, grad/param norm = 4.8406e-02, time/batch = 0.2239s	
2058/2700 (epoch 38.111), train_loss = 0.06804952, grad/param norm = 4.8532e-02, time/batch = 0.2237s	
2059/2700 (epoch 38.130), train_loss = 0.06722219, grad/param norm = 5.0380e-02, time/batch = 0.2245s	
2060/2700 (epoch 38.148), train_loss = 0.07712133, grad/param norm = 4.5241e-02, time/batch = 0.2245s	
2061/2700 (epoch 38.167), train_loss = 0.06388218, grad/param norm = 3.5515e-02, time/batch = 0.2234s	
2062/2700 (epoch 38.185), train_loss = 0.04725606, grad/param norm = 2.8296e-02, time/batch = 0.2208s	
2063/2700 (epoch 38.204), train_loss = 0.05286458, grad/param norm = 2.8562e-02, time/batch = 0.2209s	
2064/2700 (epoch 38.222), train_loss = 0.04038633, grad/param norm = 2.5642e-02, time/batch = 0.2210s	
2065/2700 (epoch 38.241), train_loss = 0.04097894, grad/param norm = 2.3712e-02, time/batch = 0.2211s	
2066/2700 (epoch 38.259), train_loss = 0.05387650, grad/param norm = 2.3737e-02, time/batch = 0.2201s	
2067/2700 (epoch 38.278), train_loss = 0.05095191, grad/param norm = 2.3270e-02, time/batch = 0.2207s	
2068/2700 (epoch 38.296), train_loss = 0.04798835, grad/param norm = 2.3538e-02, time/batch = 0.2201s	
2069/2700 (epoch 38.315), train_loss = 0.04569618, grad/param norm = 2.3643e-02, time/batch = 0.2202s	
2070/2700 (epoch 38.333), train_loss = 0.04346947, grad/param norm = 2.4631e-02, time/batch = 0.2211s	
2071/2700 (epoch 38.352), train_loss = 0.04141349, grad/param norm = 1.9612e-02, time/batch = 0.2217s	
2072/2700 (epoch 38.370), train_loss = 0.03738892, grad/param norm = 2.2296e-02, time/batch = 0.2214s	
2073/2700 (epoch 38.389), train_loss = 0.04743633, grad/param norm = 2.2949e-02, time/batch = 0.2209s	
2074/2700 (epoch 38.407), train_loss = 0.04625978, grad/param norm = 2.1715e-02, time/batch = 0.2211s	
2075/2700 (epoch 38.426), train_loss = 0.04335409, grad/param norm = 2.2081e-02, time/batch = 0.2215s	
2076/2700 (epoch 38.444), train_loss = 0.04728780, grad/param norm = 2.4500e-02, time/batch = 0.2209s	
2077/2700 (epoch 38.463), train_loss = 0.04634038, grad/param norm = 2.1508e-02, time/batch = 0.2210s	
2078/2700 (epoch 38.481), train_loss = 0.03981054, grad/param norm = 1.9604e-02, time/batch = 0.2205s	
2079/2700 (epoch 38.500), train_loss = 0.04148910, grad/param norm = 2.2339e-02, time/batch = 0.2208s	
2080/2700 (epoch 38.519), train_loss = 0.04288761, grad/param norm = 2.3993e-02, time/batch = 0.2208s	
2081/2700 (epoch 38.537), train_loss = 0.04377384, grad/param norm = 2.5258e-02, time/batch = 0.2150s	
2082/2700 (epoch 38.556), train_loss = 0.04513441, grad/param norm = 2.1032e-02, time/batch = 0.2229s	
2083/2700 (epoch 38.574), train_loss = 0.04585603, grad/param norm = 2.2565e-02, time/batch = 0.2237s	
2084/2700 (epoch 38.593), train_loss = 0.04523943, grad/param norm = 2.5840e-02, time/batch = 0.2236s	
2085/2700 (epoch 38.611), train_loss = 0.04898692, grad/param norm = 2.4551e-02, time/batch = 0.2239s	
2086/2700 (epoch 38.630), train_loss = 0.04217772, grad/param norm = 2.3867e-02, time/batch = 0.2238s	
2087/2700 (epoch 38.648), train_loss = 0.04934596, grad/param norm = 2.3249e-02, time/batch = 0.2243s	
2088/2700 (epoch 38.667), train_loss = 0.05093253, grad/param norm = 1.9919e-02, time/batch = 0.2237s	
2089/2700 (epoch 38.685), train_loss = 0.04417187, grad/param norm = 2.0038e-02, time/batch = 0.2242s	
2090/2700 (epoch 38.704), train_loss = 0.04412821, grad/param norm = 2.1732e-02, time/batch = 0.2241s	
2091/2700 (epoch 38.722), train_loss = 0.04304390, grad/param norm = 2.0162e-02, time/batch = 0.2240s	
2092/2700 (epoch 38.741), train_loss = 0.04528408, grad/param norm = 1.9364e-02, time/batch = 0.2234s	
2093/2700 (epoch 38.759), train_loss = 0.04364490, grad/param norm = 2.4203e-02, time/batch = 0.2235s	
2094/2700 (epoch 38.778), train_loss = 0.04487988, grad/param norm = 2.1110e-02, time/batch = 0.2239s	
2095/2700 (epoch 38.796), train_loss = 0.04615388, grad/param norm = 2.5179e-02, time/batch = 0.2242s	
2096/2700 (epoch 38.815), train_loss = 0.04486930, grad/param norm = 2.3553e-02, time/batch = 0.2227s	
2097/2700 (epoch 38.833), train_loss = 0.04072404, grad/param norm = 2.0232e-02, time/batch = 0.2237s	
2098/2700 (epoch 38.852), train_loss = 0.04011224, grad/param norm = 2.4688e-02, time/batch = 0.2234s	
2099/2700 (epoch 38.870), train_loss = 0.04159210, grad/param norm = 2.4932e-02, time/batch = 0.2236s	
2100/2700 (epoch 38.889), train_loss = 0.03961690, grad/param norm = 2.2034e-02, time/batch = 0.2241s	
2101/2700 (epoch 38.907), train_loss = 0.04247329, grad/param norm = 2.6011e-02, time/batch = 0.2243s	
2102/2700 (epoch 38.926), train_loss = 0.04451195, grad/param norm = 2.5595e-02, time/batch = 0.2233s	
2103/2700 (epoch 38.944), train_loss = 0.04271619, grad/param norm = 2.8419e-02, time/batch = 0.2232s	
2104/2700 (epoch 38.963), train_loss = 0.04939460, grad/param norm = 2.7404e-02, time/batch = 0.2229s	
2105/2700 (epoch 38.981), train_loss = 0.04965396, grad/param norm = 2.9250e-02, time/batch = 0.2234s	
decayed learning rate by a factor 0.97 to 0.00080201413708631	
2106/2700 (epoch 39.000), train_loss = 0.04255055, grad/param norm = 3.4779e-02, time/batch = 0.2234s	
2107/2700 (epoch 39.019), train_loss = 0.07293663, grad/param norm = 3.3051e-02, time/batch = 0.2235s	
2108/2700 (epoch 39.037), train_loss = 0.06371564, grad/param norm = 3.8411e-02, time/batch = 0.2233s	
2109/2700 (epoch 39.056), train_loss = 0.05437106, grad/param norm = 2.9893e-02, time/batch = 0.2235s	
2110/2700 (epoch 39.074), train_loss = 0.05475760, grad/param norm = 3.5715e-02, time/batch = 0.2236s	
2111/2700 (epoch 39.093), train_loss = 0.05519533, grad/param norm = 3.4403e-02, time/batch = 0.2241s	
2112/2700 (epoch 39.111), train_loss = 0.05465180, grad/param norm = 3.1372e-02, time/batch = 0.2234s	
2113/2700 (epoch 39.130), train_loss = 0.05142773, grad/param norm = 3.9472e-02, time/batch = 0.2238s	
2114/2700 (epoch 39.148), train_loss = 0.05412473, grad/param norm = 3.4123e-02, time/batch = 0.2210s	
2115/2700 (epoch 39.167), train_loss = 0.06200324, grad/param norm = 3.9858e-02, time/batch = 0.2214s	
2116/2700 (epoch 39.185), train_loss = 0.04792577, grad/param norm = 3.6443e-02, time/batch = 0.2207s	
2117/2700 (epoch 39.204), train_loss = 0.05300038, grad/param norm = 3.6982e-02, time/batch = 0.2211s	
2118/2700 (epoch 39.222), train_loss = 0.04594398, grad/param norm = 3.8656e-02, time/batch = 0.2206s	
2119/2700 (epoch 39.241), train_loss = 0.04752594, grad/param norm = 4.2051e-02, time/batch = 0.2211s	
2120/2700 (epoch 39.259), train_loss = 0.05381904, grad/param norm = 2.8384e-02, time/batch = 0.2214s	
2121/2700 (epoch 39.278), train_loss = 0.05575650, grad/param norm = 3.0972e-02, time/batch = 0.2220s	
2122/2700 (epoch 39.296), train_loss = 0.04711685, grad/param norm = 2.7050e-02, time/batch = 0.2205s	
2123/2700 (epoch 39.315), train_loss = 0.04905282, grad/param norm = 2.5542e-02, time/batch = 0.2210s	
2124/2700 (epoch 39.333), train_loss = 0.04386540, grad/param norm = 2.5572e-02, time/batch = 0.2207s	
2125/2700 (epoch 39.352), train_loss = 0.04277794, grad/param norm = 2.4604e-02, time/batch = 0.2208s	
2126/2700 (epoch 39.370), train_loss = 0.03705702, grad/param norm = 2.3052e-02, time/batch = 0.2207s	
2127/2700 (epoch 39.389), train_loss = 0.04617241, grad/param norm = 2.2670e-02, time/batch = 0.2212s	
2128/2700 (epoch 39.407), train_loss = 0.04127132, grad/param norm = 1.9614e-02, time/batch = 0.2202s	
2129/2700 (epoch 39.426), train_loss = 0.04102412, grad/param norm = 2.1242e-02, time/batch = 0.2213s	
2130/2700 (epoch 39.444), train_loss = 0.03878342, grad/param norm = 2.0671e-02, time/batch = 0.2073s	
2131/2700 (epoch 39.463), train_loss = 0.04317718, grad/param norm = 2.1709e-02, time/batch = 0.2133s	
2132/2700 (epoch 39.481), train_loss = 0.04048122, grad/param norm = 2.8041e-02, time/batch = 0.2169s	
2133/2700 (epoch 39.500), train_loss = 0.03922800, grad/param norm = 2.1817e-02, time/batch = 0.2197s	
2134/2700 (epoch 39.519), train_loss = 0.04199252, grad/param norm = 2.8248e-02, time/batch = 0.2168s	
2135/2700 (epoch 39.537), train_loss = 0.04221977, grad/param norm = 2.2275e-02, time/batch = 0.2137s	
2136/2700 (epoch 39.556), train_loss = 0.04122548, grad/param norm = 2.0758e-02, time/batch = 0.2120s	
2137/2700 (epoch 39.574), train_loss = 0.03778758, grad/param norm = 2.2587e-02, time/batch = 0.2198s	
2138/2700 (epoch 39.593), train_loss = 0.04154539, grad/param norm = 2.7165e-02, time/batch = 0.2274s	
2139/2700 (epoch 39.611), train_loss = 0.05031579, grad/param norm = 2.7053e-02, time/batch = 0.2271s	
2140/2700 (epoch 39.630), train_loss = 0.03731544, grad/param norm = 2.5624e-02, time/batch = 0.2267s	
2141/2700 (epoch 39.648), train_loss = 0.05117503, grad/param norm = 2.7546e-02, time/batch = 0.2273s	
2142/2700 (epoch 39.667), train_loss = 0.05325154, grad/param norm = 2.7205e-02, time/batch = 0.2266s	
2143/2700 (epoch 39.685), train_loss = 0.04390534, grad/param norm = 2.3007e-02, time/batch = 0.2266s	
2144/2700 (epoch 39.704), train_loss = 0.04470138, grad/param norm = 2.8640e-02, time/batch = 0.2275s	
2145/2700 (epoch 39.722), train_loss = 0.04599681, grad/param norm = 3.2565e-02, time/batch = 0.2276s	
2146/2700 (epoch 39.741), train_loss = 0.04127006, grad/param norm = 2.1975e-02, time/batch = 0.2271s	
2147/2700 (epoch 39.759), train_loss = 0.04271419, grad/param norm = 3.3310e-02, time/batch = 0.2275s	
2148/2700 (epoch 39.778), train_loss = 0.04611306, grad/param norm = 2.6580e-02, time/batch = 0.2267s	
2149/2700 (epoch 39.796), train_loss = 0.04598988, grad/param norm = 2.7354e-02, time/batch = 0.2280s	
2150/2700 (epoch 39.815), train_loss = 0.04495289, grad/param norm = 2.5709e-02, time/batch = 0.2275s	
2151/2700 (epoch 39.833), train_loss = 0.04215618, grad/param norm = 2.6973e-02, time/batch = 0.2279s	
2152/2700 (epoch 39.852), train_loss = 0.04310263, grad/param norm = 2.9586e-02, time/batch = 0.2270s	
2153/2700 (epoch 39.870), train_loss = 0.04436323, grad/param norm = 3.0431e-02, time/batch = 0.2272s	
2154/2700 (epoch 39.889), train_loss = 0.03802909, grad/param norm = 2.3848e-02, time/batch = 0.2279s	
2155/2700 (epoch 39.907), train_loss = 0.04107092, grad/param norm = 2.4318e-02, time/batch = 0.2277s	
2156/2700 (epoch 39.926), train_loss = 0.04163006, grad/param norm = 2.5998e-02, time/batch = 0.2273s	
2157/2700 (epoch 39.944), train_loss = 0.03627890, grad/param norm = 2.4693e-02, time/batch = 0.2274s	
2158/2700 (epoch 39.963), train_loss = 0.04531662, grad/param norm = 3.0645e-02, time/batch = 0.2252s	
2159/2700 (epoch 39.981), train_loss = 0.04669175, grad/param norm = 3.3010e-02, time/batch = 0.2219s	
decayed learning rate by a factor 0.97 to 0.00077795371297373	
2160/2700 (epoch 40.000), train_loss = 0.04005278, grad/param norm = 3.3212e-02, time/batch = 0.2213s	
2161/2700 (epoch 40.019), train_loss = 0.06678797, grad/param norm = 3.4567e-02, time/batch = 0.2221s	
2162/2700 (epoch 40.037), train_loss = 0.05981518, grad/param norm = 3.7034e-02, time/batch = 0.2210s	
2163/2700 (epoch 40.056), train_loss = 0.05343071, grad/param norm = 3.2123e-02, time/batch = 0.2214s	
2164/2700 (epoch 40.074), train_loss = 0.04838263, grad/param norm = 2.8545e-02, time/batch = 0.2226s	
2165/2700 (epoch 40.093), train_loss = 0.04430162, grad/param norm = 2.9175e-02, time/batch = 0.2217s	
2166/2700 (epoch 40.111), train_loss = 0.04688307, grad/param norm = 3.2812e-02, time/batch = 0.2234s	
2167/2700 (epoch 40.130), train_loss = 0.04649555, grad/param norm = 3.5019e-02, time/batch = 0.2230s	
2168/2700 (epoch 40.148), train_loss = 0.04706134, grad/param norm = 3.1407e-02, time/batch = 0.2229s	
2169/2700 (epoch 40.167), train_loss = 0.05792053, grad/param norm = 3.7881e-02, time/batch = 0.2222s	
2170/2700 (epoch 40.185), train_loss = 0.04612756, grad/param norm = 3.6266e-02, time/batch = 0.2221s	
2171/2700 (epoch 40.204), train_loss = 0.04677233, grad/param norm = 3.2095e-02, time/batch = 0.2147s	
2172/2700 (epoch 40.222), train_loss = 0.04465229, grad/param norm = 3.5626e-02, time/batch = 0.2257s	
2173/2700 (epoch 40.241), train_loss = 0.05149021, grad/param norm = 4.1883e-02, time/batch = 0.2272s	
2174/2700 (epoch 40.259), train_loss = 0.05797621, grad/param norm = 4.1105e-02, time/batch = 0.2271s	
2175/2700 (epoch 40.278), train_loss = 0.05516749, grad/param norm = 3.7371e-02, time/batch = 0.2271s	
2176/2700 (epoch 40.296), train_loss = 0.04958497, grad/param norm = 3.6765e-02, time/batch = 0.2268s	
2177/2700 (epoch 40.315), train_loss = 0.04540746, grad/param norm = 3.4861e-02, time/batch = 0.2272s	
2178/2700 (epoch 40.333), train_loss = 0.04594120, grad/param norm = 3.5073e-02, time/batch = 0.2270s	
2179/2700 (epoch 40.352), train_loss = 0.04268558, grad/param norm = 2.6755e-02, time/batch = 0.2273s	
2180/2700 (epoch 40.370), train_loss = 0.04590150, grad/param norm = 3.1548e-02, time/batch = 0.2270s	
2181/2700 (epoch 40.389), train_loss = 0.04456580, grad/param norm = 2.6707e-02, time/batch = 0.2276s	
2182/2700 (epoch 40.407), train_loss = 0.04222659, grad/param norm = 2.4081e-02, time/batch = 0.2271s	
2183/2700 (epoch 40.426), train_loss = 0.04452895, grad/param norm = 3.0091e-02, time/batch = 0.2269s	
2184/2700 (epoch 40.444), train_loss = 0.03872215, grad/param norm = 2.2957e-02, time/batch = 0.2273s	
2185/2700 (epoch 40.463), train_loss = 0.04274120, grad/param norm = 2.2545e-02, time/batch = 0.2276s	
2186/2700 (epoch 40.481), train_loss = 0.04277245, grad/param norm = 2.8051e-02, time/batch = 0.2268s	
2187/2700 (epoch 40.500), train_loss = 0.04199269, grad/param norm = 2.7885e-02, time/batch = 0.2275s	
2188/2700 (epoch 40.519), train_loss = 0.04104633, grad/param norm = 2.5841e-02, time/batch = 0.2270s	
2189/2700 (epoch 40.537), train_loss = 0.04014030, grad/param norm = 2.3056e-02, time/batch = 0.2268s	
2190/2700 (epoch 40.556), train_loss = 0.03844450, grad/param norm = 2.3531e-02, time/batch = 0.2274s	
2191/2700 (epoch 40.574), train_loss = 0.03814223, grad/param norm = 2.1456e-02, time/batch = 0.2274s	
2192/2700 (epoch 40.593), train_loss = 0.03193787, grad/param norm = 2.1968e-02, time/batch = 0.2269s	
2193/2700 (epoch 40.611), train_loss = 0.04292362, grad/param norm = 2.9178e-02, time/batch = 0.2269s	
2194/2700 (epoch 40.630), train_loss = 0.03656742, grad/param norm = 3.2040e-02, time/batch = 0.2272s	
2195/2700 (epoch 40.648), train_loss = 0.04527644, grad/param norm = 2.7016e-02, time/batch = 0.2274s	
2196/2700 (epoch 40.667), train_loss = 0.04645482, grad/param norm = 2.9944e-02, time/batch = 0.2275s	
2197/2700 (epoch 40.685), train_loss = 0.03888925, grad/param norm = 2.7160e-02, time/batch = 0.2273s	
2198/2700 (epoch 40.704), train_loss = 0.04075070, grad/param norm = 2.6937e-02, time/batch = 0.2272s	
2199/2700 (epoch 40.722), train_loss = 0.04388464, grad/param norm = 3.5268e-02, time/batch = 0.2275s	
2200/2700 (epoch 40.741), train_loss = 0.04597163, grad/param norm = 3.1624e-02, time/batch = 0.2268s	
2201/2700 (epoch 40.759), train_loss = 0.04116372, grad/param norm = 3.0418e-02, time/batch = 0.2224s	
2202/2700 (epoch 40.778), train_loss = 0.04416898, grad/param norm = 2.7505e-02, time/batch = 0.2210s	
2203/2700 (epoch 40.796), train_loss = 0.04561350, grad/param norm = 2.7026e-02, time/batch = 0.2210s	
2204/2700 (epoch 40.815), train_loss = 0.04509654, grad/param norm = 3.3913e-02, time/batch = 0.2207s	
2205/2700 (epoch 40.833), train_loss = 0.04309438, grad/param norm = 2.9132e-02, time/batch = 0.2214s	
2206/2700 (epoch 40.852), train_loss = 0.03966584, grad/param norm = 2.7860e-02, time/batch = 0.2207s	
2207/2700 (epoch 40.870), train_loss = 0.03817174, grad/param norm = 2.4721e-02, time/batch = 0.2213s	
2208/2700 (epoch 40.889), train_loss = 0.03455038, grad/param norm = 2.2753e-02, time/batch = 0.2219s	
2209/2700 (epoch 40.907), train_loss = 0.03222663, grad/param norm = 2.0904e-02, time/batch = 0.2215s	
2210/2700 (epoch 40.926), train_loss = 0.03649319, grad/param norm = 2.3562e-02, time/batch = 0.2213s	
2211/2700 (epoch 40.944), train_loss = 0.03349425, grad/param norm = 2.3719e-02, time/batch = 0.2222s	
2212/2700 (epoch 40.963), train_loss = 0.04003711, grad/param norm = 3.3366e-02, time/batch = 0.2209s	
2213/2700 (epoch 40.981), train_loss = 0.04815316, grad/param norm = 3.8308e-02, time/batch = 0.2204s	
decayed learning rate by a factor 0.97 to 0.00075461510158451	
2214/2700 (epoch 41.000), train_loss = 0.04142703, grad/param norm = 4.1937e-02, time/batch = 0.2209s	
2215/2700 (epoch 41.019), train_loss = 0.06389610, grad/param norm = 3.6650e-02, time/batch = 0.2221s	
2216/2700 (epoch 41.037), train_loss = 0.04938102, grad/param norm = 3.3450e-02, time/batch = 0.2211s	
2217/2700 (epoch 41.056), train_loss = 0.04735630, grad/param norm = 3.5328e-02, time/batch = 0.2184s	
2218/2700 (epoch 41.074), train_loss = 0.04600696, grad/param norm = 3.1491e-02, time/batch = 0.2137s	
2219/2700 (epoch 41.093), train_loss = 0.03976616, grad/param norm = 2.6422e-02, time/batch = 0.2254s	
2220/2700 (epoch 41.111), train_loss = 0.04386997, grad/param norm = 3.1526e-02, time/batch = 0.2269s	
2221/2700 (epoch 41.130), train_loss = 0.04010928, grad/param norm = 3.1743e-02, time/batch = 0.2279s	
2222/2700 (epoch 41.148), train_loss = 0.03956107, grad/param norm = 2.7856e-02, time/batch = 0.2270s	
2223/2700 (epoch 41.167), train_loss = 0.04758753, grad/param norm = 3.0842e-02, time/batch = 0.2266s	
2224/2700 (epoch 41.185), train_loss = 0.03822546, grad/param norm = 3.0414e-02, time/batch = 0.2275s	
2225/2700 (epoch 41.204), train_loss = 0.04037988, grad/param norm = 3.3076e-02, time/batch = 0.2272s	
2226/2700 (epoch 41.222), train_loss = 0.03437082, grad/param norm = 2.8826e-02, time/batch = 0.2271s	
2227/2700 (epoch 41.241), train_loss = 0.04062557, grad/param norm = 3.1011e-02, time/batch = 0.2272s	
2228/2700 (epoch 41.259), train_loss = 0.04356145, grad/param norm = 3.0868e-02, time/batch = 0.2268s	
2229/2700 (epoch 41.278), train_loss = 0.05001818, grad/param norm = 3.1981e-02, time/batch = 0.2271s	
2230/2700 (epoch 41.296), train_loss = 0.04140509, grad/param norm = 3.0682e-02, time/batch = 0.2274s	
2231/2700 (epoch 41.315), train_loss = 0.03878508, grad/param norm = 3.1601e-02, time/batch = 0.2281s	
2232/2700 (epoch 41.333), train_loss = 0.03451611, grad/param norm = 2.4736e-02, time/batch = 0.2264s	
2233/2700 (epoch 41.352), train_loss = 0.03669196, grad/param norm = 2.7037e-02, time/batch = 0.2268s	
2234/2700 (epoch 41.370), train_loss = 0.03482596, grad/param norm = 2.7622e-02, time/batch = 0.2277s	
2235/2700 (epoch 41.389), train_loss = 0.04187182, grad/param norm = 2.8521e-02, time/batch = 0.2270s	
2236/2700 (epoch 41.407), train_loss = 0.04601910, grad/param norm = 3.2328e-02, time/batch = 0.2266s	
2237/2700 (epoch 41.426), train_loss = 0.04191894, grad/param norm = 3.1001e-02, time/batch = 0.2269s	
2238/2700 (epoch 41.444), train_loss = 0.04073170, grad/param norm = 2.9223e-02, time/batch = 0.2268s	
2239/2700 (epoch 41.463), train_loss = 0.04050830, grad/param norm = 2.4427e-02, time/batch = 0.2270s	
2240/2700 (epoch 41.481), train_loss = 0.04253784, grad/param norm = 3.4904e-02, time/batch = 0.2269s	
2241/2700 (epoch 41.500), train_loss = 0.03515667, grad/param norm = 2.8770e-02, time/batch = 0.2273s	
2242/2700 (epoch 41.519), train_loss = 0.04032607, grad/param norm = 2.7873e-02, time/batch = 0.2269s	
2243/2700 (epoch 41.537), train_loss = 0.03912824, grad/param norm = 2.6398e-02, time/batch = 0.2267s	
2244/2700 (epoch 41.556), train_loss = 0.03546182, grad/param norm = 2.4948e-02, time/batch = 0.2275s	
2245/2700 (epoch 41.574), train_loss = 0.03795666, grad/param norm = 2.7094e-02, time/batch = 0.2273s	
2246/2700 (epoch 41.593), train_loss = 0.03148327, grad/param norm = 2.3782e-02, time/batch = 0.2258s	
2247/2700 (epoch 41.611), train_loss = 0.03968041, grad/param norm = 2.7768e-02, time/batch = 0.2216s	
2248/2700 (epoch 41.630), train_loss = 0.03285800, grad/param norm = 2.5370e-02, time/batch = 0.2210s	
2249/2700 (epoch 41.648), train_loss = 0.04026195, grad/param norm = 2.3136e-02, time/batch = 0.2213s	
2250/2700 (epoch 41.667), train_loss = 0.04470762, grad/param norm = 3.4030e-02, time/batch = 0.2216s	
2251/2700 (epoch 41.685), train_loss = 0.04161720, grad/param norm = 3.1457e-02, time/batch = 0.2220s	
2252/2700 (epoch 41.704), train_loss = 0.03941841, grad/param norm = 2.8990e-02, time/batch = 0.2210s	
2253/2700 (epoch 41.722), train_loss = 0.04112410, grad/param norm = 3.9195e-02, time/batch = 0.2211s	
2254/2700 (epoch 41.741), train_loss = 0.04114419, grad/param norm = 2.8790e-02, time/batch = 0.2211s	
2255/2700 (epoch 41.759), train_loss = 0.03723144, grad/param norm = 3.0869e-02, time/batch = 0.2217s	
2256/2700 (epoch 41.778), train_loss = 0.04316185, grad/param norm = 3.3196e-02, time/batch = 0.2210s	
2257/2700 (epoch 41.796), train_loss = 0.04082911, grad/param norm = 2.8513e-02, time/batch = 0.2217s	
2258/2700 (epoch 41.815), train_loss = 0.04545183, grad/param norm = 3.4571e-02, time/batch = 0.2207s	
2259/2700 (epoch 41.833), train_loss = 0.03941514, grad/param norm = 2.5991e-02, time/batch = 0.2215s	
2260/2700 (epoch 41.852), train_loss = 0.03236192, grad/param norm = 2.6024e-02, time/batch = 0.2216s	
2261/2700 (epoch 41.870), train_loss = 0.03571697, grad/param norm = 3.2100e-02, time/batch = 0.2217s	
2262/2700 (epoch 41.889), train_loss = 0.03584289, grad/param norm = 2.8415e-02, time/batch = 0.2214s	
2263/2700 (epoch 41.907), train_loss = 0.03379384, grad/param norm = 2.4910e-02, time/batch = 0.2217s	
2264/2700 (epoch 41.926), train_loss = 0.03105171, grad/param norm = 1.9664e-02, time/batch = 0.2143s	
2265/2700 (epoch 41.944), train_loss = 0.02807374, grad/param norm = 1.7724e-02, time/batch = 0.2236s	
2266/2700 (epoch 41.963), train_loss = 0.03738964, grad/param norm = 2.4116e-02, time/batch = 0.2269s	
2267/2700 (epoch 41.981), train_loss = 0.03562581, grad/param norm = 2.8514e-02, time/batch = 0.2283s	
decayed learning rate by a factor 0.97 to 0.00073197664853698	
2268/2700 (epoch 42.000), train_loss = 0.03974633, grad/param norm = 3.3501e-02, time/batch = 0.2270s	
2269/2700 (epoch 42.019), train_loss = 0.05534522, grad/param norm = 3.5798e-02, time/batch = 0.2276s	
2270/2700 (epoch 42.037), train_loss = 0.04386282, grad/param norm = 3.3242e-02, time/batch = 0.2274s	
2271/2700 (epoch 42.056), train_loss = 0.04066516, grad/param norm = 3.5108e-02, time/batch = 0.2289s	
2272/2700 (epoch 42.074), train_loss = 0.03572286, grad/param norm = 2.4285e-02, time/batch = 0.2266s	
2273/2700 (epoch 42.093), train_loss = 0.03529759, grad/param norm = 2.1825e-02, time/batch = 0.2266s	
2274/2700 (epoch 42.111), train_loss = 0.03214832, grad/param norm = 2.5316e-02, time/batch = 0.2271s	
2275/2700 (epoch 42.130), train_loss = 0.03457431, grad/param norm = 2.2801e-02, time/batch = 0.2270s	
2276/2700 (epoch 42.148), train_loss = 0.03516205, grad/param norm = 2.4470e-02, time/batch = 0.2268s	
2277/2700 (epoch 42.167), train_loss = 0.04521398, grad/param norm = 3.2348e-02, time/batch = 0.2272s	
2278/2700 (epoch 42.185), train_loss = 0.03187422, grad/param norm = 2.8012e-02, time/batch = 0.2270s	
2279/2700 (epoch 42.204), train_loss = 0.03436547, grad/param norm = 2.5489e-02, time/batch = 0.2288s	
2280/2700 (epoch 42.222), train_loss = 0.02841724, grad/param norm = 2.5450e-02, time/batch = 0.2281s	
2281/2700 (epoch 42.241), train_loss = 0.03070562, grad/param norm = 2.5599e-02, time/batch = 0.2283s	
2282/2700 (epoch 42.259), train_loss = 0.03560431, grad/param norm = 2.4837e-02, time/batch = 0.2275s	
2283/2700 (epoch 42.278), train_loss = 0.03817902, grad/param norm = 2.3106e-02, time/batch = 0.2273s	
2284/2700 (epoch 42.296), train_loss = 0.03529676, grad/param norm = 2.8896e-02, time/batch = 0.2274s	
2285/2700 (epoch 42.315), train_loss = 0.03613796, grad/param norm = 2.9140e-02, time/batch = 0.2279s	
2286/2700 (epoch 42.333), train_loss = 0.02898851, grad/param norm = 1.9473e-02, time/batch = 0.2277s	
2287/2700 (epoch 42.352), train_loss = 0.03235821, grad/param norm = 2.3242e-02, time/batch = 0.2278s	
2288/2700 (epoch 42.370), train_loss = 0.02691786, grad/param norm = 2.1263e-02, time/batch = 0.2266s	
2289/2700 (epoch 42.389), train_loss = 0.03711558, grad/param norm = 2.3642e-02, time/batch = 0.2277s	
2290/2700 (epoch 42.407), train_loss = 0.03414132, grad/param norm = 2.1852e-02, time/batch = 0.2271s	
2291/2700 (epoch 42.426), train_loss = 0.03167230, grad/param norm = 2.3912e-02, time/batch = 0.2278s	
2292/2700 (epoch 42.444), train_loss = 0.03179453, grad/param norm = 2.7241e-02, time/batch = 0.2261s	
2293/2700 (epoch 42.463), train_loss = 0.03677191, grad/param norm = 2.2983e-02, time/batch = 0.2211s	
2294/2700 (epoch 42.481), train_loss = 0.03193730, grad/param norm = 2.4852e-02, time/batch = 0.2213s	
2295/2700 (epoch 42.500), train_loss = 0.03153824, grad/param norm = 2.2778e-02, time/batch = 0.2216s	
2296/2700 (epoch 42.519), train_loss = 0.03244887, grad/param norm = 2.1629e-02, time/batch = 0.2209s	
2297/2700 (epoch 42.537), train_loss = 0.03992370, grad/param norm = 2.8895e-02, time/batch = 0.2213s	
2298/2700 (epoch 42.556), train_loss = 0.03192094, grad/param norm = 2.2630e-02, time/batch = 0.2210s	
2299/2700 (epoch 42.574), train_loss = 0.02895474, grad/param norm = 2.0119e-02, time/batch = 0.2215s	
2300/2700 (epoch 42.593), train_loss = 0.02875709, grad/param norm = 2.6216e-02, time/batch = 0.2213s	
2301/2700 (epoch 42.611), train_loss = 0.03787424, grad/param norm = 2.9711e-02, time/batch = 0.2221s	
2302/2700 (epoch 42.630), train_loss = 0.03078616, grad/param norm = 2.0639e-02, time/batch = 0.2215s	
2303/2700 (epoch 42.648), train_loss = 0.03619378, grad/param norm = 2.0951e-02, time/batch = 0.2215s	
2304/2700 (epoch 42.667), train_loss = 0.03746926, grad/param norm = 2.4173e-02, time/batch = 0.2219s	
2305/2700 (epoch 42.685), train_loss = 0.03164907, grad/param norm = 2.4117e-02, time/batch = 0.2216s	
2306/2700 (epoch 42.704), train_loss = 0.03320301, grad/param norm = 2.4369e-02, time/batch = 0.2214s	
2307/2700 (epoch 42.722), train_loss = 0.03071424, grad/param norm = 2.1639e-02, time/batch = 0.2224s	
2308/2700 (epoch 42.741), train_loss = 0.03509894, grad/param norm = 2.1448e-02, time/batch = 0.2213s	
2309/2700 (epoch 42.759), train_loss = 0.02833441, grad/param norm = 1.9004e-02, time/batch = 0.2214s	
2310/2700 (epoch 42.778), train_loss = 0.04175962, grad/param norm = 2.9247e-02, time/batch = 0.2217s	
2311/2700 (epoch 42.796), train_loss = 0.03586234, grad/param norm = 2.5204e-02, time/batch = 0.2187s	
2312/2700 (epoch 42.815), train_loss = 0.03594926, grad/param norm = 2.0193e-02, time/batch = 0.2270s	
2313/2700 (epoch 42.833), train_loss = 0.03166533, grad/param norm = 2.0183e-02, time/batch = 0.2271s	
2314/2700 (epoch 42.852), train_loss = 0.02649103, grad/param norm = 1.6937e-02, time/batch = 0.2273s	
2315/2700 (epoch 42.870), train_loss = 0.02721132, grad/param norm = 2.5374e-02, time/batch = 0.2273s	
2316/2700 (epoch 42.889), train_loss = 0.03065571, grad/param norm = 2.6174e-02, time/batch = 0.2271s	
2317/2700 (epoch 42.907), train_loss = 0.02948633, grad/param norm = 2.1086e-02, time/batch = 0.2272s	
2318/2700 (epoch 42.926), train_loss = 0.02772299, grad/param norm = 1.7682e-02, time/batch = 0.2272s	
2319/2700 (epoch 42.944), train_loss = 0.02270951, grad/param norm = 1.5615e-02, time/batch = 0.2276s	
2320/2700 (epoch 42.963), train_loss = 0.03179502, grad/param norm = 1.8235e-02, time/batch = 0.2282s	
2321/2700 (epoch 42.981), train_loss = 0.02959245, grad/param norm = 2.4344e-02, time/batch = 0.2284s	
decayed learning rate by a factor 0.97 to 0.00071001734908087	
2322/2700 (epoch 43.000), train_loss = 0.02582230, grad/param norm = 1.8243e-02, time/batch = 0.2273s	
2323/2700 (epoch 43.019), train_loss = 0.04206052, grad/param norm = 2.3617e-02, time/batch = 0.2269s	
2324/2700 (epoch 43.037), train_loss = 0.03060695, grad/param norm = 2.1105e-02, time/batch = 0.2274s	
2325/2700 (epoch 43.056), train_loss = 0.03848823, grad/param norm = 2.8105e-02, time/batch = 0.2274s	
2326/2700 (epoch 43.074), train_loss = 0.03181170, grad/param norm = 2.2919e-02, time/batch = 0.2267s	
2327/2700 (epoch 43.093), train_loss = 0.02734859, grad/param norm = 1.7588e-02, time/batch = 0.2270s	
2328/2700 (epoch 43.111), train_loss = 0.02880607, grad/param norm = 1.9927e-02, time/batch = 0.2267s	
2329/2700 (epoch 43.130), train_loss = 0.02374706, grad/param norm = 1.5165e-02, time/batch = 0.2268s	
2330/2700 (epoch 43.148), train_loss = 0.02520871, grad/param norm = 1.8240e-02, time/batch = 0.2276s	
2331/2700 (epoch 43.167), train_loss = 0.03607446, grad/param norm = 2.7687e-02, time/batch = 0.2281s	
2332/2700 (epoch 43.185), train_loss = 0.02589340, grad/param norm = 2.1319e-02, time/batch = 0.2269s	
2333/2700 (epoch 43.204), train_loss = 0.03044681, grad/param norm = 2.0012e-02, time/batch = 0.2273s	
2334/2700 (epoch 43.222), train_loss = 0.02283743, grad/param norm = 2.0685e-02, time/batch = 0.2273s	
2335/2700 (epoch 43.241), train_loss = 0.02603932, grad/param norm = 2.4516e-02, time/batch = 0.2275s	
2336/2700 (epoch 43.259), train_loss = 0.03240348, grad/param norm = 2.0591e-02, time/batch = 0.2267s	
2337/2700 (epoch 43.278), train_loss = 0.03208219, grad/param norm = 2.0265e-02, time/batch = 0.2272s	
2338/2700 (epoch 43.296), train_loss = 0.02722967, grad/param norm = 1.9341e-02, time/batch = 0.2266s	
2339/2700 (epoch 43.315), train_loss = 0.02497628, grad/param norm = 2.0196e-02, time/batch = 0.2274s	
2340/2700 (epoch 43.333), train_loss = 0.02364395, grad/param norm = 1.6967e-02, time/batch = 0.2223s	
2341/2700 (epoch 43.352), train_loss = 0.02724885, grad/param norm = 1.9384e-02, time/batch = 0.1990s	
2342/2700 (epoch 43.370), train_loss = 0.02807872, grad/param norm = 1.7446e-02, time/batch = 0.2197s	
2343/2700 (epoch 43.389), train_loss = 0.03698779, grad/param norm = 2.2208e-02, time/batch = 0.2158s	
2344/2700 (epoch 43.407), train_loss = 0.03254864, grad/param norm = 1.9749e-02, time/batch = 0.2183s	
2345/2700 (epoch 43.426), train_loss = 0.02929184, grad/param norm = 2.0201e-02, time/batch = 0.2128s	
2346/2700 (epoch 43.444), train_loss = 0.02923129, grad/param norm = 2.5185e-02, time/batch = 0.2129s	
2347/2700 (epoch 43.463), train_loss = 0.03190904, grad/param norm = 2.3185e-02, time/batch = 0.2129s	
2348/2700 (epoch 43.481), train_loss = 0.03492219, grad/param norm = 2.6160e-02, time/batch = 0.2239s	
2349/2700 (epoch 43.500), train_loss = 0.02798889, grad/param norm = 1.9664e-02, time/batch = 0.2309s	
2350/2700 (epoch 43.519), train_loss = 0.02539502, grad/param norm = 1.9214e-02, time/batch = 0.2305s	
2351/2700 (epoch 43.537), train_loss = 0.03378981, grad/param norm = 3.2182e-02, time/batch = 0.2320s	
2352/2700 (epoch 43.556), train_loss = 0.03042748, grad/param norm = 2.3435e-02, time/batch = 0.2311s	
2353/2700 (epoch 43.574), train_loss = 0.03212331, grad/param norm = 2.6141e-02, time/batch = 0.2311s	
2354/2700 (epoch 43.593), train_loss = 0.03582675, grad/param norm = 3.2366e-02, time/batch = 0.2314s	
2355/2700 (epoch 43.611), train_loss = 0.03885294, grad/param norm = 2.9094e-02, time/batch = 0.2314s	
2356/2700 (epoch 43.630), train_loss = 0.02531872, grad/param norm = 2.1835e-02, time/batch = 0.2314s	
2357/2700 (epoch 43.648), train_loss = 0.03562965, grad/param norm = 2.6118e-02, time/batch = 0.2304s	
2358/2700 (epoch 43.667), train_loss = 0.03392692, grad/param norm = 2.1475e-02, time/batch = 0.2309s	
2359/2700 (epoch 43.685), train_loss = 0.02674301, grad/param norm = 1.9608e-02, time/batch = 0.2313s	
2360/2700 (epoch 43.704), train_loss = 0.03260054, grad/param norm = 2.4159e-02, time/batch = 0.2309s	
2361/2700 (epoch 43.722), train_loss = 0.02743173, grad/param norm = 2.0624e-02, time/batch = 0.2310s	
2362/2700 (epoch 43.741), train_loss = 0.03086767, grad/param norm = 1.8167e-02, time/batch = 0.2305s	
2363/2700 (epoch 43.759), train_loss = 0.02314433, grad/param norm = 2.1239e-02, time/batch = 0.2314s	
2364/2700 (epoch 43.778), train_loss = 0.03193233, grad/param norm = 2.7895e-02, time/batch = 0.2307s	
2365/2700 (epoch 43.796), train_loss = 0.02862537, grad/param norm = 2.1178e-02, time/batch = 0.2308s	
2366/2700 (epoch 43.815), train_loss = 0.02827173, grad/param norm = 2.0804e-02, time/batch = 0.2268s	
2367/2700 (epoch 43.833), train_loss = 0.02747073, grad/param norm = 2.0531e-02, time/batch = 0.2238s	
2368/2700 (epoch 43.852), train_loss = 0.02548349, grad/param norm = 1.6913e-02, time/batch = 0.2232s	
2369/2700 (epoch 43.870), train_loss = 0.02717711, grad/param norm = 1.9701e-02, time/batch = 0.2234s	
2370/2700 (epoch 43.889), train_loss = 0.02518258, grad/param norm = 2.1862e-02, time/batch = 0.2233s	
2371/2700 (epoch 43.907), train_loss = 0.02755705, grad/param norm = 1.9094e-02, time/batch = 0.2243s	
2372/2700 (epoch 43.926), train_loss = 0.02491815, grad/param norm = 1.8910e-02, time/batch = 0.2229s	
2373/2700 (epoch 43.944), train_loss = 0.02170814, grad/param norm = 1.4943e-02, time/batch = 0.2225s	
2374/2700 (epoch 43.963), train_loss = 0.02597451, grad/param norm = 1.7975e-02, time/batch = 0.2233s	
2375/2700 (epoch 43.981), train_loss = 0.02703866, grad/param norm = 1.9628e-02, time/batch = 0.2245s	
decayed learning rate by a factor 0.97 to 0.00068871682860844	
2376/2700 (epoch 44.000), train_loss = 0.02045348, grad/param norm = 1.8636e-02, time/batch = 0.2234s	
2377/2700 (epoch 44.019), train_loss = 0.03524902, grad/param norm = 1.9804e-02, time/batch = 0.2274s	
2378/2700 (epoch 44.037), train_loss = 0.02461634, grad/param norm = 1.7936e-02, time/batch = 0.2143s	
2379/2700 (epoch 44.056), train_loss = 0.03278118, grad/param norm = 2.4439e-02, time/batch = 0.2287s	
2380/2700 (epoch 44.074), train_loss = 0.02810665, grad/param norm = 2.0582e-02, time/batch = 0.2305s	
2381/2700 (epoch 44.093), train_loss = 0.02729769, grad/param norm = 2.2822e-02, time/batch = 0.2312s	
2382/2700 (epoch 44.111), train_loss = 0.02742931, grad/param norm = 2.0252e-02, time/batch = 0.2303s	
2383/2700 (epoch 44.130), train_loss = 0.02250011, grad/param norm = 1.5783e-02, time/batch = 0.2302s	
2384/2700 (epoch 44.148), train_loss = 0.02692687, grad/param norm = 2.2766e-02, time/batch = 0.2309s	
2385/2700 (epoch 44.167), train_loss = 0.03569380, grad/param norm = 2.8627e-02, time/batch = 0.2309s	
2386/2700 (epoch 44.185), train_loss = 0.02545238, grad/param norm = 2.3007e-02, time/batch = 0.2302s	
2387/2700 (epoch 44.204), train_loss = 0.02813822, grad/param norm = 2.5361e-02, time/batch = 0.2303s	
2388/2700 (epoch 44.222), train_loss = 0.02457650, grad/param norm = 2.5655e-02, time/batch = 0.2303s	
2389/2700 (epoch 44.241), train_loss = 0.02347146, grad/param norm = 1.8933e-02, time/batch = 0.2304s	
2390/2700 (epoch 44.259), train_loss = 0.03159120, grad/param norm = 2.1409e-02, time/batch = 0.2310s	
2391/2700 (epoch 44.278), train_loss = 0.02921439, grad/param norm = 2.3172e-02, time/batch = 0.2316s	
2392/2700 (epoch 44.296), train_loss = 0.02428468, grad/param norm = 1.7641e-02, time/batch = 0.2305s	
2393/2700 (epoch 44.315), train_loss = 0.02327770, grad/param norm = 2.9832e-02, time/batch = 0.2304s	
2394/2700 (epoch 44.333), train_loss = 0.02479544, grad/param norm = 2.8207e-02, time/batch = 0.2312s	
2395/2700 (epoch 44.352), train_loss = 0.02794545, grad/param norm = 2.3464e-02, time/batch = 0.2310s	
2396/2700 (epoch 44.370), train_loss = 0.02545099, grad/param norm = 2.1587e-02, time/batch = 0.2303s	
2397/2700 (epoch 44.389), train_loss = 0.03495701, grad/param norm = 2.6654e-02, time/batch = 0.2305s	
2398/2700 (epoch 44.407), train_loss = 0.03210545, grad/param norm = 2.2536e-02, time/batch = 0.2308s	
2399/2700 (epoch 44.426), train_loss = 0.02574117, grad/param norm = 2.4845e-02, time/batch = 0.2305s	
2400/2700 (epoch 44.444), train_loss = 0.02996063, grad/param norm = 2.9516e-02, time/batch = 0.2309s	
2401/2700 (epoch 44.463), train_loss = 0.02956696, grad/param norm = 2.4880e-02, time/batch = 0.2313s	
2402/2700 (epoch 44.481), train_loss = 0.03159752, grad/param norm = 2.5084e-02, time/batch = 0.2302s	
2403/2700 (epoch 44.500), train_loss = 0.02779573, grad/param norm = 2.1393e-02, time/batch = 0.2276s	
2404/2700 (epoch 44.519), train_loss = 0.02553159, grad/param norm = 2.1923e-02, time/batch = 0.2240s	
2405/2700 (epoch 44.537), train_loss = 0.03205471, grad/param norm = 3.4340e-02, time/batch = 0.2241s	
2406/2700 (epoch 44.556), train_loss = 0.02829651, grad/param norm = 2.6521e-02, time/batch = 0.2235s	
2407/2700 (epoch 44.574), train_loss = 0.03173554, grad/param norm = 2.3612e-02, time/batch = 0.2235s	
2408/2700 (epoch 44.593), train_loss = 0.03033235, grad/param norm = 3.1126e-02, time/batch = 0.2234s	
2409/2700 (epoch 44.611), train_loss = 0.03271601, grad/param norm = 2.9532e-02, time/batch = 0.2235s	
2410/2700 (epoch 44.630), train_loss = 0.03018262, grad/param norm = 3.2634e-02, time/batch = 0.2245s	
2411/2700 (epoch 44.648), train_loss = 0.03844755, grad/param norm = 2.3895e-02, time/batch = 0.2248s	
2412/2700 (epoch 44.667), train_loss = 0.03284456, grad/param norm = 2.6166e-02, time/batch = 0.2242s	
2413/2700 (epoch 44.685), train_loss = 0.02710139, grad/param norm = 2.2747e-02, time/batch = 0.2237s	
2414/2700 (epoch 44.704), train_loss = 0.03085270, grad/param norm = 2.8188e-02, time/batch = 0.2239s	
2415/2700 (epoch 44.722), train_loss = 0.02432472, grad/param norm = 2.1871e-02, time/batch = 0.2248s	
2416/2700 (epoch 44.741), train_loss = 0.03070318, grad/param norm = 2.1305e-02, time/batch = 0.2239s	
2417/2700 (epoch 44.759), train_loss = 0.02668372, grad/param norm = 2.4169e-02, time/batch = 0.2248s	
2418/2700 (epoch 44.778), train_loss = 0.02938269, grad/param norm = 2.4665e-02, time/batch = 0.2234s	
2419/2700 (epoch 44.796), train_loss = 0.03292454, grad/param norm = 3.1035e-02, time/batch = 0.2169s	
2420/2700 (epoch 44.815), train_loss = 0.02977225, grad/param norm = 2.1527e-02, time/batch = 0.2226s	
2421/2700 (epoch 44.833), train_loss = 0.03001695, grad/param norm = 2.5612e-02, time/batch = 0.2313s	
2422/2700 (epoch 44.852), train_loss = 0.02692141, grad/param norm = 2.5016e-02, time/batch = 0.2303s	
2423/2700 (epoch 44.870), train_loss = 0.03215662, grad/param norm = 2.3278e-02, time/batch = 0.2308s	
2424/2700 (epoch 44.889), train_loss = 0.02428586, grad/param norm = 1.9125e-02, time/batch = 0.2315s	
2425/2700 (epoch 44.907), train_loss = 0.02417555, grad/param norm = 2.3179e-02, time/batch = 0.2313s	
2426/2700 (epoch 44.926), train_loss = 0.02618602, grad/param norm = 3.0383e-02, time/batch = 0.2298s	
2427/2700 (epoch 44.944), train_loss = 0.02149402, grad/param norm = 1.8269e-02, time/batch = 0.2310s	
2428/2700 (epoch 44.963), train_loss = 0.02716424, grad/param norm = 2.3451e-02, time/batch = 0.2306s	
2429/2700 (epoch 44.981), train_loss = 0.02618922, grad/param norm = 1.9353e-02, time/batch = 0.2304s	
decayed learning rate by a factor 0.97 to 0.00066805532375019	
2430/2700 (epoch 45.000), train_loss = 0.02303248, grad/param norm = 2.1575e-02, time/batch = 0.2310s	
2431/2700 (epoch 45.019), train_loss = 0.03497243, grad/param norm = 2.0703e-02, time/batch = 0.2315s	
2432/2700 (epoch 45.037), train_loss = 0.02667345, grad/param norm = 1.9846e-02, time/batch = 0.2304s	
2433/2700 (epoch 45.056), train_loss = 0.03130489, grad/param norm = 2.7602e-02, time/batch = 0.2307s	
2434/2700 (epoch 45.074), train_loss = 0.02904388, grad/param norm = 2.0059e-02, time/batch = 0.2310s	
2435/2700 (epoch 45.093), train_loss = 0.03213713, grad/param norm = 2.7290e-02, time/batch = 0.2313s	
2436/2700 (epoch 45.111), train_loss = 0.02561244, grad/param norm = 2.2244e-02, time/batch = 0.2306s	
2437/2700 (epoch 45.130), train_loss = 0.02391873, grad/param norm = 1.9566e-02, time/batch = 0.2307s	
2438/2700 (epoch 45.148), train_loss = 0.02510675, grad/param norm = 2.3587e-02, time/batch = 0.2310s	
2439/2700 (epoch 45.167), train_loss = 0.03080549, grad/param norm = 2.1776e-02, time/batch = 0.2307s	
2440/2700 (epoch 45.185), train_loss = 0.02167893, grad/param norm = 2.0002e-02, time/batch = 0.2315s	
2441/2700 (epoch 45.204), train_loss = 0.02512605, grad/param norm = 2.6161e-02, time/batch = 0.2317s	
2442/2700 (epoch 45.222), train_loss = 0.02743693, grad/param norm = 3.3596e-02, time/batch = 0.2308s	
2443/2700 (epoch 45.241), train_loss = 0.02261455, grad/param norm = 1.9332e-02, time/batch = 0.2308s	
2444/2700 (epoch 45.259), train_loss = 0.03018466, grad/param norm = 2.5282e-02, time/batch = 0.2280s	
2445/2700 (epoch 45.278), train_loss = 0.02964555, grad/param norm = 2.4053e-02, time/batch = 0.2237s	
2446/2700 (epoch 45.296), train_loss = 0.02623629, grad/param norm = 2.3890e-02, time/batch = 0.2237s	
2447/2700 (epoch 45.315), train_loss = 0.02489212, grad/param norm = 2.5706e-02, time/batch = 0.2236s	
2448/2700 (epoch 45.333), train_loss = 0.02663173, grad/param norm = 2.9864e-02, time/batch = 0.2234s	
2449/2700 (epoch 45.352), train_loss = 0.03305929, grad/param norm = 3.0108e-02, time/batch = 0.2238s	
2450/2700 (epoch 45.370), train_loss = 0.02829497, grad/param norm = 2.8340e-02, time/batch = 0.2242s	
2451/2700 (epoch 45.389), train_loss = 0.02994601, grad/param norm = 2.4981e-02, time/batch = 0.2241s	
2452/2700 (epoch 45.407), train_loss = 0.03178486, grad/param norm = 2.0130e-02, time/batch = 0.2235s	
2453/2700 (epoch 45.426), train_loss = 0.02532609, grad/param norm = 1.9263e-02, time/batch = 0.2238s	
2454/2700 (epoch 45.444), train_loss = 0.02713781, grad/param norm = 2.1731e-02, time/batch = 0.2239s	
2455/2700 (epoch 45.463), train_loss = 0.02713541, grad/param norm = 2.3347e-02, time/batch = 0.2237s	
2456/2700 (epoch 45.481), train_loss = 0.02550414, grad/param norm = 2.1123e-02, time/batch = 0.2238s	
2457/2700 (epoch 45.500), train_loss = 0.02648859, grad/param norm = 2.0131e-02, time/batch = 0.2244s	
2458/2700 (epoch 45.519), train_loss = 0.02244851, grad/param norm = 2.1947e-02, time/batch = 0.2231s	
2459/2700 (epoch 45.537), train_loss = 0.03269105, grad/param norm = 2.4587e-02, time/batch = 0.2238s	
2460/2700 (epoch 45.556), train_loss = 0.02681599, grad/param norm = 2.5543e-02, time/batch = 0.2157s	
2461/2700 (epoch 45.574), train_loss = 0.02315717, grad/param norm = 2.7809e-02, time/batch = 0.2315s	
2462/2700 (epoch 45.593), train_loss = 0.02760041, grad/param norm = 2.8104e-02, time/batch = 0.2303s	
2463/2700 (epoch 45.611), train_loss = 0.02861269, grad/param norm = 2.5618e-02, time/batch = 0.2306s	
2464/2700 (epoch 45.630), train_loss = 0.02395387, grad/param norm = 2.6182e-02, time/batch = 0.2305s	
2465/2700 (epoch 45.648), train_loss = 0.03135246, grad/param norm = 2.8264e-02, time/batch = 0.2305s	
2466/2700 (epoch 45.667), train_loss = 0.02968347, grad/param norm = 2.0875e-02, time/batch = 0.2301s	
2467/2700 (epoch 45.685), train_loss = 0.02303584, grad/param norm = 1.7407e-02, time/batch = 0.2306s	
2468/2700 (epoch 45.704), train_loss = 0.02491697, grad/param norm = 2.0859e-02, time/batch = 0.2305s	
2469/2700 (epoch 45.722), train_loss = 0.02527471, grad/param norm = 2.0797e-02, time/batch = 0.2308s	
2470/2700 (epoch 45.741), train_loss = 0.02702957, grad/param norm = 1.7814e-02, time/batch = 0.2307s	
2471/2700 (epoch 45.759), train_loss = 0.01991961, grad/param norm = 1.7701e-02, time/batch = 0.2310s	
2472/2700 (epoch 45.778), train_loss = 0.02533360, grad/param norm = 1.6241e-02, time/batch = 0.2305s	
2473/2700 (epoch 45.796), train_loss = 0.02946084, grad/param norm = 2.4223e-02, time/batch = 0.2305s	
2474/2700 (epoch 45.815), train_loss = 0.02785717, grad/param norm = 1.8608e-02, time/batch = 0.2304s	
2475/2700 (epoch 45.833), train_loss = 0.02293514, grad/param norm = 2.2020e-02, time/batch = 0.2307s	
2476/2700 (epoch 45.852), train_loss = 0.02161378, grad/param norm = 2.0411e-02, time/batch = 0.2304s	
2477/2700 (epoch 45.870), train_loss = 0.02422600, grad/param norm = 1.9262e-02, time/batch = 0.2306s	
2478/2700 (epoch 45.889), train_loss = 0.02158866, grad/param norm = 1.7544e-02, time/batch = 0.2302s	
2479/2700 (epoch 45.907), train_loss = 0.02301154, grad/param norm = 2.1179e-02, time/batch = 0.2309s	
2480/2700 (epoch 45.926), train_loss = 0.02246926, grad/param norm = 2.1351e-02, time/batch = 0.2308s	
2481/2700 (epoch 45.944), train_loss = 0.01782428, grad/param norm = 1.5859e-02, time/batch = 0.2313s	
2482/2700 (epoch 45.963), train_loss = 0.02288971, grad/param norm = 1.6487e-02, time/batch = 0.2306s	
2483/2700 (epoch 45.981), train_loss = 0.02159594, grad/param norm = 1.8344e-02, time/batch = 0.2308s	
decayed learning rate by a factor 0.97 to 0.00064801366403768	
2484/2700 (epoch 46.000), train_loss = 0.01799056, grad/param norm = 1.7890e-02, time/batch = 0.2309s	
2485/2700 (epoch 46.019), train_loss = 0.03005443, grad/param norm = 1.6340e-02, time/batch = 0.2280s	
2486/2700 (epoch 46.037), train_loss = 0.02550937, grad/param norm = 2.2973e-02, time/batch = 0.2235s	
2487/2700 (epoch 46.056), train_loss = 0.02552538, grad/param norm = 2.1441e-02, time/batch = 0.2243s	
2488/2700 (epoch 46.074), train_loss = 0.02620991, grad/param norm = 1.9922e-02, time/batch = 0.2235s	
2489/2700 (epoch 46.093), train_loss = 0.02382581, grad/param norm = 2.1743e-02, time/batch = 0.2235s	
2490/2700 (epoch 46.111), train_loss = 0.02296122, grad/param norm = 1.8408e-02, time/batch = 0.2240s	
2491/2700 (epoch 46.130), train_loss = 0.02165287, grad/param norm = 2.0794e-02, time/batch = 0.2253s	
2492/2700 (epoch 46.148), train_loss = 0.02215706, grad/param norm = 1.9027e-02, time/batch = 0.2237s	
2493/2700 (epoch 46.167), train_loss = 0.02500662, grad/param norm = 1.7445e-02, time/batch = 0.2242s	
2494/2700 (epoch 46.185), train_loss = 0.01627740, grad/param norm = 1.1999e-02, time/batch = 0.2250s	
2495/2700 (epoch 46.204), train_loss = 0.02018007, grad/param norm = 2.0228e-02, time/batch = 0.2241s	
2496/2700 (epoch 46.222), train_loss = 0.01769589, grad/param norm = 2.3529e-02, time/batch = 0.2232s	
2497/2700 (epoch 46.241), train_loss = 0.02038277, grad/param norm = 1.9454e-02, time/batch = 0.2237s	
2498/2700 (epoch 46.259), train_loss = 0.02487917, grad/param norm = 2.0553e-02, time/batch = 0.2236s	
2499/2700 (epoch 46.278), train_loss = 0.02591778, grad/param norm = 2.4997e-02, time/batch = 0.2238s	
2500/2700 (epoch 46.296), train_loss = 0.02089977, grad/param norm = 1.8336e-02, time/batch = 0.2243s	
2501/2700 (epoch 46.315), train_loss = 0.02286917, grad/param norm = 2.5513e-02, time/batch = 0.2307s	
2502/2700 (epoch 46.333), train_loss = 0.02590132, grad/param norm = 2.9658e-02, time/batch = 0.2308s	
2503/2700 (epoch 46.352), train_loss = 0.02671608, grad/param norm = 2.9515e-02, time/batch = 0.2314s	
2504/2700 (epoch 46.370), train_loss = 0.02687715, grad/param norm = 2.6176e-02, time/batch = 0.2313s	
2505/2700 (epoch 46.389), train_loss = 0.02878315, grad/param norm = 2.7296e-02, time/batch = 0.2314s	
2506/2700 (epoch 46.407), train_loss = 0.02787283, grad/param norm = 2.2535e-02, time/batch = 0.2312s	
2507/2700 (epoch 46.426), train_loss = 0.02192346, grad/param norm = 1.9343e-02, time/batch = 0.2312s	
2508/2700 (epoch 46.444), train_loss = 0.02387857, grad/param norm = 2.0510e-02, time/batch = 0.2314s	
2509/2700 (epoch 46.463), train_loss = 0.02736211, grad/param norm = 2.6810e-02, time/batch = 0.2317s	
2510/2700 (epoch 46.481), train_loss = 0.02227573, grad/param norm = 1.8275e-02, time/batch = 0.2314s	
2511/2700 (epoch 46.500), train_loss = 0.02175238, grad/param norm = 1.7501e-02, time/batch = 0.2319s	
2512/2700 (epoch 46.519), train_loss = 0.02075957, grad/param norm = 2.0459e-02, time/batch = 0.2310s	
2513/2700 (epoch 46.537), train_loss = 0.02418117, grad/param norm = 2.4860e-02, time/batch = 0.2312s	
2514/2700 (epoch 46.556), train_loss = 0.02287854, grad/param norm = 1.8334e-02, time/batch = 0.2312s	
2515/2700 (epoch 46.574), train_loss = 0.02175988, grad/param norm = 1.9487e-02, time/batch = 0.2317s	
2516/2700 (epoch 46.593), train_loss = 0.02256727, grad/param norm = 2.7323e-02, time/batch = 0.2306s	
2517/2700 (epoch 46.611), train_loss = 0.02488304, grad/param norm = 2.5542e-02, time/batch = 0.2313s	
2518/2700 (epoch 46.630), train_loss = 0.02673561, grad/param norm = 2.4265e-02, time/batch = 0.2307s	
2519/2700 (epoch 46.648), train_loss = 0.03439871, grad/param norm = 3.2050e-02, time/batch = 0.2316s	
2520/2700 (epoch 46.667), train_loss = 0.02758716, grad/param norm = 2.7441e-02, time/batch = 0.2312s	
2521/2700 (epoch 46.685), train_loss = 0.02105326, grad/param norm = 2.0035e-02, time/batch = 0.2317s	
2522/2700 (epoch 46.704), train_loss = 0.02341808, grad/param norm = 1.9167e-02, time/batch = 0.2312s	
2523/2700 (epoch 46.722), train_loss = 0.02080159, grad/param norm = 1.7832e-02, time/batch = 0.2307s	
2524/2700 (epoch 46.741), train_loss = 0.02499128, grad/param norm = 2.0687e-02, time/batch = 0.2308s	
2525/2700 (epoch 46.759), train_loss = 0.01727770, grad/param norm = 1.5517e-02, time/batch = 0.2307s	
2526/2700 (epoch 46.778), train_loss = 0.02076336, grad/param norm = 1.5863e-02, time/batch = 0.2237s	
2527/2700 (epoch 46.796), train_loss = 0.02356207, grad/param norm = 2.0165e-02, time/batch = 0.2237s	
2528/2700 (epoch 46.815), train_loss = 0.02382247, grad/param norm = 1.7609e-02, time/batch = 0.2232s	
2529/2700 (epoch 46.833), train_loss = 0.02526906, grad/param norm = 1.9271e-02, time/batch = 0.2237s	
2530/2700 (epoch 46.852), train_loss = 0.02007078, grad/param norm = 1.6742e-02, time/batch = 0.2238s	
2531/2700 (epoch 46.870), train_loss = 0.01995949, grad/param norm = 1.3962e-02, time/batch = 0.2244s	
2532/2700 (epoch 46.889), train_loss = 0.01800624, grad/param norm = 1.5527e-02, time/batch = 0.2234s	
2533/2700 (epoch 46.907), train_loss = 0.02298686, grad/param norm = 2.8407e-02, time/batch = 0.2238s	
2534/2700 (epoch 46.926), train_loss = 0.02348302, grad/param norm = 3.0451e-02, time/batch = 0.2231s	
2535/2700 (epoch 46.944), train_loss = 0.01966890, grad/param norm = 2.6300e-02, time/batch = 0.2244s	
2536/2700 (epoch 46.963), train_loss = 0.02152191, grad/param norm = 2.7539e-02, time/batch = 0.2231s	
2537/2700 (epoch 46.981), train_loss = 0.02138489, grad/param norm = 2.1641e-02, time/batch = 0.2239s	
decayed learning rate by a factor 0.97 to 0.00062857325411655	
2538/2700 (epoch 47.000), train_loss = 0.02066140, grad/param norm = 1.8883e-02, time/batch = 0.2235s	
2539/2700 (epoch 47.019), train_loss = 0.02882843, grad/param norm = 2.2879e-02, time/batch = 0.2243s	
2540/2700 (epoch 47.037), train_loss = 0.03040966, grad/param norm = 2.8746e-02, time/batch = 0.2238s	
2541/2700 (epoch 47.056), train_loss = 0.02631646, grad/param norm = 3.2695e-02, time/batch = 0.2159s	
2542/2700 (epoch 47.074), train_loss = 0.02623420, grad/param norm = 2.0587e-02, time/batch = 0.2295s	
2543/2700 (epoch 47.093), train_loss = 0.02290666, grad/param norm = 2.2105e-02, time/batch = 0.2307s	
2544/2700 (epoch 47.111), train_loss = 0.02019530, grad/param norm = 1.7161e-02, time/batch = 0.2308s	
2545/2700 (epoch 47.130), train_loss = 0.02226259, grad/param norm = 2.1417e-02, time/batch = 0.2315s	
2546/2700 (epoch 47.148), train_loss = 0.02339625, grad/param norm = 2.2948e-02, time/batch = 0.2308s	
2547/2700 (epoch 47.167), train_loss = 0.02341753, grad/param norm = 2.2889e-02, time/batch = 0.2313s	
2548/2700 (epoch 47.185), train_loss = 0.01654828, grad/param norm = 1.3689e-02, time/batch = 0.2306s	
2549/2700 (epoch 47.204), train_loss = 0.02264417, grad/param norm = 2.1491e-02, time/batch = 0.2308s	
2550/2700 (epoch 47.222), train_loss = 0.01748894, grad/param norm = 2.2144e-02, time/batch = 0.2307s	
2551/2700 (epoch 47.241), train_loss = 0.02057001, grad/param norm = 2.3188e-02, time/batch = 0.2318s	
2552/2700 (epoch 47.259), train_loss = 0.02577219, grad/param norm = 3.1862e-02, time/batch = 0.2303s	
2553/2700 (epoch 47.278), train_loss = 0.02697407, grad/param norm = 2.9457e-02, time/batch = 0.2305s	
2554/2700 (epoch 47.296), train_loss = 0.02427377, grad/param norm = 2.4310e-02, time/batch = 0.2307s	
2555/2700 (epoch 47.315), train_loss = 0.02212864, grad/param norm = 2.6959e-02, time/batch = 0.2306s	
2556/2700 (epoch 47.333), train_loss = 0.02457853, grad/param norm = 3.3408e-02, time/batch = 0.2018s	
2557/2700 (epoch 47.352), train_loss = 0.02526897, grad/param norm = 2.3962e-02, time/batch = 0.2167s	
2558/2700 (epoch 47.370), train_loss = 0.02529681, grad/param norm = 2.8904e-02, time/batch = 0.2123s	
2559/2700 (epoch 47.389), train_loss = 0.03381919, grad/param norm = 2.6437e-02, time/batch = 0.2143s	
2560/2700 (epoch 47.407), train_loss = 0.02701157, grad/param norm = 2.0784e-02, time/batch = 0.2187s	
2561/2700 (epoch 47.426), train_loss = 0.01865757, grad/param norm = 1.7584e-02, time/batch = 0.2172s	
2562/2700 (epoch 47.444), train_loss = 0.02410333, grad/param norm = 2.6238e-02, time/batch = 0.2143s	
2563/2700 (epoch 47.463), train_loss = 0.02497249, grad/param norm = 2.7096e-02, time/batch = 0.2200s	
2564/2700 (epoch 47.481), train_loss = 0.02148378, grad/param norm = 1.9054e-02, time/batch = 0.2250s	
2565/2700 (epoch 47.500), train_loss = 0.01941592, grad/param norm = 1.8663e-02, time/batch = 0.2250s	
2566/2700 (epoch 47.519), train_loss = 0.01868934, grad/param norm = 1.5807e-02, time/batch = 0.2245s	
2567/2700 (epoch 47.537), train_loss = 0.02634499, grad/param norm = 2.5197e-02, time/batch = 0.2247s	
2568/2700 (epoch 47.556), train_loss = 0.01753463, grad/param norm = 1.5397e-02, time/batch = 0.2241s	
2569/2700 (epoch 47.574), train_loss = 0.02128081, grad/param norm = 1.5892e-02, time/batch = 0.2250s	
2570/2700 (epoch 47.593), train_loss = 0.01965265, grad/param norm = 2.0944e-02, time/batch = 0.2249s	
2571/2700 (epoch 47.611), train_loss = 0.02166397, grad/param norm = 1.6066e-02, time/batch = 0.2252s	
2572/2700 (epoch 47.630), train_loss = 0.01661878, grad/param norm = 1.4128e-02, time/batch = 0.2243s	
2573/2700 (epoch 47.648), train_loss = 0.02520960, grad/param norm = 2.1588e-02, time/batch = 0.2243s	
2574/2700 (epoch 47.667), train_loss = 0.02248187, grad/param norm = 2.1099e-02, time/batch = 0.2250s	
2575/2700 (epoch 47.685), train_loss = 0.02347794, grad/param norm = 2.3559e-02, time/batch = 0.2251s	
2576/2700 (epoch 47.704), train_loss = 0.02039351, grad/param norm = 2.3254e-02, time/batch = 0.2243s	
2577/2700 (epoch 47.722), train_loss = 0.02156686, grad/param norm = 1.9493e-02, time/batch = 0.2247s	
2578/2700 (epoch 47.741), train_loss = 0.01894773, grad/param norm = 1.5964e-02, time/batch = 0.2244s	
2579/2700 (epoch 47.759), train_loss = 0.01827988, grad/param norm = 1.6126e-02, time/batch = 0.2247s	
2580/2700 (epoch 47.778), train_loss = 0.02136656, grad/param norm = 2.2629e-02, time/batch = 0.2245s	
2581/2700 (epoch 47.796), train_loss = 0.02397072, grad/param norm = 2.0372e-02, time/batch = 0.2253s	
2582/2700 (epoch 47.815), train_loss = 0.02217331, grad/param norm = 1.6941e-02, time/batch = 0.2242s	
2583/2700 (epoch 47.833), train_loss = 0.02172983, grad/param norm = 1.9477e-02, time/batch = 0.2245s	
2584/2700 (epoch 47.852), train_loss = 0.01789836, grad/param norm = 1.5245e-02, time/batch = 0.2250s	
2585/2700 (epoch 47.870), train_loss = 0.01805637, grad/param norm = 1.5848e-02, time/batch = 0.2253s	
2586/2700 (epoch 47.889), train_loss = 0.01738853, grad/param norm = 1.3875e-02, time/batch = 0.2228s	
2587/2700 (epoch 47.907), train_loss = 0.02003667, grad/param norm = 2.3284e-02, time/batch = 0.2204s	
2588/2700 (epoch 47.926), train_loss = 0.01814625, grad/param norm = 1.8474e-02, time/batch = 0.2202s	
2589/2700 (epoch 47.944), train_loss = 0.01912281, grad/param norm = 2.2684e-02, time/batch = 0.2204s	
2590/2700 (epoch 47.963), train_loss = 0.02176881, grad/param norm = 1.8090e-02, time/batch = 0.2201s	
2591/2700 (epoch 47.981), train_loss = 0.01778251, grad/param norm = 1.5080e-02, time/batch = 0.2206s	
decayed learning rate by a factor 0.97 to 0.00060971605649306	
2592/2700 (epoch 48.000), train_loss = 0.01568417, grad/param norm = 1.6153e-02, time/batch = 0.2197s	
2593/2700 (epoch 48.019), train_loss = 0.03011293, grad/param norm = 2.0417e-02, time/batch = 0.2206s	
2594/2700 (epoch 48.037), train_loss = 0.02005350, grad/param norm = 1.9000e-02, time/batch = 0.2208s	
2595/2700 (epoch 48.056), train_loss = 0.02847279, grad/param norm = 3.8766e-02, time/batch = 0.2215s	
2596/2700 (epoch 48.074), train_loss = 0.02541165, grad/param norm = 2.8999e-02, time/batch = 0.2214s	
2597/2700 (epoch 48.093), train_loss = 0.02330903, grad/param norm = 2.5144e-02, time/batch = 0.2218s	
2598/2700 (epoch 48.111), train_loss = 0.01817169, grad/param norm = 2.2003e-02, time/batch = 0.2213s	
2599/2700 (epoch 48.130), train_loss = 0.02041313, grad/param norm = 2.3198e-02, time/batch = 0.2229s	
2600/2700 (epoch 48.148), train_loss = 0.01900957, grad/param norm = 2.3376e-02, time/batch = 0.2159s	
2601/2700 (epoch 48.167), train_loss = 0.02572768, grad/param norm = 2.2476e-02, time/batch = 0.2255s	
2602/2700 (epoch 48.185), train_loss = 0.01612013, grad/param norm = 1.6960e-02, time/batch = 0.2242s	
2603/2700 (epoch 48.204), train_loss = 0.01810827, grad/param norm = 2.0167e-02, time/batch = 0.2242s	
2604/2700 (epoch 48.222), train_loss = 0.01661341, grad/param norm = 2.5172e-02, time/batch = 0.2248s	
2605/2700 (epoch 48.241), train_loss = 0.01718444, grad/param norm = 1.6977e-02, time/batch = 0.2250s	
2606/2700 (epoch 48.259), train_loss = 0.02407277, grad/param norm = 2.5802e-02, time/batch = 0.2238s	
2607/2700 (epoch 48.278), train_loss = 0.02012909, grad/param norm = 1.9579e-02, time/batch = 0.2245s	
2608/2700 (epoch 48.296), train_loss = 0.01983686, grad/param norm = 2.3231e-02, time/batch = 0.2242s	
2609/2700 (epoch 48.315), train_loss = 0.01813716, grad/param norm = 2.1390e-02, time/batch = 0.2247s	
2610/2700 (epoch 48.333), train_loss = 0.02071559, grad/param norm = 2.9824e-02, time/batch = 0.2246s	
2611/2700 (epoch 48.352), train_loss = 0.02499427, grad/param norm = 2.2686e-02, time/batch = 0.2250s	
2612/2700 (epoch 48.370), train_loss = 0.02478376, grad/param norm = 2.9981e-02, time/batch = 0.2244s	
2613/2700 (epoch 48.389), train_loss = 0.02803947, grad/param norm = 3.3739e-02, time/batch = 0.2246s	
2614/2700 (epoch 48.407), train_loss = 0.02646472, grad/param norm = 2.1763e-02, time/batch = 0.2244s	
2615/2700 (epoch 48.426), train_loss = 0.02692004, grad/param norm = 2.6725e-02, time/batch = 0.2249s	
2616/2700 (epoch 48.444), train_loss = 0.02166399, grad/param norm = 2.0777e-02, time/batch = 0.2245s	
2617/2700 (epoch 48.463), train_loss = 0.02190955, grad/param norm = 2.3230e-02, time/batch = 0.2253s	
2618/2700 (epoch 48.481), train_loss = 0.02419110, grad/param norm = 2.4331e-02, time/batch = 0.2231s	
2619/2700 (epoch 48.500), train_loss = 0.01938034, grad/param norm = 2.7232e-02, time/batch = 0.2243s	
2620/2700 (epoch 48.519), train_loss = 0.02165382, grad/param norm = 2.3433e-02, time/batch = 0.2250s	
2621/2700 (epoch 48.537), train_loss = 0.02560898, grad/param norm = 2.5483e-02, time/batch = 0.2246s	
2622/2700 (epoch 48.556), train_loss = 0.01960796, grad/param norm = 1.9126e-02, time/batch = 0.2246s	
2623/2700 (epoch 48.574), train_loss = 0.01944407, grad/param norm = 2.4887e-02, time/batch = 0.2246s	
2624/2700 (epoch 48.593), train_loss = 0.01857704, grad/param norm = 2.0368e-02, time/batch = 0.2252s	
2625/2700 (epoch 48.611), train_loss = 0.02327822, grad/param norm = 1.8931e-02, time/batch = 0.2251s	
2626/2700 (epoch 48.630), train_loss = 0.01639921, grad/param norm = 1.8735e-02, time/batch = 0.2249s	
2627/2700 (epoch 48.648), train_loss = 0.02884591, grad/param norm = 2.3487e-02, time/batch = 0.2252s	
2628/2700 (epoch 48.667), train_loss = 0.01956391, grad/param norm = 1.8140e-02, time/batch = 0.2248s	
2629/2700 (epoch 48.685), train_loss = 0.02095542, grad/param norm = 1.8182e-02, time/batch = 0.2246s	
2630/2700 (epoch 48.704), train_loss = 0.01880901, grad/param norm = 2.4565e-02, time/batch = 0.2249s	
2631/2700 (epoch 48.722), train_loss = 0.01922327, grad/param norm = 1.7699e-02, time/batch = 0.2256s	
2632/2700 (epoch 48.741), train_loss = 0.01979175, grad/param norm = 1.6681e-02, time/batch = 0.2237s	
2633/2700 (epoch 48.759), train_loss = 0.01565916, grad/param norm = 2.0126e-02, time/batch = 0.2214s	
2634/2700 (epoch 48.778), train_loss = 0.01910385, grad/param norm = 2.3186e-02, time/batch = 0.2222s	
2635/2700 (epoch 48.796), train_loss = 0.02113945, grad/param norm = 2.2207e-02, time/batch = 0.2221s	
2636/2700 (epoch 48.815), train_loss = 0.02195781, grad/param norm = 1.5851e-02, time/batch = 0.2215s	
2637/2700 (epoch 48.833), train_loss = 0.02014818, grad/param norm = 1.8945e-02, time/batch = 0.2216s	
2638/2700 (epoch 48.852), train_loss = 0.01802433, grad/param norm = 1.7654e-02, time/batch = 0.2215s	
2639/2700 (epoch 48.870), train_loss = 0.01628795, grad/param norm = 1.3278e-02, time/batch = 0.2219s	
2640/2700 (epoch 48.889), train_loss = 0.01611579, grad/param norm = 1.3122e-02, time/batch = 0.2218s	
2641/2700 (epoch 48.907), train_loss = 0.01772799, grad/param norm = 1.8873e-02, time/batch = 0.2226s	
2642/2700 (epoch 48.926), train_loss = 0.01520962, grad/param norm = 1.5040e-02, time/batch = 0.2213s	
2643/2700 (epoch 48.944), train_loss = 0.01442459, grad/param norm = 1.9107e-02, time/batch = 0.2221s	
2644/2700 (epoch 48.963), train_loss = 0.01893421, grad/param norm = 1.5276e-02, time/batch = 0.2219s	
2645/2700 (epoch 48.981), train_loss = 0.01974473, grad/param norm = 2.0182e-02, time/batch = 0.2219s	
decayed learning rate by a factor 0.97 to 0.00059142457479826	
2646/2700 (epoch 49.000), train_loss = 0.01454582, grad/param norm = 1.4642e-02, time/batch = 0.2218s	
2647/2700 (epoch 49.019), train_loss = 0.02294184, grad/param norm = 1.9117e-02, time/batch = 0.2220s	
2648/2700 (epoch 49.037), train_loss = 0.02032416, grad/param norm = 1.6619e-02, time/batch = 0.2213s	
2649/2700 (epoch 49.056), train_loss = 0.01744508, grad/param norm = 2.0508e-02, time/batch = 0.2218s	
2650/2700 (epoch 49.074), train_loss = 0.02375946, grad/param norm = 2.5312e-02, time/batch = 0.2216s	
2651/2700 (epoch 49.093), train_loss = 0.01748782, grad/param norm = 1.8393e-02, time/batch = 0.2178s	
2652/2700 (epoch 49.111), train_loss = 0.02102775, grad/param norm = 2.5538e-02, time/batch = 0.2157s	
2653/2700 (epoch 49.130), train_loss = 0.01762940, grad/param norm = 1.8461e-02, time/batch = 0.2236s	
2654/2700 (epoch 49.148), train_loss = 0.01700476, grad/param norm = 1.6862e-02, time/batch = 0.2236s	
2655/2700 (epoch 49.167), train_loss = 0.01969714, grad/param norm = 1.3720e-02, time/batch = 0.2247s	
2656/2700 (epoch 49.185), train_loss = 0.01787650, grad/param norm = 2.2163e-02, time/batch = 0.2244s	
2657/2700 (epoch 49.204), train_loss = 0.01918554, grad/param norm = 2.3703e-02, time/batch = 0.2237s	
2658/2700 (epoch 49.222), train_loss = 0.02061953, grad/param norm = 2.9351e-02, time/batch = 0.2243s	
2659/2700 (epoch 49.241), train_loss = 0.01983216, grad/param norm = 2.1739e-02, time/batch = 0.2237s	
2660/2700 (epoch 49.259), train_loss = 0.01796320, grad/param norm = 1.7839e-02, time/batch = 0.2236s	
2661/2700 (epoch 49.278), train_loss = 0.01804813, grad/param norm = 1.5425e-02, time/batch = 0.2243s	
2662/2700 (epoch 49.296), train_loss = 0.02008106, grad/param norm = 2.0988e-02, time/batch = 0.2233s	
2663/2700 (epoch 49.315), train_loss = 0.01642370, grad/param norm = 1.9555e-02, time/batch = 0.2239s	
2664/2700 (epoch 49.333), train_loss = 0.01777241, grad/param norm = 2.2914e-02, time/batch = 0.2236s	
2665/2700 (epoch 49.352), train_loss = 0.01745678, grad/param norm = 1.4254e-02, time/batch = 0.2248s	
2666/2700 (epoch 49.370), train_loss = 0.01690415, grad/param norm = 1.5306e-02, time/batch = 0.2243s	
2667/2700 (epoch 49.389), train_loss = 0.02856548, grad/param norm = 2.7533e-02, time/batch = 0.2246s	
2668/2700 (epoch 49.407), train_loss = 0.02630964, grad/param norm = 2.4898e-02, time/batch = 0.2243s	
2669/2700 (epoch 49.426), train_loss = 0.02326171, grad/param norm = 2.4804e-02, time/batch = 0.2246s	
2670/2700 (epoch 49.444), train_loss = 0.01880304, grad/param norm = 2.1125e-02, time/batch = 0.2248s	
2671/2700 (epoch 49.463), train_loss = 0.02093359, grad/param norm = 2.1092e-02, time/batch = 0.2256s	
2672/2700 (epoch 49.481), train_loss = 0.01644658, grad/param norm = 1.6880e-02, time/batch = 0.2236s	
2673/2700 (epoch 49.500), train_loss = 0.01715668, grad/param norm = 1.9555e-02, time/batch = 0.2246s	
2674/2700 (epoch 49.519), train_loss = 0.01451911, grad/param norm = 1.7316e-02, time/batch = 0.2247s	
2675/2700 (epoch 49.537), train_loss = 0.01950384, grad/param norm = 1.4973e-02, time/batch = 0.2248s	
2676/2700 (epoch 49.556), train_loss = 0.01638610, grad/param norm = 1.6183e-02, time/batch = 0.2233s	
2677/2700 (epoch 49.574), train_loss = 0.02186038, grad/param norm = 2.4929e-02, time/batch = 0.2246s	
2678/2700 (epoch 49.593), train_loss = 0.01889246, grad/param norm = 2.2478e-02, time/batch = 0.2244s	
2679/2700 (epoch 49.611), train_loss = 0.02107655, grad/param norm = 1.5541e-02, time/batch = 0.2249s	
2680/2700 (epoch 49.630), train_loss = 0.01331060, grad/param norm = 1.7085e-02, time/batch = 0.2232s	
2681/2700 (epoch 49.648), train_loss = 0.02456151, grad/param norm = 2.5325e-02, time/batch = 0.2249s	
2682/2700 (epoch 49.667), train_loss = 0.02032888, grad/param norm = 1.6871e-02, time/batch = 0.2245s	
2683/2700 (epoch 49.685), train_loss = 0.01911746, grad/param norm = 2.1201e-02, time/batch = 0.2246s	
2684/2700 (epoch 49.704), train_loss = 0.01873334, grad/param norm = 1.8260e-02, time/batch = 0.2246s	
2685/2700 (epoch 49.722), train_loss = 0.01586642, grad/param norm = 1.4217e-02, time/batch = 0.2223s	
2686/2700 (epoch 49.741), train_loss = 0.01606183, grad/param norm = 1.1998e-02, time/batch = 0.2215s	
2687/2700 (epoch 49.759), train_loss = 0.01598032, grad/param norm = 1.2235e-02, time/batch = 0.2217s	
2688/2700 (epoch 49.778), train_loss = 0.01585858, grad/param norm = 1.6243e-02, time/batch = 0.2213s	
2689/2700 (epoch 49.796), train_loss = 0.01685591, grad/param norm = 2.0227e-02, time/batch = 0.2221s	
2690/2700 (epoch 49.815), train_loss = 0.01863034, grad/param norm = 1.4862e-02, time/batch = 0.2220s	
2691/2700 (epoch 49.833), train_loss = 0.01624615, grad/param norm = 1.4120e-02, time/batch = 0.2233s	
2692/2700 (epoch 49.852), train_loss = 0.01399162, grad/param norm = 1.3140e-02, time/batch = 0.2217s	
2693/2700 (epoch 49.870), train_loss = 0.01392940, grad/param norm = 1.1509e-02, time/batch = 0.2219s	
2694/2700 (epoch 49.889), train_loss = 0.01343114, grad/param norm = 1.0767e-02, time/batch = 0.2220s	
2695/2700 (epoch 49.907), train_loss = 0.01516342, grad/param norm = 1.3022e-02, time/batch = 0.2226s	
2696/2700 (epoch 49.926), train_loss = 0.01418463, grad/param norm = 1.5749e-02, time/batch = 0.2217s	
2697/2700 (epoch 49.944), train_loss = 0.01143260, grad/param norm = 1.1571e-02, time/batch = 0.2219s	
2698/2700 (epoch 49.963), train_loss = 0.01508946, grad/param norm = 1.2291e-02, time/batch = 0.2231s	
2699/2700 (epoch 49.981), train_loss = 0.01611428, grad/param norm = 1.5408e-02, time/batch = 0.2223s	
decayed learning rate by a factor 0.97 to 0.00057368183755432	
evaluating loss over split index 2	
1/3...	
2/3...	
3/3...	
saving checkpoint to cv/lm_lstm_epoch50.00_3.1627.t7	
2700/2700 (epoch 50.000), train_loss = 0.01175338, grad/param norm = 1.1039e-02, time/batch = 0.2219s	
