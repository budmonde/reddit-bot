using CUDA on GPU 0...	
loading data files...	
cutting off end of data so that the batches/sequences divide evenly	
reshaping tensor...	
data load done. Number of data batches in train: 54, val: 3, test: 0	
vocab size: 91	
creating an lstm with 4 layers	
setting forget gate biases to 1 in LSTM layer 1	
setting forget gate biases to 1 in LSTM layer 2	
setting forget gate biases to 1 in LSTM layer 3	
setting forget gate biases to 1 in LSTM layer 4	
number of parameters in the model: 7589467	
cloning rnn	
cloning criterion	
1/2700 (epoch 0.019), train_loss = 4.62173102, grad/param norm = 1.0351e+00, time/batch = 0.6384s	
2/2700 (epoch 0.037), train_loss = 3.53170298, grad/param norm = 8.9867e-01, time/batch = 0.3038s	
3/2700 (epoch 0.056), train_loss = 4.53033092, grad/param norm = 8.8647e-01, time/batch = 0.2698s	
4/2700 (epoch 0.074), train_loss = 3.73752202, grad/param norm = 9.4791e-01, time/batch = 0.2684s	
5/2700 (epoch 0.093), train_loss = 3.61811355, grad/param norm = 6.7580e-01, time/batch = 0.2683s	
6/2700 (epoch 0.111), train_loss = 3.52796636, grad/param norm = 6.1523e-01, time/batch = 0.2686s	
7/2700 (epoch 0.130), train_loss = 3.34548318, grad/param norm = 3.3911e-01, time/batch = 0.2688s	
8/2700 (epoch 0.148), train_loss = 3.25467304, grad/param norm = 2.1004e-01, time/batch = 0.2689s	
9/2700 (epoch 0.167), train_loss = 3.27683869, grad/param norm = 2.6158e-01, time/batch = 0.2690s	
10/2700 (epoch 0.185), train_loss = 3.25764652, grad/param norm = 1.8835e-01, time/batch = 0.2688s	
11/2700 (epoch 0.204), train_loss = 3.18247700, grad/param norm = 1.6531e-01, time/batch = 0.2690s	
12/2700 (epoch 0.222), train_loss = 3.15178569, grad/param norm = 1.6651e-01, time/batch = 0.2683s	
13/2700 (epoch 0.241), train_loss = 3.17751593, grad/param norm = 1.6126e-01, time/batch = 0.2685s	
14/2700 (epoch 0.259), train_loss = 3.21499808, grad/param norm = 1.5619e-01, time/batch = 0.2689s	
15/2700 (epoch 0.278), train_loss = 3.28342396, grad/param norm = 1.5329e-01, time/batch = 0.2688s	
16/2700 (epoch 0.296), train_loss = 3.28754608, grad/param norm = 1.6381e-01, time/batch = 0.2681s	
17/2700 (epoch 0.315), train_loss = 3.26070367, grad/param norm = 1.4826e-01, time/batch = 0.2683s	
18/2700 (epoch 0.333), train_loss = 3.34191413, grad/param norm = 1.4479e-01, time/batch = 0.2678s	
19/2700 (epoch 0.352), train_loss = 3.35187431, grad/param norm = 1.7939e-01, time/batch = 0.2683s	
20/2700 (epoch 0.370), train_loss = 3.29177240, grad/param norm = 1.4998e-01, time/batch = 0.2685s	
21/2700 (epoch 0.389), train_loss = 3.25508945, grad/param norm = 1.2581e-01, time/batch = 0.2692s	
22/2700 (epoch 0.407), train_loss = 3.27945924, grad/param norm = 1.1825e-01, time/batch = 0.2680s	
23/2700 (epoch 0.426), train_loss = 3.28234904, grad/param norm = 1.2694e-01, time/batch = 0.2687s	
24/2700 (epoch 0.444), train_loss = 3.20849185, grad/param norm = 1.0422e-01, time/batch = 0.2686s	
25/2700 (epoch 0.463), train_loss = 3.25128758, grad/param norm = 1.1734e-01, time/batch = 0.2684s	
26/2700 (epoch 0.481), train_loss = 3.32756944, grad/param norm = 1.0494e-01, time/batch = 0.2689s	
27/2700 (epoch 0.500), train_loss = 3.37474012, grad/param norm = 1.5483e-01, time/batch = 0.2689s	
28/2700 (epoch 0.519), train_loss = 3.32697468, grad/param norm = 1.4913e-01, time/batch = 0.2681s	
29/2700 (epoch 0.537), train_loss = 3.32949516, grad/param norm = 1.6060e-01, time/batch = 0.2691s	
30/2700 (epoch 0.556), train_loss = 3.26810215, grad/param norm = 1.3019e-01, time/batch = 0.2684s	
31/2700 (epoch 0.574), train_loss = 3.23213370, grad/param norm = 1.3832e-01, time/batch = 0.2697s	
32/2700 (epoch 0.593), train_loss = 3.23880187, grad/param norm = 1.9519e-01, time/batch = 0.2683s	
33/2700 (epoch 0.611), train_loss = 3.17748738, grad/param norm = 1.6156e-01, time/batch = 0.2683s	
34/2700 (epoch 0.630), train_loss = 3.22046958, grad/param norm = 1.9328e-01, time/batch = 0.2688s	
35/2700 (epoch 0.648), train_loss = 3.29412174, grad/param norm = 2.1716e-01, time/batch = 0.2686s	
36/2700 (epoch 0.667), train_loss = 3.22758072, grad/param norm = 2.2191e-01, time/batch = 0.2690s	
37/2700 (epoch 0.685), train_loss = 3.21957540, grad/param norm = 1.9635e-01, time/batch = 0.2688s	
38/2700 (epoch 0.704), train_loss = 3.19194413, grad/param norm = 2.1828e-01, time/batch = 0.2683s	
39/2700 (epoch 0.722), train_loss = 3.18386775, grad/param norm = 1.8639e-01, time/batch = 0.2693s	
40/2700 (epoch 0.741), train_loss = 3.31612576, grad/param norm = 1.7027e-01, time/batch = 0.2689s	
41/2700 (epoch 0.759), train_loss = 3.26669863, grad/param norm = 1.9594e-01, time/batch = 0.2701s	
42/2700 (epoch 0.778), train_loss = 3.25968818, grad/param norm = 2.1544e-01, time/batch = 0.2691s	
43/2700 (epoch 0.796), train_loss = 3.25271283, grad/param norm = 2.1240e-01, time/batch = 0.2694s	
44/2700 (epoch 0.815), train_loss = 3.20273839, grad/param norm = 1.8564e-01, time/batch = 0.2693s	
45/2700 (epoch 0.833), train_loss = 3.23880294, grad/param norm = 1.7254e-01, time/batch = 0.2699s	
46/2700 (epoch 0.852), train_loss = 3.22528517, grad/param norm = 1.6830e-01, time/batch = 0.2690s	
47/2700 (epoch 0.870), train_loss = 3.21870786, grad/param norm = 1.3677e-01, time/batch = 0.2689s	
48/2700 (epoch 0.889), train_loss = 3.25509197, grad/param norm = 1.4265e-01, time/batch = 0.2686s	
49/2700 (epoch 0.907), train_loss = 3.30913156, grad/param norm = 1.8441e-01, time/batch = 0.2693s	
50/2700 (epoch 0.926), train_loss = 3.26214696, grad/param norm = 1.9544e-01, time/batch = 0.2691s	
51/2700 (epoch 0.944), train_loss = 3.26874277, grad/param norm = 1.5811e-01, time/batch = 0.2706s	
52/2700 (epoch 0.963), train_loss = 3.34396670, grad/param norm = 1.4470e-01, time/batch = 0.2691s	
53/2700 (epoch 0.981), train_loss = 3.40466022, grad/param norm = 1.5828e-01, time/batch = 0.2695s	
54/2700 (epoch 1.000), train_loss = 3.31109237, grad/param norm = 1.7399e-01, time/batch = 0.2702s	
55/2700 (epoch 1.019), train_loss = 3.25270087, grad/param norm = 1.8311e-01, time/batch = 0.2687s	
56/2700 (epoch 1.037), train_loss = 3.26245983, grad/param norm = 1.5761e-01, time/batch = 0.2688s	
57/2700 (epoch 1.056), train_loss = 3.26285979, grad/param norm = 1.2485e-01, time/batch = 0.2697s	
58/2700 (epoch 1.074), train_loss = 3.29580270, grad/param norm = 1.6275e-01, time/batch = 0.2689s	
59/2700 (epoch 1.093), train_loss = 3.30384026, grad/param norm = 1.9646e-01, time/batch = 0.2693s	
60/2700 (epoch 1.111), train_loss = 3.27632975, grad/param norm = 1.9966e-01, time/batch = 0.2689s	
61/2700 (epoch 1.130), train_loss = 3.29270355, grad/param norm = 1.8873e-01, time/batch = 0.2708s	
62/2700 (epoch 1.148), train_loss = 3.25317627, grad/param norm = 2.0827e-01, time/batch = 0.2688s	
63/2700 (epoch 1.167), train_loss = 3.26934825, grad/param norm = 2.4379e-01, time/batch = 0.2696s	
64/2700 (epoch 1.185), train_loss = 3.24829635, grad/param norm = 1.7885e-01, time/batch = 0.2697s	
65/2700 (epoch 1.204), train_loss = 3.18034842, grad/param norm = 1.8174e-01, time/batch = 0.2699s	
66/2700 (epoch 1.222), train_loss = 3.15904685, grad/param norm = 2.1633e-01, time/batch = 0.2693s	
67/2700 (epoch 1.241), train_loss = 3.17932909, grad/param norm = 2.0388e-01, time/batch = 0.2695s	
68/2700 (epoch 1.259), train_loss = 3.21162712, grad/param norm = 1.9207e-01, time/batch = 0.2690s	
69/2700 (epoch 1.278), train_loss = 3.28409175, grad/param norm = 1.8562e-01, time/batch = 0.2696s	
70/2700 (epoch 1.296), train_loss = 3.28879934, grad/param norm = 2.0912e-01, time/batch = 0.2695s	
71/2700 (epoch 1.315), train_loss = 3.26616707, grad/param norm = 2.0622e-01, time/batch = 0.2701s	
72/2700 (epoch 1.333), train_loss = 3.34483646, grad/param norm = 1.9180e-01, time/batch = 0.2688s	
73/2700 (epoch 1.352), train_loss = 3.35168268, grad/param norm = 1.9090e-01, time/batch = 0.2691s	
74/2700 (epoch 1.370), train_loss = 3.29339217, grad/param norm = 1.6543e-01, time/batch = 0.2699s	
75/2700 (epoch 1.389), train_loss = 3.25695522, grad/param norm = 1.3473e-01, time/batch = 0.2691s	
76/2700 (epoch 1.407), train_loss = 3.27940372, grad/param norm = 1.1833e-01, time/batch = 0.2693s	
77/2700 (epoch 1.426), train_loss = 3.28176868, grad/param norm = 1.2278e-01, time/batch = 0.2697s	
78/2700 (epoch 1.444), train_loss = 3.20831335, grad/param norm = 1.0594e-01, time/batch = 0.2687s	
79/2700 (epoch 1.463), train_loss = 3.25190727, grad/param norm = 1.2682e-01, time/batch = 0.2689s	
80/2700 (epoch 1.481), train_loss = 3.32777627, grad/param norm = 1.1850e-01, time/batch = 0.2695s	
81/2700 (epoch 1.500), train_loss = 3.37522655, grad/param norm = 1.7956e-01, time/batch = 0.2703s	
82/2700 (epoch 1.519), train_loss = 3.33068859, grad/param norm = 1.9517e-01, time/batch = 0.2694s	
83/2700 (epoch 1.537), train_loss = 3.33502755, grad/param norm = 2.0877e-01, time/batch = 0.2693s	
84/2700 (epoch 1.556), train_loss = 3.27133218, grad/param norm = 1.7840e-01, time/batch = 0.2697s	
85/2700 (epoch 1.574), train_loss = 3.23545896, grad/param norm = 1.6757e-01, time/batch = 0.2693s	
86/2700 (epoch 1.593), train_loss = 3.23906013, grad/param norm = 2.0654e-01, time/batch = 0.2692s	
87/2700 (epoch 1.611), train_loss = 3.17776985, grad/param norm = 1.6579e-01, time/batch = 0.2690s	
88/2700 (epoch 1.630), train_loss = 3.21882563, grad/param norm = 1.8203e-01, time/batch = 0.2689s	
89/2700 (epoch 1.648), train_loss = 3.29265737, grad/param norm = 1.9697e-01, time/batch = 0.2700s	
90/2700 (epoch 1.667), train_loss = 3.22373945, grad/param norm = 1.8324e-01, time/batch = 0.2694s	
91/2700 (epoch 1.685), train_loss = 3.21670614, grad/param norm = 1.5745e-01, time/batch = 0.2695s	
92/2700 (epoch 1.704), train_loss = 3.18966704, grad/param norm = 1.8094e-01, time/batch = 0.2693s	
93/2700 (epoch 1.722), train_loss = 3.18085442, grad/param norm = 1.4350e-01, time/batch = 0.2692s	
94/2700 (epoch 1.741), train_loss = 3.31228050, grad/param norm = 1.3167e-01, time/batch = 0.2697s	
95/2700 (epoch 1.759), train_loss = 3.26378151, grad/param norm = 1.7752e-01, time/batch = 0.2690s	
96/2700 (epoch 1.778), train_loss = 3.25815774, grad/param norm = 2.0370e-01, time/batch = 0.2692s	
97/2700 (epoch 1.796), train_loss = 3.25298049, grad/param norm = 2.1319e-01, time/batch = 0.2697s	
98/2700 (epoch 1.815), train_loss = 3.20375046, grad/param norm = 1.9664e-01, time/batch = 0.2688s	
99/2700 (epoch 1.833), train_loss = 3.24155077, grad/param norm = 2.0186e-01, time/batch = 0.2697s	
100/2700 (epoch 1.852), train_loss = 3.22970065, grad/param norm = 2.0989e-01, time/batch = 0.2698s	
101/2700 (epoch 1.870), train_loss = 3.22226662, grad/param norm = 1.7726e-01, time/batch = 0.2710s	
102/2700 (epoch 1.889), train_loss = 3.25680455, grad/param norm = 1.6751e-01, time/batch = 0.2691s	
103/2700 (epoch 1.907), train_loss = 3.31011148, grad/param norm = 2.0048e-01, time/batch = 0.2689s	
104/2700 (epoch 1.926), train_loss = 3.26278960, grad/param norm = 2.0759e-01, time/batch = 0.2705s	
105/2700 (epoch 1.944), train_loss = 3.27014091, grad/param norm = 1.7359e-01, time/batch = 0.2696s	
106/2700 (epoch 1.963), train_loss = 3.34580621, grad/param norm = 1.5811e-01, time/batch = 0.2685s	
107/2700 (epoch 1.981), train_loss = 3.40670468, grad/param norm = 1.6750e-01, time/batch = 0.2683s	
108/2700 (epoch 2.000), train_loss = 3.30978939, grad/param norm = 1.7387e-01, time/batch = 0.2681s	
109/2700 (epoch 2.019), train_loss = 3.24848651, grad/param norm = 1.8122e-01, time/batch = 0.2684s	
110/2700 (epoch 2.037), train_loss = 3.26343946, grad/param norm = 1.6187e-01, time/batch = 0.2685s	
111/2700 (epoch 2.056), train_loss = 3.26159527, grad/param norm = 1.1684e-01, time/batch = 0.2699s	
112/2700 (epoch 2.074), train_loss = 3.29486286, grad/param norm = 1.5637e-01, time/batch = 0.2683s	
113/2700 (epoch 2.093), train_loss = 3.30275119, grad/param norm = 1.8427e-01, time/batch = 0.2689s	
114/2700 (epoch 2.111), train_loss = 3.27368720, grad/param norm = 1.7431e-01, time/batch = 0.2698s	
115/2700 (epoch 2.130), train_loss = 3.29085917, grad/param norm = 1.6501e-01, time/batch = 0.2694s	
116/2700 (epoch 2.148), train_loss = 3.25260722, grad/param norm = 1.9676e-01, time/batch = 0.2692s	
117/2700 (epoch 2.167), train_loss = 3.26679648, grad/param norm = 2.2360e-01, time/batch = 0.2688s	
118/2700 (epoch 2.185), train_loss = 3.24685424, grad/param norm = 1.6429e-01, time/batch = 0.2683s	
119/2700 (epoch 2.204), train_loss = 3.18037762, grad/param norm = 1.8078e-01, time/batch = 0.2691s	
120/2700 (epoch 2.222), train_loss = 3.15958043, grad/param norm = 2.2077e-01, time/batch = 0.2686s	
121/2700 (epoch 2.241), train_loss = 3.17873631, grad/param norm = 2.0675e-01, time/batch = 0.2694s	
122/2700 (epoch 2.259), train_loss = 3.21184863, grad/param norm = 2.0773e-01, time/batch = 0.2685s	
123/2700 (epoch 2.278), train_loss = 3.28479608, grad/param norm = 1.9638e-01, time/batch = 0.2686s	
124/2700 (epoch 2.296), train_loss = 3.28868963, grad/param norm = 2.1373e-01, time/batch = 0.2690s	
125/2700 (epoch 2.315), train_loss = 3.26494972, grad/param norm = 2.0203e-01, time/batch = 0.2689s	
126/2700 (epoch 2.333), train_loss = 3.34376584, grad/param norm = 1.8238e-01, time/batch = 0.2687s	
127/2700 (epoch 2.352), train_loss = 3.34990225, grad/param norm = 1.7790e-01, time/batch = 0.2690s	
128/2700 (epoch 2.370), train_loss = 3.29315258, grad/param norm = 1.6043e-01, time/batch = 0.2689s	
129/2700 (epoch 2.389), train_loss = 3.25720998, grad/param norm = 1.3137e-01, time/batch = 0.2695s	
130/2700 (epoch 2.407), train_loss = 3.27944158, grad/param norm = 1.1936e-01, time/batch = 0.2689s	
131/2700 (epoch 2.426), train_loss = 3.28162950, grad/param norm = 1.2094e-01, time/batch = 0.2710s	
132/2700 (epoch 2.444), train_loss = 3.20847813, grad/param norm = 1.1010e-01, time/batch = 0.2686s	
133/2700 (epoch 2.463), train_loss = 3.25253193, grad/param norm = 1.3602e-01, time/batch = 0.2687s	
134/2700 (epoch 2.481), train_loss = 3.32875216, grad/param norm = 1.3599e-01, time/batch = 0.2694s	
135/2700 (epoch 2.500), train_loss = 3.37675940, grad/param norm = 1.9811e-01, time/batch = 0.2694s	
136/2700 (epoch 2.519), train_loss = 3.33167523, grad/param norm = 2.0870e-01, time/batch = 0.2686s	
137/2700 (epoch 2.537), train_loss = 3.33568911, grad/param norm = 2.1477e-01, time/batch = 0.2685s	
138/2700 (epoch 2.556), train_loss = 3.27116989, grad/param norm = 1.7704e-01, time/batch = 0.2684s	
139/2700 (epoch 2.574), train_loss = 3.23463300, grad/param norm = 1.5785e-01, time/batch = 0.2695s	
140/2700 (epoch 2.593), train_loss = 3.23748509, grad/param norm = 1.9330e-01, time/batch = 0.2690s	
141/2700 (epoch 2.611), train_loss = 3.17636679, grad/param norm = 1.5032e-01, time/batch = 0.2694s	
142/2700 (epoch 2.630), train_loss = 3.21709286, grad/param norm = 1.6390e-01, time/batch = 0.2681s	
143/2700 (epoch 2.648), train_loss = 3.29051223, grad/param norm = 1.7858e-01, time/batch = 0.2683s	
144/2700 (epoch 2.667), train_loss = 3.22216761, grad/param norm = 1.6460e-01, time/batch = 0.2696s	
145/2700 (epoch 2.685), train_loss = 3.21557692, grad/param norm = 1.4310e-01, time/batch = 0.2686s	
146/2700 (epoch 2.704), train_loss = 3.18908731, grad/param norm = 1.7223e-01, time/batch = 0.2687s	
147/2700 (epoch 2.722), train_loss = 3.17993410, grad/param norm = 1.3261e-01, time/batch = 0.2689s	
148/2700 (epoch 2.741), train_loss = 3.31134549, grad/param norm = 1.2644e-01, time/batch = 0.2684s	
149/2700 (epoch 2.759), train_loss = 3.26244868, grad/param norm = 1.7070e-01, time/batch = 0.2685s	
150/2700 (epoch 2.778), train_loss = 3.25723825, grad/param norm = 1.9484e-01, time/batch = 0.2687s	
151/2700 (epoch 2.796), train_loss = 3.25211526, grad/param norm = 2.0472e-01, time/batch = 0.2701s	
152/2700 (epoch 2.815), train_loss = 3.20291173, grad/param norm = 1.8861e-01, time/batch = 0.2685s	
153/2700 (epoch 2.833), train_loss = 3.24112210, grad/param norm = 2.0319e-01, time/batch = 0.2684s	
154/2700 (epoch 2.852), train_loss = 3.23017443, grad/param norm = 2.1895e-01, time/batch = 0.2688s	
155/2700 (epoch 2.870), train_loss = 3.22297163, grad/param norm = 1.9331e-01, time/batch = 0.2692s	
156/2700 (epoch 2.889), train_loss = 3.25794353, grad/param norm = 1.8318e-01, time/batch = 0.2688s	
157/2700 (epoch 2.907), train_loss = 3.31026156, grad/param norm = 2.0805e-01, time/batch = 0.2690s	
158/2700 (epoch 2.926), train_loss = 3.26237510, grad/param norm = 2.0813e-01, time/batch = 0.2687s	
159/2700 (epoch 2.944), train_loss = 3.26995178, grad/param norm = 1.7241e-01, time/batch = 0.2688s	
160/2700 (epoch 2.963), train_loss = 3.34588197, grad/param norm = 1.5757e-01, time/batch = 0.2688s	
161/2700 (epoch 2.981), train_loss = 3.40672933, grad/param norm = 1.6730e-01, time/batch = 0.2703s	
162/2700 (epoch 3.000), train_loss = 3.30832350, grad/param norm = 1.6789e-01, time/batch = 0.2683s	
163/2700 (epoch 3.019), train_loss = 3.24705623, grad/param norm = 1.7672e-01, time/batch = 0.2684s	
164/2700 (epoch 3.037), train_loss = 3.26335487, grad/param norm = 1.6157e-01, time/batch = 0.2694s	
165/2700 (epoch 3.056), train_loss = 3.26167219, grad/param norm = 1.2040e-01, time/batch = 0.2691s	
166/2700 (epoch 3.074), train_loss = 3.29485809, grad/param norm = 1.6128e-01, time/batch = 0.2690s	
167/2700 (epoch 3.093), train_loss = 3.30209803, grad/param norm = 1.8526e-01, time/batch = 0.2685s	
168/2700 (epoch 3.111), train_loss = 3.27203891, grad/param norm = 1.6615e-01, time/batch = 0.2686s	
169/2700 (epoch 3.130), train_loss = 3.28866683, grad/param norm = 1.4869e-01, time/batch = 0.2684s	
170/2700 (epoch 3.148), train_loss = 3.25045729, grad/param norm = 1.8019e-01, time/batch = 0.2692s	
171/2700 (epoch 3.167), train_loss = 3.26548382, grad/param norm = 2.1713e-01, time/batch = 0.2692s	
172/2700 (epoch 3.185), train_loss = 3.24646431, grad/param norm = 1.6006e-01, time/batch = 0.2680s	
173/2700 (epoch 3.204), train_loss = 3.18022616, grad/param norm = 1.7678e-01, time/batch = 0.2687s	
174/2700 (epoch 3.222), train_loss = 3.15929008, grad/param norm = 2.1892e-01, time/batch = 0.2693s	
175/2700 (epoch 3.241), train_loss = 3.17721223, grad/param norm = 1.9569e-01, time/batch = 0.2689s	
176/2700 (epoch 3.259), train_loss = 3.21064518, grad/param norm = 1.9832e-01, time/batch = 0.2684s	
177/2700 (epoch 3.278), train_loss = 3.28447411, grad/param norm = 1.9452e-01, time/batch = 0.2685s	
178/2700 (epoch 3.296), train_loss = 3.28902413, grad/param norm = 2.1769e-01, time/batch = 0.2684s	
179/2700 (epoch 3.315), train_loss = 3.26508779, grad/param norm = 2.0630e-01, time/batch = 0.2691s	
180/2700 (epoch 3.333), train_loss = 3.34368211, grad/param norm = 1.8618e-01, time/batch = 0.2690s	
181/2700 (epoch 3.352), train_loss = 3.35009043, grad/param norm = 1.8088e-01, time/batch = 0.2703s	
182/2700 (epoch 3.370), train_loss = 3.29266952, grad/param norm = 1.5969e-01, time/batch = 0.2684s	
183/2700 (epoch 3.389), train_loss = 3.25661265, grad/param norm = 1.2831e-01, time/batch = 0.2692s	
184/2700 (epoch 3.407), train_loss = 3.27872937, grad/param norm = 1.1493e-01, time/batch = 0.2696s	
185/2700 (epoch 3.426), train_loss = 3.28164696, grad/param norm = 1.2107e-01, time/batch = 0.2689s	
186/2700 (epoch 3.444), train_loss = 3.20851889, grad/param norm = 1.1036e-01, time/batch = 0.2694s	
187/2700 (epoch 3.463), train_loss = 3.25226907, grad/param norm = 1.3427e-01, time/batch = 0.2696s	
188/2700 (epoch 3.481), train_loss = 3.32835672, grad/param norm = 1.3273e-01, time/batch = 0.2694s	
189/2700 (epoch 3.500), train_loss = 3.37637778, grad/param norm = 1.9712e-01, time/batch = 0.2688s	
190/2700 (epoch 3.519), train_loss = 3.33129055, grad/param norm = 2.0783e-01, time/batch = 0.2695s	
191/2700 (epoch 3.537), train_loss = 3.33510078, grad/param norm = 2.1120e-01, time/batch = 0.2708s	
192/2700 (epoch 3.556), train_loss = 3.27027452, grad/param norm = 1.6983e-01, time/batch = 0.2694s	
193/2700 (epoch 3.574), train_loss = 3.23433525, grad/param norm = 1.5357e-01, time/batch = 0.2696s	
194/2700 (epoch 3.593), train_loss = 3.23708570, grad/param norm = 1.8864e-01, time/batch = 0.2700s	
195/2700 (epoch 3.611), train_loss = 3.17602106, grad/param norm = 1.4493e-01, time/batch = 0.2694s	
196/2700 (epoch 3.630), train_loss = 3.21674213, grad/param norm = 1.5916e-01, time/batch = 0.2694s	
197/2700 (epoch 3.648), train_loss = 3.28982847, grad/param norm = 1.7347e-01, time/batch = 0.2696s	
198/2700 (epoch 3.667), train_loss = 3.22156687, grad/param norm = 1.6016e-01, time/batch = 0.2687s	
199/2700 (epoch 3.685), train_loss = 3.21550095, grad/param norm = 1.4213e-01, time/batch = 0.2694s	
200/2700 (epoch 3.704), train_loss = 3.18905603, grad/param norm = 1.7249e-01, time/batch = 0.2689s	
201/2700 (epoch 3.722), train_loss = 3.17984465, grad/param norm = 1.3270e-01, time/batch = 0.2695s	
202/2700 (epoch 3.741), train_loss = 3.31106921, grad/param norm = 1.2596e-01, time/batch = 0.2693s	
203/2700 (epoch 3.759), train_loss = 3.26233650, grad/param norm = 1.7085e-01, time/batch = 0.2698s	
204/2700 (epoch 3.778), train_loss = 3.25736862, grad/param norm = 1.9698e-01, time/batch = 0.2692s	
205/2700 (epoch 3.796), train_loss = 3.25228526, grad/param norm = 2.0752e-01, time/batch = 0.2690s	
206/2700 (epoch 3.815), train_loss = 3.20308566, grad/param norm = 1.9132e-01, time/batch = 0.2685s	
207/2700 (epoch 3.833), train_loss = 3.24111752, grad/param norm = 2.0588e-01, time/batch = 0.2691s	
208/2700 (epoch 3.852), train_loss = 3.23002459, grad/param norm = 2.1964e-01, time/batch = 0.2691s	
209/2700 (epoch 3.870), train_loss = 3.22258181, grad/param norm = 1.9010e-01, time/batch = 0.2692s	
210/2700 (epoch 3.889), train_loss = 3.25754701, grad/param norm = 1.7898e-01, time/batch = 0.2695s	
211/2700 (epoch 3.907), train_loss = 3.30999365, grad/param norm = 2.0373e-01, time/batch = 0.2697s	
212/2700 (epoch 3.926), train_loss = 3.26140438, grad/param norm = 2.0284e-01, time/batch = 0.2690s	
213/2700 (epoch 3.944), train_loss = 3.26917672, grad/param norm = 1.6716e-01, time/batch = 0.2695s	
214/2700 (epoch 3.963), train_loss = 3.34535385, grad/param norm = 1.5429e-01, time/batch = 0.2699s	
215/2700 (epoch 3.981), train_loss = 3.40610463, grad/param norm = 1.6431e-01, time/batch = 0.2689s	
216/2700 (epoch 4.000), train_loss = 3.30769036, grad/param norm = 1.6638e-01, time/batch = 0.2692s	
217/2700 (epoch 4.019), train_loss = 3.24696081, grad/param norm = 1.7835e-01, time/batch = 0.2687s	
218/2700 (epoch 4.037), train_loss = 3.26346295, grad/param norm = 1.6426e-01, time/batch = 0.2680s	
219/2700 (epoch 4.056), train_loss = 3.26153891, grad/param norm = 1.1884e-01, time/batch = 0.2688s	
220/2700 (epoch 4.074), train_loss = 3.29446845, grad/param norm = 1.5480e-01, time/batch = 0.2690s	
221/2700 (epoch 4.093), train_loss = 3.30145876, grad/param norm = 1.7831e-01, time/batch = 0.2698s	
222/2700 (epoch 4.111), train_loss = 3.27162642, grad/param norm = 1.6014e-01, time/batch = 0.2687s	
223/2700 (epoch 4.130), train_loss = 3.28842212, grad/param norm = 1.4512e-01, time/batch = 0.2693s	
224/2700 (epoch 4.148), train_loss = 3.25039116, grad/param norm = 1.7977e-01, time/batch = 0.2698s	
225/2700 (epoch 4.167), train_loss = 3.26495370, grad/param norm = 2.1414e-01, time/batch = 0.2687s	
226/2700 (epoch 4.185), train_loss = 3.24623132, grad/param norm = 1.5858e-01, time/batch = 0.2688s	
227/2700 (epoch 4.204), train_loss = 3.18041736, grad/param norm = 1.7967e-01, time/batch = 0.2688s	
228/2700 (epoch 4.222), train_loss = 3.15915966, grad/param norm = 2.2031e-01, time/batch = 0.2684s	
229/2700 (epoch 4.241), train_loss = 3.17745864, grad/param norm = 2.0399e-01, time/batch = 0.2693s	
230/2700 (epoch 4.259), train_loss = 3.21161233, grad/param norm = 2.1326e-01, time/batch = 0.2686s	
231/2700 (epoch 4.278), train_loss = 3.28483895, grad/param norm = 1.9943e-01, time/batch = 0.2695s	
232/2700 (epoch 4.296), train_loss = 3.28873777, grad/param norm = 2.1590e-01, time/batch = 0.2683s	
233/2700 (epoch 4.315), train_loss = 3.26455239, grad/param norm = 2.0227e-01, time/batch = 0.2692s	
234/2700 (epoch 4.333), train_loss = 3.34319105, grad/param norm = 1.7859e-01, time/batch = 0.2690s	
235/2700 (epoch 4.352), train_loss = 3.34968135, grad/param norm = 1.7481e-01, time/batch = 0.2691s	
236/2700 (epoch 4.370), train_loss = 3.29254552, grad/param norm = 1.5743e-01, time/batch = 0.2691s	
237/2700 (epoch 4.389), train_loss = 3.25569490, grad/param norm = 1.2094e-01, time/batch = 0.2689s	
238/2700 (epoch 4.407), train_loss = 3.27799819, grad/param norm = 1.1000e-01, time/batch = 0.2688s	
239/2700 (epoch 4.426), train_loss = 3.28122757, grad/param norm = 1.2011e-01, time/batch = 0.2698s	
240/2700 (epoch 4.444), train_loss = 3.20803266, grad/param norm = 1.0754e-01, time/batch = 0.2694s	
241/2700 (epoch 4.463), train_loss = 3.25186142, grad/param norm = 1.3017e-01, time/batch = 0.2694s	
242/2700 (epoch 4.481), train_loss = 3.32774439, grad/param norm = 1.2328e-01, time/batch = 0.2688s	
243/2700 (epoch 4.500), train_loss = 3.37582987, grad/param norm = 1.8720e-01, time/batch = 0.2685s	
244/2700 (epoch 4.519), train_loss = 3.33090948, grad/param norm = 1.9919e-01, time/batch = 0.2688s	
245/2700 (epoch 4.537), train_loss = 3.33434270, grad/param norm = 2.0459e-01, time/batch = 0.2688s	
246/2700 (epoch 4.556), train_loss = 3.26973186, grad/param norm = 1.6514e-01, time/batch = 0.2685s	
247/2700 (epoch 4.574), train_loss = 3.23376918, grad/param norm = 1.5104e-01, time/batch = 0.2686s	
248/2700 (epoch 4.593), train_loss = 3.23721222, grad/param norm = 1.8992e-01, time/batch = 0.2682s	
249/2700 (epoch 4.611), train_loss = 3.17626194, grad/param norm = 1.4793e-01, time/batch = 0.2692s	
250/2700 (epoch 4.630), train_loss = 3.21721526, grad/param norm = 1.6447e-01, time/batch = 0.2691s	
251/2700 (epoch 4.648), train_loss = 3.29032013, grad/param norm = 1.7900e-01, time/batch = 0.2703s	
252/2700 (epoch 4.667), train_loss = 3.22206656, grad/param norm = 1.6808e-01, time/batch = 0.2687s	
253/2700 (epoch 4.685), train_loss = 3.21602354, grad/param norm = 1.4939e-01, time/batch = 0.2689s	
254/2700 (epoch 4.704), train_loss = 3.18948766, grad/param norm = 1.7822e-01, time/batch = 0.2689s	
255/2700 (epoch 4.722), train_loss = 3.18029833, grad/param norm = 1.3960e-01, time/batch = 0.2690s	
256/2700 (epoch 4.741), train_loss = 3.31127771, grad/param norm = 1.2881e-01, time/batch = 0.2683s	
257/2700 (epoch 4.759), train_loss = 3.26286413, grad/param norm = 1.7589e-01, time/batch = 0.2686s	
258/2700 (epoch 4.778), train_loss = 3.25816967, grad/param norm = 2.0681e-01, time/batch = 0.2719s	
259/2700 (epoch 4.796), train_loss = 3.25315330, grad/param norm = 2.1828e-01, time/batch = 0.2691s	
260/2700 (epoch 4.815), train_loss = 3.20377951, grad/param norm = 2.0040e-01, time/batch = 0.2686s	
261/2700 (epoch 4.833), train_loss = 3.24119044, grad/param norm = 2.0695e-01, time/batch = 0.2695s	
262/2700 (epoch 4.852), train_loss = 3.22930995, grad/param norm = 2.1291e-01, time/batch = 0.2686s	
263/2700 (epoch 4.870), train_loss = 3.22165961, grad/param norm = 1.7543e-01, time/batch = 0.2683s	
264/2700 (epoch 4.889), train_loss = 3.25634164, grad/param norm = 1.6349e-01, time/batch = 0.2688s	
265/2700 (epoch 4.907), train_loss = 3.30929889, grad/param norm = 1.9370e-01, time/batch = 0.2702s	
266/2700 (epoch 4.926), train_loss = 3.26078768, grad/param norm = 1.9646e-01, time/batch = 0.2691s	
267/2700 (epoch 4.944), train_loss = 3.26897797, grad/param norm = 1.6456e-01, time/batch = 0.2691s	
268/2700 (epoch 4.963), train_loss = 3.34535027, grad/param norm = 1.5388e-01, time/batch = 0.2681s	
269/2700 (epoch 4.981), train_loss = 3.40612430, grad/param norm = 1.6493e-01, time/batch = 0.2686s	
270/2700 (epoch 5.000), train_loss = 3.30756015, grad/param norm = 1.6576e-01, time/batch = 0.2687s	
271/2700 (epoch 5.019), train_loss = 3.24690704, grad/param norm = 1.7822e-01, time/batch = 0.2700s	
272/2700 (epoch 5.037), train_loss = 3.26347021, grad/param norm = 1.6430e-01, time/batch = 0.2683s	
273/2700 (epoch 5.056), train_loss = 3.26175084, grad/param norm = 1.2204e-01, time/batch = 0.2687s	
274/2700 (epoch 5.074), train_loss = 3.29476687, grad/param norm = 1.5966e-01, time/batch = 0.2688s	
275/2700 (epoch 5.093), train_loss = 3.30181588, grad/param norm = 1.8377e-01, time/batch = 0.2694s	
276/2700 (epoch 5.111), train_loss = 3.27191336, grad/param norm = 1.6474e-01, time/batch = 0.2684s	
277/2700 (epoch 5.130), train_loss = 3.28874207, grad/param norm = 1.5055e-01, time/batch = 0.2686s	
278/2700 (epoch 5.148), train_loss = 3.25047383, grad/param norm = 1.8158e-01, time/batch = 0.2682s	
279/2700 (epoch 5.167), train_loss = 3.26509981, grad/param norm = 2.1748e-01, time/batch = 0.2696s	
280/2700 (epoch 5.185), train_loss = 3.24663042, grad/param norm = 1.6609e-01, time/batch = 0.2687s	
281/2700 (epoch 5.204), train_loss = 3.18126922, grad/param norm = 1.9001e-01, time/batch = 0.2696s	
282/2700 (epoch 5.222), train_loss = 3.15971718, grad/param norm = 2.2721e-01, time/batch = 0.2681s	
283/2700 (epoch 5.241), train_loss = 3.17745677, grad/param norm = 2.0578e-01, time/batch = 0.2686s	
284/2700 (epoch 5.259), train_loss = 3.21127913, grad/param norm = 2.0665e-01, time/batch = 0.2690s	
285/2700 (epoch 5.278), train_loss = 3.28419837, grad/param norm = 1.9070e-01, time/batch = 0.2687s	
286/2700 (epoch 5.296), train_loss = 3.28792063, grad/param norm = 2.0607e-01, time/batch = 0.2692s	
287/2700 (epoch 5.315), train_loss = 3.26361323, grad/param norm = 1.9350e-01, time/batch = 0.2698s	
288/2700 (epoch 5.333), train_loss = 3.34260410, grad/param norm = 1.7257e-01, time/batch = 0.2696s	
289/2700 (epoch 5.352), train_loss = 3.34935583, grad/param norm = 1.7204e-01, time/batch = 0.2695s	
290/2700 (epoch 5.370), train_loss = 3.29268596, grad/param norm = 1.5847e-01, time/batch = 0.2686s	
291/2700 (epoch 5.389), train_loss = 3.25591261, grad/param norm = 1.2322e-01, time/batch = 0.2698s	
292/2700 (epoch 5.407), train_loss = 3.27784465, grad/param norm = 1.0869e-01, time/batch = 0.2687s	
293/2700 (epoch 5.426), train_loss = 3.28101278, grad/param norm = 1.1892e-01, time/batch = 0.2687s	
294/2700 (epoch 5.444), train_loss = 3.20783093, grad/param norm = 1.0488e-01, time/batch = 0.2689s	
295/2700 (epoch 5.463), train_loss = 3.25186378, grad/param norm = 1.2957e-01, time/batch = 0.2690s	
296/2700 (epoch 5.481), train_loss = 3.32766325, grad/param norm = 1.2155e-01, time/batch = 0.2683s	
297/2700 (epoch 5.500), train_loss = 3.37575799, grad/param norm = 1.8557e-01, time/batch = 0.2686s	
298/2700 (epoch 5.519), train_loss = 3.33090008, grad/param norm = 1.9831e-01, time/batch = 0.2683s	
299/2700 (epoch 5.537), train_loss = 3.33435127, grad/param norm = 2.0387e-01, time/batch = 0.2688s	
300/2700 (epoch 5.556), train_loss = 3.26967816, grad/param norm = 1.6387e-01, time/batch = 0.2686s	
301/2700 (epoch 5.574), train_loss = 3.23375845, grad/param norm = 1.5085e-01, time/batch = 0.2698s	
302/2700 (epoch 5.593), train_loss = 3.23724635, grad/param norm = 1.8999e-01, time/batch = 0.2683s	
303/2700 (epoch 5.611), train_loss = 3.17630646, grad/param norm = 1.4849e-01, time/batch = 0.2688s	
304/2700 (epoch 5.630), train_loss = 3.21736277, grad/param norm = 1.6582e-01, time/batch = 0.2692s	
305/2700 (epoch 5.648), train_loss = 3.29046484, grad/param norm = 1.8056e-01, time/batch = 0.2690s	
306/2700 (epoch 5.667), train_loss = 3.22223088, grad/param norm = 1.7006e-01, time/batch = 0.2692s	
307/2700 (epoch 5.685), train_loss = 3.21611753, grad/param norm = 1.5187e-01, time/batch = 0.2694s	
308/2700 (epoch 5.704), train_loss = 3.18973568, grad/param norm = 1.8122e-01, time/batch = 0.2685s	
309/2700 (epoch 5.722), train_loss = 3.18053090, grad/param norm = 1.4322e-01, time/batch = 0.2694s	
310/2700 (epoch 5.741), train_loss = 3.31135135, grad/param norm = 1.3020e-01, time/batch = 0.2694s	
311/2700 (epoch 5.759), train_loss = 3.26300138, grad/param norm = 1.7741e-01, time/batch = 0.2707s	
312/2700 (epoch 5.778), train_loss = 3.25835872, grad/param norm = 2.0957e-01, time/batch = 0.2684s	
313/2700 (epoch 5.796), train_loss = 3.25327784, grad/param norm = 2.2081e-01, time/batch = 0.2685s	
314/2700 (epoch 5.815), train_loss = 3.20378591, grad/param norm = 2.0162e-01, time/batch = 0.2691s	
315/2700 (epoch 5.833), train_loss = 3.24092101, grad/param norm = 2.0438e-01, time/batch = 0.2691s	
316/2700 (epoch 5.852), train_loss = 3.22882155, grad/param norm = 2.0790e-01, time/batch = 0.2686s	
317/2700 (epoch 5.870), train_loss = 3.22127199, grad/param norm = 1.6834e-01, time/batch = 0.2685s	
318/2700 (epoch 5.889), train_loss = 3.25592210, grad/param norm = 1.5662e-01, time/batch = 0.2683s	
319/2700 (epoch 5.907), train_loss = 3.30892845, grad/param norm = 1.8846e-01, time/batch = 0.2690s	
320/2700 (epoch 5.926), train_loss = 3.26045322, grad/param norm = 1.9344e-01, time/batch = 0.2692s	
321/2700 (epoch 5.944), train_loss = 3.26880003, grad/param norm = 1.6286e-01, time/batch = 0.2697s	
322/2700 (epoch 5.963), train_loss = 3.34525055, grad/param norm = 1.5360e-01, time/batch = 0.2697s	
323/2700 (epoch 5.981), train_loss = 3.40602064, grad/param norm = 1.6496e-01, time/batch = 0.2697s	
324/2700 (epoch 6.000), train_loss = 3.30730797, grad/param norm = 1.6399e-01, time/batch = 0.2695s	
325/2700 (epoch 6.019), train_loss = 3.24701371, grad/param norm = 1.7731e-01, time/batch = 0.2696s	
326/2700 (epoch 6.037), train_loss = 3.26359135, grad/param norm = 1.6507e-01, time/batch = 0.2698s	
327/2700 (epoch 6.056), train_loss = 3.26229952, grad/param norm = 1.3040e-01, time/batch = 0.2691s	
328/2700 (epoch 6.074), train_loss = 3.29534764, grad/param norm = 1.6996e-01, time/batch = 0.2683s	
329/2700 (epoch 6.093), train_loss = 3.30235706, grad/param norm = 1.9218e-01, time/batch = 0.2689s	
330/2700 (epoch 6.111), train_loss = 3.27210228, grad/param norm = 1.6916e-01, time/batch = 0.2689s	
331/2700 (epoch 6.130), train_loss = 3.28845566, grad/param norm = 1.4809e-01, time/batch = 0.2699s	
332/2700 (epoch 6.148), train_loss = 3.24957315, grad/param norm = 1.6940e-01, time/batch = 0.2683s	
333/2700 (epoch 6.167), train_loss = 3.26345334, grad/param norm = 2.0437e-01, time/batch = 0.2687s	
334/2700 (epoch 6.185), train_loss = 3.24539913, grad/param norm = 1.4533e-01, time/batch = 0.2691s	
335/2700 (epoch 6.204), train_loss = 3.17891049, grad/param norm = 1.6008e-01, time/batch = 0.2686s	
336/2700 (epoch 6.222), train_loss = 3.15747108, grad/param norm = 2.0463e-01, time/batch = 0.2685s	
337/2700 (epoch 6.241), train_loss = 3.17586189, grad/param norm = 1.8309e-01, time/batch = 0.2690s	
338/2700 (epoch 6.259), train_loss = 3.21038225, grad/param norm = 1.9833e-01, time/batch = 0.2684s	
339/2700 (epoch 6.278), train_loss = 3.28431750, grad/param norm = 1.9722e-01, time/batch = 0.2689s	
340/2700 (epoch 6.296), train_loss = 3.28892123, grad/param norm = 2.2104e-01, time/batch = 0.2688s	
341/2700 (epoch 6.315), train_loss = 3.26529331, grad/param norm = 2.1050e-01, time/batch = 0.2697s	
342/2700 (epoch 6.333), train_loss = 3.34373687, grad/param norm = 1.8626e-01, time/batch = 0.2687s	
343/2700 (epoch 6.352), train_loss = 3.35004457, grad/param norm = 1.7907e-01, time/batch = 0.2686s	
344/2700 (epoch 6.370), train_loss = 3.29242372, grad/param norm = 1.5685e-01, time/batch = 0.2691s	
345/2700 (epoch 6.389), train_loss = 3.25553506, grad/param norm = 1.1936e-01, time/batch = 0.2690s	
346/2700 (epoch 6.407), train_loss = 3.27804147, grad/param norm = 1.1054e-01, time/batch = 0.2694s	
347/2700 (epoch 6.426), train_loss = 3.28131993, grad/param norm = 1.2055e-01, time/batch = 0.2693s	
348/2700 (epoch 6.444), train_loss = 3.20811170, grad/param norm = 1.0784e-01, time/batch = 0.2685s	
349/2700 (epoch 6.463), train_loss = 3.25175607, grad/param norm = 1.2929e-01, time/batch = 0.2691s	
350/2700 (epoch 6.481), train_loss = 3.32754322, grad/param norm = 1.2182e-01, time/batch = 0.2690s	
351/2700 (epoch 6.500), train_loss = 3.37572569, grad/param norm = 1.8628e-01, time/batch = 0.2699s	
352/2700 (epoch 6.519), train_loss = 3.33089985, grad/param norm = 1.9866e-01, time/batch = 0.2686s	
353/2700 (epoch 6.537), train_loss = 3.33421495, grad/param norm = 2.0349e-01, time/batch = 0.2690s	
354/2700 (epoch 6.556), train_loss = 3.26954472, grad/param norm = 1.6379e-01, time/batch = 0.2693s	
355/2700 (epoch 6.574), train_loss = 3.23370043, grad/param norm = 1.4971e-01, time/batch = 0.2689s	
356/2700 (epoch 6.593), train_loss = 3.23699981, grad/param norm = 1.8780e-01, time/batch = 0.2704s	
357/2700 (epoch 6.611), train_loss = 3.17598385, grad/param norm = 1.4485e-01, time/batch = 0.2687s	
358/2700 (epoch 6.630), train_loss = 3.21699747, grad/param norm = 1.6145e-01, time/batch = 0.2684s	
359/2700 (epoch 6.648), train_loss = 3.28998746, grad/param norm = 1.7555e-01, time/batch = 0.2690s	
360/2700 (epoch 6.667), train_loss = 3.22178522, grad/param norm = 1.6485e-01, time/batch = 0.2690s	
361/2700 (epoch 6.685), train_loss = 3.21585874, grad/param norm = 1.4710e-01, time/batch = 0.2697s	
362/2700 (epoch 6.704), train_loss = 3.18929605, grad/param norm = 1.7598e-01, time/batch = 0.2682s	
363/2700 (epoch 6.722), train_loss = 3.18010028, grad/param norm = 1.3751e-01, time/batch = 0.2686s	
364/2700 (epoch 6.741), train_loss = 3.31107064, grad/param norm = 1.2745e-01, time/batch = 0.2691s	
365/2700 (epoch 6.759), train_loss = 3.26266237, grad/param norm = 1.7440e-01, time/batch = 0.2690s	
366/2700 (epoch 6.778), train_loss = 3.25802907, grad/param norm = 2.0545e-01, time/batch = 0.2687s	
367/2700 (epoch 6.796), train_loss = 3.25302511, grad/param norm = 2.1680e-01, time/batch = 0.2689s	
368/2700 (epoch 6.815), train_loss = 3.20361291, grad/param norm = 1.9861e-01, time/batch = 0.2685s	
369/2700 (epoch 6.833), train_loss = 3.24104448, grad/param norm = 2.0583e-01, time/batch = 0.2690s	
370/2700 (epoch 6.852), train_loss = 3.22917939, grad/param norm = 2.1178e-01, time/batch = 0.2691s	
371/2700 (epoch 6.870), train_loss = 3.22148304, grad/param norm = 1.7469e-01, time/batch = 0.2707s	
372/2700 (epoch 6.889), train_loss = 3.25631577, grad/param norm = 1.6306e-01, time/batch = 0.2685s	
373/2700 (epoch 6.907), train_loss = 3.30927493, grad/param norm = 1.9298e-01, time/batch = 0.2700s	
374/2700 (epoch 6.926), train_loss = 3.26049901, grad/param norm = 1.9294e-01, time/batch = 0.2695s	
375/2700 (epoch 6.944), train_loss = 3.26874825, grad/param norm = 1.6054e-01, time/batch = 0.2690s	
376/2700 (epoch 6.963), train_loss = 3.34543027, grad/param norm = 1.5173e-01, time/batch = 0.2692s	
377/2700 (epoch 6.981), train_loss = 3.40619741, grad/param norm = 1.6440e-01, time/batch = 0.2695s	
378/2700 (epoch 7.000), train_loss = 3.30714698, grad/param norm = 1.6291e-01, time/batch = 0.2683s	
379/2700 (epoch 7.019), train_loss = 3.24692716, grad/param norm = 1.7829e-01, time/batch = 0.2691s	
380/2700 (epoch 7.037), train_loss = 3.26363739, grad/param norm = 1.6480e-01, time/batch = 0.2688s	
381/2700 (epoch 7.056), train_loss = 3.26193570, grad/param norm = 1.2425e-01, time/batch = 0.2695s	
382/2700 (epoch 7.074), train_loss = 3.29488719, grad/param norm = 1.6236e-01, time/batch = 0.2688s	
383/2700 (epoch 7.093), train_loss = 3.30189385, grad/param norm = 1.8421e-01, time/batch = 0.2691s	
384/2700 (epoch 7.111), train_loss = 3.27169487, grad/param norm = 1.6351e-01, time/batch = 0.2693s	
385/2700 (epoch 7.130), train_loss = 3.28795070, grad/param norm = 1.4128e-01, time/batch = 0.2686s	
386/2700 (epoch 7.148), train_loss = 3.24908477, grad/param norm = 1.6395e-01, time/batch = 0.2691s	
387/2700 (epoch 7.167), train_loss = 3.26324152, grad/param norm = 2.0263e-01, time/batch = 0.2687s	
388/2700 (epoch 7.185), train_loss = 3.24553050, grad/param norm = 1.4610e-01, time/batch = 0.2681s	
389/2700 (epoch 7.204), train_loss = 3.17899658, grad/param norm = 1.6110e-01, time/batch = 0.2694s	
390/2700 (epoch 7.222), train_loss = 3.15796568, grad/param norm = 2.0741e-01, time/batch = 0.2689s	
391/2700 (epoch 7.241), train_loss = 3.17640986, grad/param norm = 1.8765e-01, time/batch = 0.2696s	
392/2700 (epoch 7.259), train_loss = 3.21061702, grad/param norm = 2.0218e-01, time/batch = 0.2686s	
393/2700 (epoch 7.278), train_loss = 3.28478360, grad/param norm = 2.0035e-01, time/batch = 0.2693s	
394/2700 (epoch 7.296), train_loss = 3.28959952, grad/param norm = 2.2564e-01, time/batch = 0.2690s	
395/2700 (epoch 7.315), train_loss = 3.26587664, grad/param norm = 2.1462e-01, time/batch = 0.2689s	
396/2700 (epoch 7.333), train_loss = 3.34404709, grad/param norm = 1.8851e-01, time/batch = 0.2685s	
397/2700 (epoch 7.352), train_loss = 3.35012439, grad/param norm = 1.7958e-01, time/batch = 0.2686s	
398/2700 (epoch 7.370), train_loss = 3.29233910, grad/param norm = 1.5620e-01, time/batch = 0.2686s	
399/2700 (epoch 7.389), train_loss = 3.25540817, grad/param norm = 1.1830e-01, time/batch = 0.2691s	
400/2700 (epoch 7.407), train_loss = 3.27803505, grad/param norm = 1.1054e-01, time/batch = 0.2696s	
401/2700 (epoch 7.426), train_loss = 3.28132504, grad/param norm = 1.2066e-01, time/batch = 0.2695s	
402/2700 (epoch 7.444), train_loss = 3.20810225, grad/param norm = 1.0774e-01, time/batch = 0.2687s	
403/2700 (epoch 7.463), train_loss = 3.25164569, grad/param norm = 1.2806e-01, time/batch = 0.2691s	
404/2700 (epoch 7.481), train_loss = 3.32756499, grad/param norm = 1.2144e-01, time/batch = 0.2690s	
405/2700 (epoch 7.500), train_loss = 3.37578436, grad/param norm = 1.8593e-01, time/batch = 0.2690s	
406/2700 (epoch 7.519), train_loss = 3.33082890, grad/param norm = 1.9769e-01, time/batch = 0.2685s	
407/2700 (epoch 7.537), train_loss = 3.33409806, grad/param norm = 2.0234e-01, time/batch = 0.2688s	
408/2700 (epoch 7.556), train_loss = 3.26950231, grad/param norm = 1.6301e-01, time/batch = 0.2683s	
409/2700 (epoch 7.574), train_loss = 3.23360164, grad/param norm = 1.4884e-01, time/batch = 0.2699s	
410/2700 (epoch 7.593), train_loss = 3.23689423, grad/param norm = 1.8690e-01, time/batch = 0.2692s	
411/2700 (epoch 7.611), train_loss = 3.17590578, grad/param norm = 1.4399e-01, time/batch = 0.2708s	
412/2700 (epoch 7.630), train_loss = 3.21693031, grad/param norm = 1.6071e-01, time/batch = 0.2689s	
413/2700 (epoch 7.648), train_loss = 3.28989100, grad/param norm = 1.7457e-01, time/batch = 0.2695s	
414/2700 (epoch 7.667), train_loss = 3.22166187, grad/param norm = 1.6364e-01, time/batch = 0.2694s	
415/2700 (epoch 7.685), train_loss = 3.21582249, grad/param norm = 1.4631e-01, time/batch = 0.2694s	
416/2700 (epoch 7.704), train_loss = 3.18925844, grad/param norm = 1.7538e-01, time/batch = 0.2692s	
417/2700 (epoch 7.722), train_loss = 3.18008608, grad/param norm = 1.3698e-01, time/batch = 0.2696s	
418/2700 (epoch 7.741), train_loss = 3.31107939, grad/param norm = 1.2727e-01, time/batch = 0.2687s	
419/2700 (epoch 7.759), train_loss = 3.26264935, grad/param norm = 1.7418e-01, time/batch = 0.2690s	
420/2700 (epoch 7.778), train_loss = 3.25806029, grad/param norm = 2.0546e-01, time/batch = 0.2699s	
421/2700 (epoch 7.796), train_loss = 3.25309594, grad/param norm = 2.1711e-01, time/batch = 0.2720s	
422/2700 (epoch 7.815), train_loss = 3.20370004, grad/param norm = 1.9923e-01, time/batch = 0.2685s	
423/2700 (epoch 7.833), train_loss = 3.24113449, grad/param norm = 2.0687e-01, time/batch = 0.2696s	
424/2700 (epoch 7.852), train_loss = 3.22927950, grad/param norm = 2.1277e-01, time/batch = 0.2695s	
425/2700 (epoch 7.870), train_loss = 3.22152078, grad/param norm = 1.7531e-01, time/batch = 0.2694s	
426/2700 (epoch 7.889), train_loss = 3.25633950, grad/param norm = 1.6336e-01, time/batch = 0.2688s	
427/2700 (epoch 7.907), train_loss = 3.30928818, grad/param norm = 1.9290e-01, time/batch = 0.2687s	
428/2700 (epoch 7.926), train_loss = 3.26049803, grad/param norm = 1.9260e-01, time/batch = 0.2686s	
429/2700 (epoch 7.944), train_loss = 3.26874296, grad/param norm = 1.6024e-01, time/batch = 0.2689s	
430/2700 (epoch 7.963), train_loss = 3.34541360, grad/param norm = 1.5141e-01, time/batch = 0.2694s	
431/2700 (epoch 7.981), train_loss = 3.40618346, grad/param norm = 1.6408e-01, time/batch = 0.2697s	
432/2700 (epoch 8.000), train_loss = 3.30711871, grad/param norm = 1.6262e-01, time/batch = 0.2682s	
433/2700 (epoch 8.019), train_loss = 3.24690562, grad/param norm = 1.7805e-01, time/batch = 0.2689s	
434/2700 (epoch 8.037), train_loss = 3.26362527, grad/param norm = 1.6449e-01, time/batch = 0.2691s	
435/2700 (epoch 8.056), train_loss = 3.26193081, grad/param norm = 1.2409e-01, time/batch = 0.2689s	
436/2700 (epoch 8.074), train_loss = 3.29489541, grad/param norm = 1.6233e-01, time/batch = 0.2687s	
437/2700 (epoch 8.093), train_loss = 3.30189273, grad/param norm = 1.8397e-01, time/batch = 0.2685s	
438/2700 (epoch 8.111), train_loss = 3.27167404, grad/param norm = 1.6310e-01, time/batch = 0.2686s	
439/2700 (epoch 8.130), train_loss = 3.28792692, grad/param norm = 1.4076e-01, time/batch = 0.2686s	
440/2700 (epoch 8.148), train_loss = 3.24904584, grad/param norm = 1.6339e-01, time/batch = 0.2687s	
441/2700 (epoch 8.167), train_loss = 3.26321279, grad/param norm = 2.0218e-01, time/batch = 0.2699s	
442/2700 (epoch 8.185), train_loss = 3.24551671, grad/param norm = 1.4568e-01, time/batch = 0.2687s	
443/2700 (epoch 8.204), train_loss = 3.17897225, grad/param norm = 1.6054e-01, time/batch = 0.2689s	
444/2700 (epoch 8.222), train_loss = 3.15795228, grad/param norm = 2.0691e-01, time/batch = 0.2698s	
445/2700 (epoch 8.241), train_loss = 3.17637275, grad/param norm = 1.8687e-01, time/batch = 0.2695s	
446/2700 (epoch 8.259), train_loss = 3.21056791, grad/param norm = 2.0120e-01, time/batch = 0.2688s	
447/2700 (epoch 8.278), train_loss = 3.28476420, grad/param norm = 1.9984e-01, time/batch = 0.2690s	
448/2700 (epoch 8.296), train_loss = 3.28963535, grad/param norm = 2.2566e-01, time/batch = 0.2692s	
449/2700 (epoch 8.315), train_loss = 3.26592024, grad/param norm = 2.1477e-01, time/batch = 0.2696s	
450/2700 (epoch 8.333), train_loss = 3.34407453, grad/param norm = 1.8866e-01, time/batch = 0.2689s	
451/2700 (epoch 8.352), train_loss = 3.35015688, grad/param norm = 1.7973e-01, time/batch = 0.2705s	
452/2700 (epoch 8.370), train_loss = 3.29234725, grad/param norm = 1.5612e-01, time/batch = 0.2688s	
453/2700 (epoch 8.389), train_loss = 3.25540121, grad/param norm = 1.1814e-01, time/batch = 0.2687s	
454/2700 (epoch 8.407), train_loss = 3.27803754, grad/param norm = 1.1044e-01, time/batch = 0.2699s	
455/2700 (epoch 8.426), train_loss = 3.28132769, grad/param norm = 1.2056e-01, time/batch = 0.2690s	
456/2700 (epoch 8.444), train_loss = 3.20809683, grad/param norm = 1.0754e-01, time/batch = 0.2694s	
457/2700 (epoch 8.463), train_loss = 3.25162849, grad/param norm = 1.2773e-01, time/batch = 0.2692s	
458/2700 (epoch 8.481), train_loss = 3.32756417, grad/param norm = 1.2122e-01, time/batch = 0.2685s	
459/2700 (epoch 8.500), train_loss = 3.37579782, grad/param norm = 1.8573e-01, time/batch = 0.2687s	
460/2700 (epoch 8.519), train_loss = 3.33084064, grad/param norm = 1.9755e-01, time/batch = 0.2695s	
461/2700 (epoch 8.537), train_loss = 3.33409433, grad/param norm = 2.0208e-01, time/batch = 0.2704s	
462/2700 (epoch 8.556), train_loss = 3.26949932, grad/param norm = 1.6272e-01, time/batch = 0.2688s	
463/2700 (epoch 8.574), train_loss = 3.23358598, grad/param norm = 1.4858e-01, time/batch = 0.2689s	
464/2700 (epoch 8.593), train_loss = 3.23688538, grad/param norm = 1.8665e-01, time/batch = 0.2697s	
465/2700 (epoch 8.611), train_loss = 3.17589926, grad/param norm = 1.4380e-01, time/batch = 0.2695s	
466/2700 (epoch 8.630), train_loss = 3.21693140, grad/param norm = 1.6055e-01, time/batch = 0.2686s	
467/2700 (epoch 8.648), train_loss = 3.28987829, grad/param norm = 1.7429e-01, time/batch = 0.2687s	
468/2700 (epoch 8.667), train_loss = 3.22164530, grad/param norm = 1.6336e-01, time/batch = 0.2689s	
469/2700 (epoch 8.685), train_loss = 3.21582372, grad/param norm = 1.4614e-01, time/batch = 0.2688s	
470/2700 (epoch 8.704), train_loss = 3.18925982, grad/param norm = 1.7521e-01, time/batch = 0.2688s	
471/2700 (epoch 8.722), train_loss = 3.18009134, grad/param norm = 1.3688e-01, time/batch = 0.2696s	
472/2700 (epoch 8.741), train_loss = 3.31107995, grad/param norm = 1.2717e-01, time/batch = 0.2686s	
473/2700 (epoch 8.759), train_loss = 3.26265114, grad/param norm = 1.7405e-01, time/batch = 0.2694s	
474/2700 (epoch 8.778), train_loss = 3.25807860, grad/param norm = 2.0543e-01, time/batch = 0.2697s	
475/2700 (epoch 8.796), train_loss = 3.25312216, grad/param norm = 2.1711e-01, time/batch = 0.2696s	
476/2700 (epoch 8.815), train_loss = 3.20371799, grad/param norm = 1.9920e-01, time/batch = 0.2687s	
477/2700 (epoch 8.833), train_loss = 3.24114190, grad/param norm = 2.0679e-01, time/batch = 0.2688s	
478/2700 (epoch 8.852), train_loss = 3.22927956, grad/param norm = 2.1261e-01, time/batch = 0.2689s	
479/2700 (epoch 8.870), train_loss = 3.22151454, grad/param norm = 1.7510e-01, time/batch = 0.2694s	
480/2700 (epoch 8.889), train_loss = 3.25633250, grad/param norm = 1.6313e-01, time/batch = 0.2687s	
481/2700 (epoch 8.907), train_loss = 3.30928562, grad/param norm = 1.9263e-01, time/batch = 0.2703s	
482/2700 (epoch 8.926), train_loss = 3.26049511, grad/param norm = 1.9232e-01, time/batch = 0.2684s	
483/2700 (epoch 8.944), train_loss = 3.26874462, grad/param norm = 1.6003e-01, time/batch = 0.2693s	
484/2700 (epoch 8.963), train_loss = 3.34541022, grad/param norm = 1.5121e-01, time/batch = 0.2692s	
485/2700 (epoch 8.981), train_loss = 3.40617509, grad/param norm = 1.6384e-01, time/batch = 0.2692s	
486/2700 (epoch 9.000), train_loss = 3.30710455, grad/param norm = 1.6239e-01, time/batch = 0.2691s	
487/2700 (epoch 9.019), train_loss = 3.24690117, grad/param norm = 1.7787e-01, time/batch = 0.2694s	
488/2700 (epoch 9.037), train_loss = 3.26362113, grad/param norm = 1.6428e-01, time/batch = 0.2690s	
489/2700 (epoch 9.056), train_loss = 3.26192741, grad/param norm = 1.2392e-01, time/batch = 0.2690s	
490/2700 (epoch 9.074), train_loss = 3.29489605, grad/param norm = 1.6216e-01, time/batch = 0.2694s	
491/2700 (epoch 9.093), train_loss = 3.30189361, grad/param norm = 1.8378e-01, time/batch = 0.2700s	
492/2700 (epoch 9.111), train_loss = 3.27167022, grad/param norm = 1.6289e-01, time/batch = 0.2694s	
493/2700 (epoch 9.130), train_loss = 3.28792446, grad/param norm = 1.4056e-01, time/batch = 0.2695s	
494/2700 (epoch 9.148), train_loss = 3.24903701, grad/param norm = 1.6314e-01, time/batch = 0.2698s	
495/2700 (epoch 9.167), train_loss = 3.26320819, grad/param norm = 2.0196e-01, time/batch = 0.2697s	
496/2700 (epoch 9.185), train_loss = 3.24551886, grad/param norm = 1.4555e-01, time/batch = 0.2693s	
497/2700 (epoch 9.204), train_loss = 3.17897583, grad/param norm = 1.6041e-01, time/batch = 0.2693s	
498/2700 (epoch 9.222), train_loss = 3.15796115, grad/param norm = 2.0674e-01, time/batch = 0.2688s	
499/2700 (epoch 9.241), train_loss = 3.17637975, grad/param norm = 1.8673e-01, time/batch = 0.2689s	
500/2700 (epoch 9.259), train_loss = 3.21056497, grad/param norm = 2.0094e-01, time/batch = 0.2692s	
501/2700 (epoch 9.278), train_loss = 3.28475714, grad/param norm = 1.9952e-01, time/batch = 0.2698s	
502/2700 (epoch 9.296), train_loss = 3.28963199, grad/param norm = 2.2533e-01, time/batch = 0.2686s	
503/2700 (epoch 9.315), train_loss = 3.26591183, grad/param norm = 2.1445e-01, time/batch = 0.2688s	
504/2700 (epoch 9.333), train_loss = 3.34406939, grad/param norm = 1.8840e-01, time/batch = 0.2696s	
505/2700 (epoch 9.352), train_loss = 3.35015716, grad/param norm = 1.7955e-01, time/batch = 0.2689s	
506/2700 (epoch 9.370), train_loss = 3.29234455, grad/param norm = 1.5594e-01, time/batch = 0.2697s	
507/2700 (epoch 9.389), train_loss = 3.25539902, grad/param norm = 1.1800e-01, time/batch = 0.2692s	
508/2700 (epoch 9.407), train_loss = 3.27804009, grad/param norm = 1.1034e-01, time/batch = 0.2692s	
509/2700 (epoch 9.426), train_loss = 3.28132770, grad/param norm = 1.2042e-01, time/batch = 0.2694s	
510/2700 (epoch 9.444), train_loss = 3.20809494, grad/param norm = 1.0741e-01, time/batch = 0.2693s	
511/2700 (epoch 9.463), train_loss = 3.25162570, grad/param norm = 1.2757e-01, time/batch = 0.2697s	
512/2700 (epoch 9.481), train_loss = 3.32756359, grad/param norm = 1.2111e-01, time/batch = 0.2691s	
513/2700 (epoch 9.500), train_loss = 3.37580401, grad/param norm = 1.8559e-01, time/batch = 0.2688s	
514/2700 (epoch 9.519), train_loss = 3.33085067, grad/param norm = 1.9747e-01, time/batch = 0.2692s	
515/2700 (epoch 9.537), train_loss = 3.33409362, grad/param norm = 2.0185e-01, time/batch = 0.2700s	
516/2700 (epoch 9.556), train_loss = 3.26949212, grad/param norm = 1.6239e-01, time/batch = 0.2693s	
517/2700 (epoch 9.574), train_loss = 3.23357301, grad/param norm = 1.4830e-01, time/batch = 0.2692s	
518/2700 (epoch 9.593), train_loss = 3.23687985, grad/param norm = 1.8637e-01, time/batch = 0.2695s	
519/2700 (epoch 9.611), train_loss = 3.17589431, grad/param norm = 1.4357e-01, time/batch = 0.2714s	
520/2700 (epoch 9.630), train_loss = 3.21692951, grad/param norm = 1.6031e-01, time/batch = 0.2700s	
521/2700 (epoch 9.648), train_loss = 3.28986835, grad/param norm = 1.7400e-01, time/batch = 0.2698s	
522/2700 (epoch 9.667), train_loss = 3.22163458, grad/param norm = 1.6308e-01, time/batch = 0.2687s	
523/2700 (epoch 9.685), train_loss = 3.21582317, grad/param norm = 1.4597e-01, time/batch = 0.2694s	
524/2700 (epoch 9.704), train_loss = 3.18926138, grad/param norm = 1.7505e-01, time/batch = 0.2695s	
525/2700 (epoch 9.722), train_loss = 3.18009506, grad/param norm = 1.3679e-01, time/batch = 0.2696s	
526/2700 (epoch 9.741), train_loss = 3.31107891, grad/param norm = 1.2707e-01, time/batch = 0.2693s	
527/2700 (epoch 9.759), train_loss = 3.26265400, grad/param norm = 1.7393e-01, time/batch = 0.2693s	
528/2700 (epoch 9.778), train_loss = 3.25809664, grad/param norm = 2.0541e-01, time/batch = 0.2686s	
529/2700 (epoch 9.796), train_loss = 3.25315042, grad/param norm = 2.1713e-01, time/batch = 0.2688s	
530/2700 (epoch 9.815), train_loss = 3.20373497, grad/param norm = 1.9919e-01, time/batch = 0.2688s	
531/2700 (epoch 9.833), train_loss = 3.24114887, grad/param norm = 2.0671e-01, time/batch = 0.2698s	
532/2700 (epoch 9.852), train_loss = 3.22927839, grad/param norm = 2.1242e-01, time/batch = 0.2683s	
533/2700 (epoch 9.870), train_loss = 3.22150612, grad/param norm = 1.7482e-01, time/batch = 0.2688s	
534/2700 (epoch 9.889), train_loss = 3.25632288, grad/param norm = 1.6284e-01, time/batch = 0.2694s	
535/2700 (epoch 9.907), train_loss = 3.30927954, grad/param norm = 1.9232e-01, time/batch = 0.2688s	
536/2700 (epoch 9.926), train_loss = 3.26049019, grad/param norm = 1.9205e-01, time/batch = 0.2691s	
537/2700 (epoch 9.944), train_loss = 3.26874524, grad/param norm = 1.5984e-01, time/batch = 0.2691s	
538/2700 (epoch 9.963), train_loss = 3.34540789, grad/param norm = 1.5103e-01, time/batch = 0.2688s	
539/2700 (epoch 9.981), train_loss = 3.40616950, grad/param norm = 1.6364e-01, time/batch = 0.2687s	
decayed learning rate by a factor 0.97 to 0.00194	
540/2700 (epoch 10.000), train_loss = 3.30709563, grad/param norm = 1.6219e-01, time/batch = 0.2689s	
541/2700 (epoch 10.019), train_loss = 3.24689946, grad/param norm = 1.7770e-01, time/batch = 0.2698s	
542/2700 (epoch 10.037), train_loss = 3.26241353, grad/param norm = 1.5826e-01, time/batch = 0.2687s	
543/2700 (epoch 10.056), train_loss = 3.26142604, grad/param norm = 1.2135e-01, time/batch = 0.2690s	
544/2700 (epoch 10.074), train_loss = 3.29431693, grad/param norm = 1.5995e-01, time/batch = 0.2698s	
545/2700 (epoch 10.093), train_loss = 3.30070664, grad/param norm = 1.7708e-01, time/batch = 0.2694s	
546/2700 (epoch 10.111), train_loss = 3.27028889, grad/param norm = 1.5311e-01, time/batch = 0.2694s	
547/2700 (epoch 10.130), train_loss = 3.28656085, grad/param norm = 1.3031e-01, time/batch = 0.2690s	
548/2700 (epoch 10.148), train_loss = 3.24787981, grad/param norm = 1.5264e-01, time/batch = 0.2684s	
549/2700 (epoch 10.167), train_loss = 3.26133511, grad/param norm = 1.9054e-01, time/batch = 0.2691s	
550/2700 (epoch 10.185), train_loss = 3.24416358, grad/param norm = 1.3336e-01, time/batch = 0.2696s	
551/2700 (epoch 10.204), train_loss = 3.17771541, grad/param norm = 1.4606e-01, time/batch = 0.2706s	
552/2700 (epoch 10.222), train_loss = 3.15633089, grad/param norm = 1.9416e-01, time/batch = 0.2697s	
553/2700 (epoch 10.241), train_loss = 3.17449526, grad/param norm = 1.6774e-01, time/batch = 0.2697s	
554/2700 (epoch 10.259), train_loss = 3.20884611, grad/param norm = 1.8157e-01, time/batch = 0.2697s	
555/2700 (epoch 10.278), train_loss = 3.28365939, grad/param norm = 1.9093e-01, time/batch = 0.2697s	
556/2700 (epoch 10.296), train_loss = 3.28925278, grad/param norm = 2.2554e-01, time/batch = 0.2692s	
557/2700 (epoch 10.315), train_loss = 3.26582921, grad/param norm = 2.1790e-01, time/batch = 0.2687s	
558/2700 (epoch 10.333), train_loss = 3.34387031, grad/param norm = 1.9247e-01, time/batch = 0.2687s	
559/2700 (epoch 10.352), train_loss = 3.34990159, grad/param norm = 1.8253e-01, time/batch = 0.2688s	
560/2700 (epoch 10.370), train_loss = 3.29173898, grad/param norm = 1.5572e-01, time/batch = 0.2690s	
561/2700 (epoch 10.389), train_loss = 3.25497978, grad/param norm = 1.1610e-01, time/batch = 0.2700s	
562/2700 (epoch 10.407), train_loss = 3.27757970, grad/param norm = 1.0858e-01, time/batch = 0.2684s	
563/2700 (epoch 10.426), train_loss = 3.28082147, grad/param norm = 1.1897e-01, time/batch = 0.2687s	
564/2700 (epoch 10.444), train_loss = 3.20781728, grad/param norm = 1.0433e-01, time/batch = 0.2692s	
565/2700 (epoch 10.463), train_loss = 3.25113280, grad/param norm = 1.2418e-01, time/batch = 0.2689s	
566/2700 (epoch 10.481), train_loss = 3.32726279, grad/param norm = 1.1935e-01, time/batch = 0.2692s	
567/2700 (epoch 10.500), train_loss = 3.37552251, grad/param norm = 1.8416e-01, time/batch = 0.2686s	
568/2700 (epoch 10.519), train_loss = 3.33016207, grad/param norm = 1.9256e-01, time/batch = 0.2690s	
569/2700 (epoch 10.537), train_loss = 3.33323249, grad/param norm = 1.9674e-01, time/batch = 0.2692s	
570/2700 (epoch 10.556), train_loss = 3.26871585, grad/param norm = 1.5872e-01, time/batch = 0.2692s	
571/2700 (epoch 10.574), train_loss = 3.23340500, grad/param norm = 1.4594e-01, time/batch = 0.2707s	
572/2700 (epoch 10.593), train_loss = 3.23612605, grad/param norm = 1.8210e-01, time/batch = 0.2687s	
573/2700 (epoch 10.611), train_loss = 3.17512084, grad/param norm = 1.3733e-01, time/batch = 0.2692s	
574/2700 (epoch 10.630), train_loss = 3.21626263, grad/param norm = 1.5497e-01, time/batch = 0.2691s	
575/2700 (epoch 10.648), train_loss = 3.28890747, grad/param norm = 1.6838e-01, time/batch = 0.2694s	
576/2700 (epoch 10.667), train_loss = 3.22067267, grad/param norm = 1.5674e-01, time/batch = 0.2692s	
577/2700 (epoch 10.685), train_loss = 3.21514724, grad/param norm = 1.4117e-01, time/batch = 0.2692s	
578/2700 (epoch 10.704), train_loss = 3.18849778, grad/param norm = 1.7072e-01, time/batch = 0.2695s	
579/2700 (epoch 10.722), train_loss = 3.17939612, grad/param norm = 1.3214e-01, time/batch = 0.2690s	
580/2700 (epoch 10.741), train_loss = 3.31056562, grad/param norm = 1.2445e-01, time/batch = 0.2690s	
581/2700 (epoch 10.759), train_loss = 3.26197169, grad/param norm = 1.7085e-01, time/batch = 0.2703s	
582/2700 (epoch 10.778), train_loss = 3.25708213, grad/param norm = 2.0009e-01, time/batch = 0.2689s	
583/2700 (epoch 10.796), train_loss = 3.25197363, grad/param norm = 2.1187e-01, time/batch = 0.2688s	
584/2700 (epoch 10.815), train_loss = 3.20293236, grad/param norm = 1.9498e-01, time/batch = 0.2691s	
585/2700 (epoch 10.833), train_loss = 3.24044016, grad/param norm = 2.0410e-01, time/batch = 0.2691s	
586/2700 (epoch 10.852), train_loss = 3.22859107, grad/param norm = 2.0994e-01, time/batch = 0.2688s	
587/2700 (epoch 10.870), train_loss = 3.22065354, grad/param norm = 1.7148e-01, time/batch = 0.2688s	
588/2700 (epoch 10.889), train_loss = 3.25594751, grad/param norm = 1.6050e-01, time/batch = 0.2691s	
589/2700 (epoch 10.907), train_loss = 3.30888290, grad/param norm = 1.8972e-01, time/batch = 0.2695s	
590/2700 (epoch 10.926), train_loss = 3.25957360, grad/param norm = 1.8718e-01, time/batch = 0.2693s	
591/2700 (epoch 10.944), train_loss = 3.26790556, grad/param norm = 1.5603e-01, time/batch = 0.2701s	
592/2700 (epoch 10.963), train_loss = 3.34490778, grad/param norm = 1.4793e-01, time/batch = 0.2688s	
593/2700 (epoch 10.981), train_loss = 3.40573007, grad/param norm = 1.6075e-01, time/batch = 0.2693s	
decayed learning rate by a factor 0.97 to 0.0018818	
594/2700 (epoch 11.000), train_loss = 3.30634508, grad/param norm = 1.5962e-01, time/batch = 0.2694s	
595/2700 (epoch 11.019), train_loss = 3.24631863, grad/param norm = 1.7529e-01, time/batch = 0.2690s	
596/2700 (epoch 11.037), train_loss = 3.26180821, grad/param norm = 1.5531e-01, time/batch = 0.2692s	
597/2700 (epoch 11.056), train_loss = 3.26116892, grad/param norm = 1.2066e-01, time/batch = 0.2687s	
598/2700 (epoch 11.074), train_loss = 3.29405663, grad/param norm = 1.6040e-01, time/batch = 0.2692s	
599/2700 (epoch 11.093), train_loss = 3.30016758, grad/param norm = 1.7574e-01, time/batch = 0.2695s	
600/2700 (epoch 11.111), train_loss = 3.26968966, grad/param norm = 1.5004e-01, time/batch = 0.2691s	
601/2700 (epoch 11.130), train_loss = 3.28584824, grad/param norm = 1.2711e-01, time/batch = 0.2702s	
602/2700 (epoch 11.148), train_loss = 3.24742865, grad/param norm = 1.4876e-01, time/batch = 0.2687s	
603/2700 (epoch 11.167), train_loss = 3.26042301, grad/param norm = 1.8608e-01, time/batch = 0.2686s	
604/2700 (epoch 11.185), train_loss = 3.24347749, grad/param norm = 1.2845e-01, time/batch = 0.2693s	
605/2700 (epoch 11.204), train_loss = 3.17724410, grad/param norm = 1.4093e-01, time/batch = 0.2695s	
606/2700 (epoch 11.222), train_loss = 3.15562372, grad/param norm = 1.8900e-01, time/batch = 0.2692s	
607/2700 (epoch 11.241), train_loss = 3.17360764, grad/param norm = 1.5907e-01, time/batch = 0.2687s	
608/2700 (epoch 11.259), train_loss = 3.20778318, grad/param norm = 1.6993e-01, time/batch = 0.2683s	
609/2700 (epoch 11.278), train_loss = 3.28260394, grad/param norm = 1.8201e-01, time/batch = 0.2688s	
610/2700 (epoch 11.296), train_loss = 3.28833199, grad/param norm = 2.1915e-01, time/batch = 0.2688s	
611/2700 (epoch 11.315), train_loss = 3.26505062, grad/param norm = 2.1403e-01, time/batch = 0.2697s	
612/2700 (epoch 11.333), train_loss = 3.34337850, grad/param norm = 1.9115e-01, time/batch = 0.2687s	
613/2700 (epoch 11.352), train_loss = 3.34939937, grad/param norm = 1.8188e-01, time/batch = 0.2694s	
614/2700 (epoch 11.370), train_loss = 3.29111742, grad/param norm = 1.5443e-01, time/batch = 0.2697s	
615/2700 (epoch 11.389), train_loss = 3.25463157, grad/param norm = 1.1423e-01, time/batch = 0.2692s	
616/2700 (epoch 11.407), train_loss = 3.27719230, grad/param norm = 1.0719e-01, time/batch = 0.2691s	
617/2700 (epoch 11.426), train_loss = 3.28038326, grad/param norm = 1.1761e-01, time/batch = 0.2700s	
618/2700 (epoch 11.444), train_loss = 3.20764136, grad/param norm = 1.0242e-01, time/batch = 0.2697s	
619/2700 (epoch 11.463), train_loss = 3.25076698, grad/param norm = 1.2213e-01, time/batch = 0.2691s	
620/2700 (epoch 11.481), train_loss = 3.32702417, grad/param norm = 1.1864e-01, time/batch = 0.2695s	
621/2700 (epoch 11.500), train_loss = 3.37522790, grad/param norm = 1.8366e-01, time/batch = 0.2698s	
622/2700 (epoch 11.519), train_loss = 3.32957478, grad/param norm = 1.8937e-01, time/batch = 0.2683s	
623/2700 (epoch 11.537), train_loss = 3.33255019, grad/param norm = 1.9308e-01, time/batch = 0.2688s	
624/2700 (epoch 11.556), train_loss = 3.26802694, grad/param norm = 1.5565e-01, time/batch = 0.2701s	
625/2700 (epoch 11.574), train_loss = 3.23324239, grad/param norm = 1.4360e-01, time/batch = 0.2696s	
626/2700 (epoch 11.593), train_loss = 3.23545839, grad/param norm = 1.7811e-01, time/batch = 0.2689s	
627/2700 (epoch 11.611), train_loss = 3.17446548, grad/param norm = 1.3145e-01, time/batch = 0.2693s	
628/2700 (epoch 11.630), train_loss = 3.21565780, grad/param norm = 1.4960e-01, time/batch = 0.2685s	
629/2700 (epoch 11.648), train_loss = 3.28799530, grad/param norm = 1.6252e-01, time/batch = 0.2694s	
630/2700 (epoch 11.667), train_loss = 3.21976513, grad/param norm = 1.4989e-01, time/batch = 0.2691s	
631/2700 (epoch 11.685), train_loss = 3.21450592, grad/param norm = 1.3596e-01, time/batch = 0.2702s	
632/2700 (epoch 11.704), train_loss = 3.18775469, grad/param norm = 1.6592e-01, time/batch = 0.2687s	
633/2700 (epoch 11.722), train_loss = 3.17867599, grad/param norm = 1.2650e-01, time/batch = 0.2693s	
634/2700 (epoch 11.741), train_loss = 3.31004704, grad/param norm = 1.2153e-01, time/batch = 0.2691s	
635/2700 (epoch 11.759), train_loss = 3.26122962, grad/param norm = 1.6700e-01, time/batch = 0.2699s	
636/2700 (epoch 11.778), train_loss = 3.25601530, grad/param norm = 1.9323e-01, time/batch = 0.2693s	
637/2700 (epoch 11.796), train_loss = 3.25072081, grad/param norm = 2.0456e-01, time/batch = 0.2695s	
638/2700 (epoch 11.815), train_loss = 3.20201963, grad/param norm = 1.8886e-01, time/batch = 0.2692s	
639/2700 (epoch 11.833), train_loss = 3.23978044, grad/param norm = 2.0176e-01, time/batch = 0.2689s	
640/2700 (epoch 11.852), train_loss = 3.22811094, grad/param norm = 2.0930e-01, time/batch = 0.2693s	
641/2700 (epoch 11.870), train_loss = 3.22003336, grad/param norm = 1.7106e-01, time/batch = 0.2704s	
642/2700 (epoch 11.889), train_loss = 3.25579693, grad/param norm = 1.6087e-01, time/batch = 0.2691s	
643/2700 (epoch 11.907), train_loss = 3.30858452, grad/param norm = 1.8857e-01, time/batch = 0.2688s	
644/2700 (epoch 11.926), train_loss = 3.25876789, grad/param norm = 1.8330e-01, time/batch = 0.2694s	
645/2700 (epoch 11.944), train_loss = 3.26714833, grad/param norm = 1.5269e-01, time/batch = 0.2690s	
646/2700 (epoch 11.963), train_loss = 3.34442869, grad/param norm = 1.4494e-01, time/batch = 0.2692s	
647/2700 (epoch 11.981), train_loss = 3.40528447, grad/param norm = 1.5775e-01, time/batch = 0.2692s	
decayed learning rate by a factor 0.97 to 0.001825346	
648/2700 (epoch 12.000), train_loss = 3.30560831, grad/param norm = 1.5710e-01, time/batch = 0.2688s	
649/2700 (epoch 12.019), train_loss = 3.24573713, grad/param norm = 1.7301e-01, time/batch = 0.2690s	
650/2700 (epoch 12.037), train_loss = 3.26122517, grad/param norm = 1.5250e-01, time/batch = 0.2694s	
651/2700 (epoch 12.056), train_loss = 3.26092014, grad/param norm = 1.1979e-01, time/batch = 0.2698s	
652/2700 (epoch 12.074), train_loss = 3.29375504, grad/param norm = 1.6010e-01, time/batch = 0.2686s	
653/2700 (epoch 12.093), train_loss = 3.29958514, grad/param norm = 1.7363e-01, time/batch = 0.2693s	
654/2700 (epoch 12.111), train_loss = 3.26909613, grad/param norm = 1.4648e-01, time/batch = 0.2694s	
655/2700 (epoch 12.130), train_loss = 3.28517228, grad/param norm = 1.2381e-01, time/batch = 0.2689s	
656/2700 (epoch 12.148), train_loss = 3.24703620, grad/param norm = 1.4531e-01, time/batch = 0.2694s	
657/2700 (epoch 12.167), train_loss = 3.25958287, grad/param norm = 1.8183e-01, time/batch = 0.2697s	
658/2700 (epoch 12.185), train_loss = 3.24282848, grad/param norm = 1.2361e-01, time/batch = 0.2694s	
659/2700 (epoch 12.204), train_loss = 3.17679855, grad/param norm = 1.3582e-01, time/batch = 0.2699s	
660/2700 (epoch 12.222), train_loss = 3.15492918, grad/param norm = 1.8391e-01, time/batch = 0.2696s	
661/2700 (epoch 12.241), train_loss = 3.17277795, grad/param norm = 1.5062e-01, time/batch = 0.2712s	
662/2700 (epoch 12.259), train_loss = 3.20681522, grad/param norm = 1.5876e-01, time/batch = 0.2691s	
663/2700 (epoch 12.278), train_loss = 3.28163406, grad/param norm = 1.7344e-01, time/batch = 0.2691s	
664/2700 (epoch 12.296), train_loss = 3.28741406, grad/param norm = 2.1248e-01, time/batch = 0.2697s	
665/2700 (epoch 12.315), train_loss = 3.26428509, grad/param norm = 2.1012e-01, time/batch = 0.2689s	
666/2700 (epoch 12.333), train_loss = 3.34293989, grad/param norm = 1.9027e-01, time/batch = 0.2691s	
667/2700 (epoch 12.352), train_loss = 3.34895743, grad/param norm = 1.8167e-01, time/batch = 0.2687s	
668/2700 (epoch 12.370), train_loss = 3.29053957, grad/param norm = 1.5335e-01, time/batch = 0.2685s	
669/2700 (epoch 12.389), train_loss = 3.25431244, grad/param norm = 1.1257e-01, time/batch = 0.2695s	
670/2700 (epoch 12.407), train_loss = 3.27682489, grad/param norm = 1.0593e-01, time/batch = 0.2699s	
671/2700 (epoch 12.426), train_loss = 3.27996593, grad/param norm = 1.1639e-01, time/batch = 0.2707s	
672/2700 (epoch 12.444), train_loss = 3.20748728, grad/param norm = 1.0069e-01, time/batch = 0.2701s	
673/2700 (epoch 12.463), train_loss = 3.25042455, grad/param norm = 1.2019e-01, time/batch = 0.2699s	
674/2700 (epoch 12.481), train_loss = 3.32680443, grad/param norm = 1.1793e-01, time/batch = 0.2703s	
675/2700 (epoch 12.500), train_loss = 3.37489634, grad/param norm = 1.8290e-01, time/batch = 0.2706s	
676/2700 (epoch 12.519), train_loss = 3.32896163, grad/param norm = 1.8592e-01, time/batch = 0.2702s	
677/2700 (epoch 12.537), train_loss = 3.33190415, grad/param norm = 1.8948e-01, time/batch = 0.2700s	
678/2700 (epoch 12.556), train_loss = 3.26737624, grad/param norm = 1.5269e-01, time/batch = 0.2693s	
679/2700 (epoch 12.574), train_loss = 3.23307752, grad/param norm = 1.4136e-01, time/batch = 0.2702s	
680/2700 (epoch 12.593), train_loss = 3.23482540, grad/param norm = 1.7436e-01, time/batch = 0.2699s	
681/2700 (epoch 12.611), train_loss = 3.17388526, grad/param norm = 1.2605e-01, time/batch = 0.2702s	
682/2700 (epoch 12.630), train_loss = 3.21512994, grad/param norm = 1.4483e-01, time/batch = 0.2708s	
683/2700 (epoch 12.648), train_loss = 3.28714618, grad/param norm = 1.5720e-01, time/batch = 0.2695s	
684/2700 (epoch 12.667), train_loss = 3.21894227, grad/param norm = 1.4361e-01, time/batch = 0.2696s	
685/2700 (epoch 12.685), train_loss = 3.21393240, grad/param norm = 1.3129e-01, time/batch = 0.2691s	
686/2700 (epoch 12.704), train_loss = 3.18708191, grad/param norm = 1.6159e-01, time/batch = 0.2693s	
687/2700 (epoch 12.722), train_loss = 3.17801182, grad/param norm = 1.2119e-01, time/batch = 0.2687s	
688/2700 (epoch 12.741), train_loss = 3.30957402, grad/param norm = 1.1909e-01, time/batch = 0.2688s	
689/2700 (epoch 12.759), train_loss = 3.26051766, grad/param norm = 1.6339e-01, time/batch = 0.2689s	
690/2700 (epoch 12.778), train_loss = 3.25496462, grad/param norm = 1.8606e-01, time/batch = 0.2695s	
691/2700 (epoch 12.796), train_loss = 3.24943465, grad/param norm = 1.9623e-01, time/batch = 0.2702s	
692/2700 (epoch 12.815), train_loss = 3.20100714, grad/param norm = 1.8108e-01, time/batch = 0.2685s	
693/2700 (epoch 12.833), train_loss = 3.23905962, grad/param norm = 1.9823e-01, time/batch = 0.2691s	
694/2700 (epoch 12.852), train_loss = 3.22764196, grad/param norm = 2.0859e-01, time/batch = 0.2693s	
695/2700 (epoch 12.870), train_loss = 3.21952112, grad/param norm = 1.7178e-01, time/batch = 0.2698s	
696/2700 (epoch 12.889), train_loss = 3.25574403, grad/param norm = 1.6239e-01, time/batch = 0.2693s	
697/2700 (epoch 12.907), train_loss = 3.30831750, grad/param norm = 1.8805e-01, time/batch = 0.2688s	
698/2700 (epoch 12.926), train_loss = 3.25803305, grad/param norm = 1.8002e-01, time/batch = 0.2687s	
699/2700 (epoch 12.944), train_loss = 3.26645862, grad/param norm = 1.4971e-01, time/batch = 0.2692s	
700/2700 (epoch 12.963), train_loss = 3.34398152, grad/param norm = 1.4213e-01, time/batch = 0.2691s	
701/2700 (epoch 12.981), train_loss = 3.40485967, grad/param norm = 1.5484e-01, time/batch = 0.2701s	
decayed learning rate by a factor 0.97 to 0.00177058562	
702/2700 (epoch 13.000), train_loss = 3.30489935, grad/param norm = 1.5466e-01, time/batch = 0.2698s	
703/2700 (epoch 13.019), train_loss = 3.24516977, grad/param norm = 1.7090e-01, time/batch = 0.2696s	
704/2700 (epoch 13.037), train_loss = 3.26066854, grad/param norm = 1.4982e-01, time/batch = 0.2692s	
705/2700 (epoch 13.056), train_loss = 3.26068504, grad/param norm = 1.1887e-01, time/batch = 0.2696s	
706/2700 (epoch 13.074), train_loss = 3.29342927, grad/param norm = 1.5936e-01, time/batch = 0.2694s	
707/2700 (epoch 13.093), train_loss = 3.29899337, grad/param norm = 1.7121e-01, time/batch = 0.2695s	
708/2700 (epoch 13.111), train_loss = 3.26852925, grad/param norm = 1.4286e-01, time/batch = 0.2690s	
709/2700 (epoch 13.130), train_loss = 3.28454304, grad/param norm = 1.2076e-01, time/batch = 0.2692s	
710/2700 (epoch 13.148), train_loss = 3.24669328, grad/param norm = 1.4229e-01, time/batch = 0.2691s	
711/2700 (epoch 13.167), train_loss = 3.25881125, grad/param norm = 1.7792e-01, time/batch = 0.2703s	
712/2700 (epoch 13.185), train_loss = 3.24222907, grad/param norm = 1.1915e-01, time/batch = 0.2689s	
713/2700 (epoch 13.204), train_loss = 3.17640203, grad/param norm = 1.3120e-01, time/batch = 0.2700s	
714/2700 (epoch 13.222), train_loss = 3.15428026, grad/param norm = 1.7924e-01, time/batch = 0.2697s	
715/2700 (epoch 13.241), train_loss = 3.17204118, grad/param norm = 1.4297e-01, time/batch = 0.2702s	
716/2700 (epoch 13.259), train_loss = 3.20596420, grad/param norm = 1.4862e-01, time/batch = 0.2696s	
717/2700 (epoch 13.278), train_loss = 3.28075022, grad/param norm = 1.6527e-01, time/batch = 0.2699s	
718/2700 (epoch 13.296), train_loss = 3.28646299, grad/param norm = 2.0513e-01, time/batch = 0.2693s	
719/2700 (epoch 13.315), train_loss = 3.26346530, grad/param norm = 2.0555e-01, time/batch = 0.2689s	
720/2700 (epoch 13.333), train_loss = 3.34251169, grad/param norm = 1.8926e-01, time/batch = 0.2689s	
721/2700 (epoch 13.352), train_loss = 3.34853902, grad/param norm = 1.8152e-01, time/batch = 0.2703s	
722/2700 (epoch 13.370), train_loss = 3.28998742, grad/param norm = 1.5234e-01, time/batch = 0.2691s	
723/2700 (epoch 13.389), train_loss = 3.25401843, grad/param norm = 1.1108e-01, time/batch = 0.2689s	
724/2700 (epoch 13.407), train_loss = 3.27647656, grad/param norm = 1.0479e-01, time/batch = 0.2699s	
725/2700 (epoch 13.426), train_loss = 3.27956716, grad/param norm = 1.1528e-01, time/batch = 0.2699s	
726/2700 (epoch 13.444), train_loss = 3.20735482, grad/param norm = 9.9165e-02, time/batch = 0.2694s	
727/2700 (epoch 13.463), train_loss = 3.25010501, grad/param norm = 1.1837e-01, time/batch = 0.2696s	
728/2700 (epoch 13.481), train_loss = 3.32659857, grad/param norm = 1.1723e-01, time/batch = 0.2692s	
729/2700 (epoch 13.500), train_loss = 3.37452702, grad/param norm = 1.8192e-01, time/batch = 0.2700s	
730/2700 (epoch 13.519), train_loss = 3.32834192, grad/param norm = 1.8239e-01, time/batch = 0.2692s	
731/2700 (epoch 13.537), train_loss = 3.33130035, grad/param norm = 1.8606e-01, time/batch = 0.2704s	
732/2700 (epoch 13.556), train_loss = 3.26675943, grad/param norm = 1.4985e-01, time/batch = 0.2684s	
733/2700 (epoch 13.574), train_loss = 3.23291256, grad/param norm = 1.3922e-01, time/batch = 0.2691s	
734/2700 (epoch 13.593), train_loss = 3.23422884, grad/param norm = 1.7086e-01, time/batch = 0.2697s	
735/2700 (epoch 13.611), train_loss = 3.17337365, grad/param norm = 1.2112e-01, time/batch = 0.2695s	
736/2700 (epoch 13.630), train_loss = 3.21466971, grad/param norm = 1.4059e-01, time/batch = 0.2690s	
737/2700 (epoch 13.648), train_loss = 3.28634877, grad/param norm = 1.5237e-01, time/batch = 0.2690s	
738/2700 (epoch 13.667), train_loss = 3.21819482, grad/param norm = 1.3786e-01, time/batch = 0.2689s	
739/2700 (epoch 13.685), train_loss = 3.21342107, grad/param norm = 1.2711e-01, time/batch = 0.2691s	
740/2700 (epoch 13.704), train_loss = 3.18647119, grad/param norm = 1.5770e-01, time/batch = 0.2691s	
741/2700 (epoch 13.722), train_loss = 3.17739818, grad/param norm = 1.1622e-01, time/batch = 0.2705s	
742/2700 (epoch 13.741), train_loss = 3.30914998, grad/param norm = 1.1724e-01, time/batch = 0.2689s	
743/2700 (epoch 13.759), train_loss = 3.25984855, grad/param norm = 1.6019e-01, time/batch = 0.2690s	
744/2700 (epoch 13.778), train_loss = 3.25395260, grad/param norm = 1.7881e-01, time/batch = 0.2703s	
745/2700 (epoch 13.796), train_loss = 3.24813397, grad/param norm = 1.8702e-01, time/batch = 0.2693s	
746/2700 (epoch 13.815), train_loss = 3.19987866, grad/param norm = 1.7114e-01, time/batch = 0.2698s	
747/2700 (epoch 13.833), train_loss = 3.23820268, grad/param norm = 1.9236e-01, time/batch = 0.2713s	
748/2700 (epoch 13.852), train_loss = 3.22711243, grad/param norm = 2.0709e-01, time/batch = 0.2691s	
749/2700 (epoch 13.870), train_loss = 3.21913025, grad/param norm = 1.7391e-01, time/batch = 0.2695s	
750/2700 (epoch 13.889), train_loss = 3.25582786, grad/param norm = 1.6557e-01, time/batch = 0.2691s	
751/2700 (epoch 13.907), train_loss = 3.30810229, grad/param norm = 1.8849e-01, time/batch = 0.2707s	
752/2700 (epoch 13.926), train_loss = 3.25738250, grad/param norm = 1.7755e-01, time/batch = 0.2687s	
753/2700 (epoch 13.944), train_loss = 3.26583257, grad/param norm = 1.4713e-01, time/batch = 0.2689s	
754/2700 (epoch 13.963), train_loss = 3.34356078, grad/param norm = 1.3948e-01, time/batch = 0.2697s	
755/2700 (epoch 13.981), train_loss = 3.40445840, grad/param norm = 1.5201e-01, time/batch = 0.2691s	
decayed learning rate by a factor 0.97 to 0.0017174680514	
756/2700 (epoch 14.000), train_loss = 3.30422005, grad/param norm = 1.5232e-01, time/batch = 0.2687s	
757/2700 (epoch 14.019), train_loss = 3.24462108, grad/param norm = 1.6894e-01, time/batch = 0.2696s	
758/2700 (epoch 14.037), train_loss = 3.26013553, grad/param norm = 1.4725e-01, time/batch = 0.2688s	
759/2700 (epoch 14.056), train_loss = 3.26046164, grad/param norm = 1.1785e-01, time/batch = 0.2688s	
760/2700 (epoch 14.074), train_loss = 3.29308010, grad/param norm = 1.5817e-01, time/batch = 0.2695s	
761/2700 (epoch 14.093), train_loss = 3.29839172, grad/param norm = 1.6848e-01, time/batch = 0.2701s	
762/2700 (epoch 14.111), train_loss = 3.26798782, grad/param norm = 1.3921e-01, time/batch = 0.2683s	
763/2700 (epoch 14.130), train_loss = 3.28396187, grad/param norm = 1.1800e-01, time/batch = 0.2688s	
764/2700 (epoch 14.148), train_loss = 3.24639808, grad/param norm = 1.3971e-01, time/batch = 0.2695s	
765/2700 (epoch 14.167), train_loss = 3.25810167, grad/param norm = 1.7436e-01, time/batch = 0.2697s	
766/2700 (epoch 14.185), train_loss = 3.24167778, grad/param norm = 1.1509e-01, time/batch = 0.2693s	
767/2700 (epoch 14.204), train_loss = 3.17605208, grad/param norm = 1.2706e-01, time/batch = 0.2689s	
768/2700 (epoch 14.222), train_loss = 3.15367623, grad/param norm = 1.7495e-01, time/batch = 0.2687s	
769/2700 (epoch 14.241), train_loss = 3.17139096, grad/param norm = 1.3610e-01, time/batch = 0.2690s	
770/2700 (epoch 14.259), train_loss = 3.20522372, grad/param norm = 1.3957e-01, time/batch = 0.2695s	
771/2700 (epoch 14.278), train_loss = 3.27995394, grad/param norm = 1.5764e-01, time/batch = 0.2704s	
772/2700 (epoch 14.296), train_loss = 3.28549180, grad/param norm = 1.9724e-01, time/batch = 0.2686s	
773/2700 (epoch 14.315), train_loss = 3.26259678, grad/param norm = 2.0028e-01, time/batch = 0.2699s	
774/2700 (epoch 14.333), train_loss = 3.34208513, grad/param norm = 1.8799e-01, time/batch = 0.2703s	
775/2700 (epoch 14.352), train_loss = 3.34813704, grad/param norm = 1.8135e-01, time/batch = 0.2694s	
776/2700 (epoch 14.370), train_loss = 3.28946041, grad/param norm = 1.5140e-01, time/batch = 0.2690s	
777/2700 (epoch 14.389), train_loss = 3.25374721, grad/param norm = 1.0975e-01, time/batch = 0.2698s	
778/2700 (epoch 14.407), train_loss = 3.27614521, grad/param norm = 1.0377e-01, time/batch = 0.2696s	
779/2700 (epoch 14.426), train_loss = 3.27918733, grad/param norm = 1.1426e-01, time/batch = 0.2697s	
780/2700 (epoch 14.444), train_loss = 3.20724132, grad/param norm = 9.7812e-02, time/batch = 0.2701s	
781/2700 (epoch 14.463), train_loss = 3.24980282, grad/param norm = 1.1667e-01, time/batch = 0.2710s	
782/2700 (epoch 14.481), train_loss = 3.32640355, grad/param norm = 1.1654e-01, time/batch = 0.2693s	
783/2700 (epoch 14.500), train_loss = 3.37412310, grad/param norm = 1.8075e-01, time/batch = 0.2698s	
784/2700 (epoch 14.519), train_loss = 3.32772578, grad/param norm = 1.7883e-01, time/batch = 0.2697s	
785/2700 (epoch 14.537), train_loss = 3.33073870, grad/param norm = 1.8281e-01, time/batch = 0.2706s	
786/2700 (epoch 14.556), train_loss = 3.26617359, grad/param norm = 1.4712e-01, time/batch = 0.2694s	
787/2700 (epoch 14.574), train_loss = 3.23274878, grad/param norm = 1.3719e-01, time/batch = 0.2700s	
788/2700 (epoch 14.593), train_loss = 3.23366410, grad/param norm = 1.6756e-01, time/batch = 0.2697s	
789/2700 (epoch 14.611), train_loss = 3.17292333, grad/param norm = 1.1661e-01, time/batch = 0.2697s	
790/2700 (epoch 14.630), train_loss = 3.21426761, grad/param norm = 1.3682e-01, time/batch = 0.2698s	
791/2700 (epoch 14.648), train_loss = 3.28559761, grad/param norm = 1.4796e-01, time/batch = 0.2715s	
792/2700 (epoch 14.667), train_loss = 3.21751276, grad/param norm = 1.3259e-01, time/batch = 0.2691s	
793/2700 (epoch 14.685), train_loss = 3.21296629, grad/param norm = 1.2343e-01, time/batch = 0.2694s	
794/2700 (epoch 14.704), train_loss = 3.18591877, grad/param norm = 1.5425e-01, time/batch = 0.2700s	
795/2700 (epoch 14.722), train_loss = 3.17683494, grad/param norm = 1.1168e-01, time/batch = 0.2689s	
796/2700 (epoch 14.741), train_loss = 3.30877758, grad/param norm = 1.1610e-01, time/batch = 0.2697s	
797/2700 (epoch 14.759), train_loss = 3.25923825, grad/param norm = 1.5759e-01, time/batch = 0.2693s	
798/2700 (epoch 14.778), train_loss = 3.25300638, grad/param norm = 1.7183e-01, time/batch = 0.2686s	
799/2700 (epoch 14.796), train_loss = 3.24686526, grad/param norm = 1.7745e-01, time/batch = 0.2694s	
800/2700 (epoch 14.815), train_loss = 3.19866429, grad/param norm = 1.5898e-01, time/batch = 0.2689s	
801/2700 (epoch 14.833), train_loss = 3.23712272, grad/param norm = 1.8263e-01, time/batch = 0.2699s	
802/2700 (epoch 14.852), train_loss = 3.22637782, grad/param norm = 2.0335e-01, time/batch = 0.2684s	
803/2700 (epoch 14.870), train_loss = 3.21885703, grad/param norm = 1.7743e-01, time/batch = 0.2688s	
804/2700 (epoch 14.889), train_loss = 3.25609374, grad/param norm = 1.7088e-01, time/batch = 0.2698s	
805/2700 (epoch 14.907), train_loss = 3.30796409, grad/param norm = 1.9032e-01, time/batch = 0.2695s	
806/2700 (epoch 14.926), train_loss = 3.25683806, grad/param norm = 1.7619e-01, time/batch = 0.2691s	
807/2700 (epoch 14.944), train_loss = 3.26526722, grad/param norm = 1.4498e-01, time/batch = 0.2688s	
808/2700 (epoch 14.963), train_loss = 3.34316272, grad/param norm = 1.3698e-01, time/batch = 0.2694s	
809/2700 (epoch 14.981), train_loss = 3.40408085, grad/param norm = 1.4930e-01, time/batch = 0.2697s	
decayed learning rate by a factor 0.97 to 0.001665944009858	
810/2700 (epoch 15.000), train_loss = 3.30357227, grad/param norm = 1.5008e-01, time/batch = 0.2690s	
811/2700 (epoch 15.019), train_loss = 3.24409055, grad/param norm = 1.6714e-01, time/batch = 0.2699s	
812/2700 (epoch 15.037), train_loss = 3.25962453, grad/param norm = 1.4480e-01, time/batch = 0.2703s	
813/2700 (epoch 15.056), train_loss = 3.26024867, grad/param norm = 1.1673e-01, time/batch = 0.2694s	
814/2700 (epoch 15.074), train_loss = 3.29270895, grad/param norm = 1.5658e-01, time/batch = 0.2694s	
815/2700 (epoch 15.093), train_loss = 3.29778648, grad/param norm = 1.6550e-01, time/batch = 0.2692s	
816/2700 (epoch 15.111), train_loss = 3.26747393, grad/param norm = 1.3557e-01, time/batch = 0.2690s	
817/2700 (epoch 15.130), train_loss = 3.28342787, grad/param norm = 1.1556e-01, time/batch = 0.2692s	
818/2700 (epoch 15.148), train_loss = 3.24614654, grad/param norm = 1.3756e-01, time/batch = 0.2686s	
819/2700 (epoch 15.167), train_loss = 3.25744845, grad/param norm = 1.7111e-01, time/batch = 0.2691s	
820/2700 (epoch 15.185), train_loss = 3.24117076, grad/param norm = 1.1140e-01, time/batch = 0.2695s	
821/2700 (epoch 15.204), train_loss = 3.17574376, grad/param norm = 1.2335e-01, time/batch = 0.2700s	
822/2700 (epoch 15.222), train_loss = 3.15311524, grad/param norm = 1.7101e-01, time/batch = 0.2691s	
823/2700 (epoch 15.241), train_loss = 3.17081592, grad/param norm = 1.2994e-01, time/batch = 0.2690s	
824/2700 (epoch 15.259), train_loss = 3.20457951, grad/param norm = 1.3159e-01, time/batch = 0.2691s	
825/2700 (epoch 15.278), train_loss = 3.27923781, grad/param norm = 1.5058e-01, time/batch = 0.2691s	
826/2700 (epoch 15.296), train_loss = 3.28451509, grad/param norm = 1.8893e-01, time/batch = 0.2693s	
827/2700 (epoch 15.315), train_loss = 3.26168559, grad/param norm = 1.9430e-01, time/batch = 0.2688s	
828/2700 (epoch 15.333), train_loss = 3.34165209, grad/param norm = 1.8631e-01, time/batch = 0.2684s	
829/2700 (epoch 15.352), train_loss = 3.34774538, grad/param norm = 1.8111e-01, time/batch = 0.2692s	
830/2700 (epoch 15.370), train_loss = 3.28895724, grad/param norm = 1.5052e-01, time/batch = 0.2689s	
831/2700 (epoch 15.389), train_loss = 3.25349900, grad/param norm = 1.0856e-01, time/batch = 0.2700s	
832/2700 (epoch 15.407), train_loss = 3.27583137, grad/param norm = 1.0286e-01, time/batch = 0.2688s	
833/2700 (epoch 15.426), train_loss = 3.27882511, grad/param norm = 1.1334e-01, time/batch = 0.2692s	
834/2700 (epoch 15.444), train_loss = 3.20714461, grad/param norm = 9.6621e-02, time/batch = 0.2699s	
835/2700 (epoch 15.463), train_loss = 3.24951811, grad/param norm = 1.1506e-01, time/batch = 0.2691s	
836/2700 (epoch 15.481), train_loss = 3.32621799, grad/param norm = 1.1581e-01, time/batch = 0.2689s	
837/2700 (epoch 15.500), train_loss = 3.37369007, grad/param norm = 1.7938e-01, time/batch = 0.2690s	
838/2700 (epoch 15.519), train_loss = 3.32712163, grad/param norm = 1.7528e-01, time/batch = 0.2686s	
839/2700 (epoch 15.537), train_loss = 3.33021576, grad/param norm = 1.7975e-01, time/batch = 0.2697s	
840/2700 (epoch 15.556), train_loss = 3.26561847, grad/param norm = 1.4452e-01, time/batch = 0.2696s	
841/2700 (epoch 15.574), train_loss = 3.23259007, grad/param norm = 1.3529e-01, time/batch = 0.2698s	
842/2700 (epoch 15.593), train_loss = 3.23312763, grad/param norm = 1.6447e-01, time/batch = 0.2693s	
843/2700 (epoch 15.611), train_loss = 3.17252582, grad/param norm = 1.1252e-01, time/batch = 0.2690s	
844/2700 (epoch 15.630), train_loss = 3.21391507, grad/param norm = 1.3348e-01, time/batch = 0.2697s	
845/2700 (epoch 15.648), train_loss = 3.28488335, grad/param norm = 1.4395e-01, time/batch = 0.2701s	
846/2700 (epoch 15.667), train_loss = 3.21689141, grad/param norm = 1.2779e-01, time/batch = 0.2697s	
847/2700 (epoch 15.685), train_loss = 3.21256279, grad/param norm = 1.2022e-01, time/batch = 0.2689s	
848/2700 (epoch 15.704), train_loss = 3.18541865, grad/param norm = 1.5122e-01, time/batch = 0.2684s	
849/2700 (epoch 15.722), train_loss = 3.17632050, grad/param norm = 1.0762e-01, time/batch = 0.2696s	
850/2700 (epoch 15.741), train_loss = 3.30845937, grad/param norm = 1.1573e-01, time/batch = 0.2691s	
851/2700 (epoch 15.759), train_loss = 3.25869525, grad/param norm = 1.5575e-01, time/batch = 0.2708s	
852/2700 (epoch 15.778), train_loss = 3.25215112, grad/param norm = 1.6554e-01, time/batch = 0.2693s	
853/2700 (epoch 15.796), train_loss = 3.24568079, grad/param norm = 1.6833e-01, time/batch = 0.2691s	
854/2700 (epoch 15.815), train_loss = 3.19744689, grad/param norm = 1.4531e-01, time/batch = 0.2695s	
855/2700 (epoch 15.833), train_loss = 3.23577897, grad/param norm = 1.6783e-01, time/batch = 0.2698s	
856/2700 (epoch 15.852), train_loss = 3.22523114, grad/param norm = 1.9499e-01, time/batch = 0.2695s	
857/2700 (epoch 15.870), train_loss = 3.21862670, grad/param norm = 1.8114e-01, time/batch = 0.2693s	
858/2700 (epoch 15.889), train_loss = 3.25655980, grad/param norm = 1.7832e-01, time/batch = 0.2687s	
859/2700 (epoch 15.907), train_loss = 3.30793533, grad/param norm = 1.9402e-01, time/batch = 0.2695s	
860/2700 (epoch 15.926), train_loss = 3.25642937, grad/param norm = 1.7632e-01, time/batch = 0.2688s	
861/2700 (epoch 15.944), train_loss = 3.26476243, grad/param norm = 1.4336e-01, time/batch = 0.2706s	
862/2700 (epoch 15.963), train_loss = 3.34278794, grad/param norm = 1.3465e-01, time/batch = 0.2689s	
863/2700 (epoch 15.981), train_loss = 3.40373149, grad/param norm = 1.4672e-01, time/batch = 0.2692s	
decayed learning rate by a factor 0.97 to 0.0016159656895623	
864/2700 (epoch 16.000), train_loss = 3.30295939, grad/param norm = 1.4796e-01, time/batch = 0.2700s	
865/2700 (epoch 16.019), train_loss = 3.24358109, grad/param norm = 1.6554e-01, time/batch = 0.2699s	
866/2700 (epoch 16.037), train_loss = 3.25913498, grad/param norm = 1.4248e-01, time/batch = 0.2698s	
867/2700 (epoch 16.056), train_loss = 3.26004299, grad/param norm = 1.1552e-01, time/batch = 0.2694s	
868/2700 (epoch 16.074), train_loss = 3.29231988, grad/param norm = 1.5464e-01, time/batch = 0.2695s	
869/2700 (epoch 16.093), train_loss = 3.29718147, grad/param norm = 1.6233e-01, time/batch = 0.2696s	
870/2700 (epoch 16.111), train_loss = 3.26698874, grad/param norm = 1.3200e-01, time/batch = 0.2692s	
871/2700 (epoch 16.130), train_loss = 3.28294046, grad/param norm = 1.1348e-01, time/batch = 0.2705s	
872/2700 (epoch 16.148), train_loss = 3.24593441, grad/param norm = 1.3580e-01, time/batch = 0.2691s	
873/2700 (epoch 16.167), train_loss = 3.25684633, grad/param norm = 1.6816e-01, time/batch = 0.2689s	
874/2700 (epoch 16.185), train_loss = 3.24070574, grad/param norm = 1.0807e-01, time/batch = 0.2695s	
875/2700 (epoch 16.204), train_loss = 3.17547272, grad/param norm = 1.2008e-01, time/batch = 0.2694s	
876/2700 (epoch 16.222), train_loss = 3.15259510, grad/param norm = 1.6741e-01, time/batch = 0.2690s	
877/2700 (epoch 16.241), train_loss = 3.17031468, grad/param norm = 1.2452e-01, time/batch = 0.2693s	
878/2700 (epoch 16.259), train_loss = 3.20402622, grad/param norm = 1.2478e-01, time/batch = 0.2690s	
879/2700 (epoch 16.278), train_loss = 3.27860102, grad/param norm = 1.4423e-01, time/batch = 0.2692s	
880/2700 (epoch 16.296), train_loss = 3.28355636, grad/param norm = 1.8046e-01, time/batch = 0.2690s	
881/2700 (epoch 16.315), train_loss = 3.26075010, grad/param norm = 1.8768e-01, time/batch = 0.2702s	
882/2700 (epoch 16.333), train_loss = 3.34120128, grad/param norm = 1.8405e-01, time/batch = 0.2690s	
883/2700 (epoch 16.352), train_loss = 3.34734645, grad/param norm = 1.8062e-01, time/batch = 0.2694s	
884/2700 (epoch 16.370), train_loss = 3.28847358, grad/param norm = 1.4964e-01, time/batch = 0.2692s	
885/2700 (epoch 16.389), train_loss = 3.25326989, grad/param norm = 1.0750e-01, time/batch = 0.2698s	
886/2700 (epoch 16.407), train_loss = 3.27553470, grad/param norm = 1.0206e-01, time/batch = 0.2691s	
887/2700 (epoch 16.426), train_loss = 3.27847925, grad/param norm = 1.1251e-01, time/batch = 0.2695s	
888/2700 (epoch 16.444), train_loss = 3.20706356, grad/param norm = 9.5584e-02, time/batch = 0.2690s	
889/2700 (epoch 16.463), train_loss = 3.24924674, grad/param norm = 1.1354e-01, time/batch = 0.2693s	
890/2700 (epoch 16.481), train_loss = 3.32603956, grad/param norm = 1.1508e-01, time/batch = 0.2695s	
891/2700 (epoch 16.500), train_loss = 3.37323352, grad/param norm = 1.7788e-01, time/batch = 0.2700s	
892/2700 (epoch 16.519), train_loss = 3.32653629, grad/param norm = 1.7178e-01, time/batch = 0.2687s	
893/2700 (epoch 16.537), train_loss = 3.32973080, grad/param norm = 1.7688e-01, time/batch = 0.2689s	
894/2700 (epoch 16.556), train_loss = 3.26509311, grad/param norm = 1.4205e-01, time/batch = 0.2695s	
895/2700 (epoch 16.574), train_loss = 3.23243586, grad/param norm = 1.3349e-01, time/batch = 0.2698s	
896/2700 (epoch 16.593), train_loss = 3.23261667, grad/param norm = 1.6153e-01, time/batch = 0.2691s	
897/2700 (epoch 16.611), train_loss = 3.17217528, grad/param norm = 1.0878e-01, time/batch = 0.2699s	
898/2700 (epoch 16.630), train_loss = 3.21360434, grad/param norm = 1.3049e-01, time/batch = 0.2693s	
899/2700 (epoch 16.648), train_loss = 3.28420126, grad/param norm = 1.4026e-01, time/batch = 0.2690s	
900/2700 (epoch 16.667), train_loss = 3.21632247, grad/param norm = 1.2340e-01, time/batch = 0.2689s	
901/2700 (epoch 16.685), train_loss = 3.21220458, grad/param norm = 1.1743e-01, time/batch = 0.2698s	
902/2700 (epoch 16.704), train_loss = 3.18496434, grad/param norm = 1.4856e-01, time/batch = 0.2686s	
903/2700 (epoch 16.722), train_loss = 3.17585160, grad/param norm = 1.0404e-01, time/batch = 0.2694s	
904/2700 (epoch 16.741), train_loss = 3.30819247, grad/param norm = 1.1614e-01, time/batch = 0.2700s	
905/2700 (epoch 16.759), train_loss = 3.25822434, grad/param norm = 1.5471e-01, time/batch = 0.2691s	
906/2700 (epoch 16.778), train_loss = 3.25139897, grad/param norm = 1.6025e-01, time/batch = 0.2697s	
907/2700 (epoch 16.796), train_loss = 3.24463257, grad/param norm = 1.6047e-01, time/batch = 0.2693s	
908/2700 (epoch 16.815), train_loss = 3.19634066, grad/param norm = 1.3168e-01, time/batch = 0.2687s	
909/2700 (epoch 16.833), train_loss = 3.23427242, grad/param norm = 1.4856e-01, time/batch = 0.2702s	
910/2700 (epoch 16.852), train_loss = 3.22351751, grad/param norm = 1.7933e-01, time/batch = 0.2710s	
911/2700 (epoch 16.870), train_loss = 3.21816865, grad/param norm = 1.8074e-01, time/batch = 0.2710s	
912/2700 (epoch 16.889), train_loss = 3.25709261, grad/param norm = 1.8612e-01, time/batch = 0.2688s	
913/2700 (epoch 16.907), train_loss = 3.30806679, grad/param norm = 2.0024e-01, time/batch = 0.2701s	
914/2700 (epoch 16.926), train_loss = 3.25622561, grad/param norm = 1.7880e-01, time/batch = 0.2695s	
915/2700 (epoch 16.944), train_loss = 3.26433385, grad/param norm = 1.4261e-01, time/batch = 0.2692s	
916/2700 (epoch 16.963), train_loss = 3.34243989, grad/param norm = 1.3263e-01, time/batch = 0.2695s	
917/2700 (epoch 16.981), train_loss = 3.40341062, grad/param norm = 1.4431e-01, time/batch = 0.2689s	
decayed learning rate by a factor 0.97 to 0.0015674867188754	
918/2700 (epoch 17.000), train_loss = 3.30238461, grad/param norm = 1.4603e-01, time/batch = 0.2691s	
919/2700 (epoch 17.019), train_loss = 3.24310293, grad/param norm = 1.6422e-01, time/batch = 0.2692s	
920/2700 (epoch 17.037), train_loss = 3.25867288, grad/param norm = 1.4037e-01, time/batch = 0.2695s	
921/2700 (epoch 17.056), train_loss = 3.25983855, grad/param norm = 1.1410e-01, time/batch = 0.2700s	
922/2700 (epoch 17.074), train_loss = 3.29191312, grad/param norm = 1.5230e-01, time/batch = 0.2684s	
923/2700 (epoch 17.093), train_loss = 3.29658566, grad/param norm = 1.5909e-01, time/batch = 0.2690s	
924/2700 (epoch 17.111), train_loss = 3.26654043, grad/param norm = 1.2867e-01, time/batch = 0.2700s	
925/2700 (epoch 17.130), train_loss = 3.28250152, grad/param norm = 1.1186e-01, time/batch = 0.2691s	
926/2700 (epoch 17.148), train_loss = 3.24576067, grad/param norm = 1.3443e-01, time/batch = 0.2694s	
927/2700 (epoch 17.167), train_loss = 3.25629219, grad/param norm = 1.6549e-01, time/batch = 0.2694s	
928/2700 (epoch 17.185), train_loss = 3.24028142, grad/param norm = 1.0512e-01, time/batch = 0.2691s	
929/2700 (epoch 17.204), train_loss = 3.17523737, grad/param norm = 1.1728e-01, time/batch = 0.2693s	
930/2700 (epoch 17.222), train_loss = 3.15212145, grad/param norm = 1.6417e-01, time/batch = 0.2695s	
931/2700 (epoch 17.241), train_loss = 3.16989148, grad/param norm = 1.2006e-01, time/batch = 0.2700s	
932/2700 (epoch 17.259), train_loss = 3.20357635, grad/param norm = 1.1967e-01, time/batch = 0.2686s	
933/2700 (epoch 17.278), train_loss = 3.27806343, grad/param norm = 1.3907e-01, time/batch = 0.2694s	
934/2700 (epoch 17.296), train_loss = 3.28268277, grad/param norm = 1.7275e-01, time/batch = 0.2697s	
935/2700 (epoch 17.315), train_loss = 3.25985092, grad/param norm = 1.8107e-01, time/batch = 0.2696s	
936/2700 (epoch 17.333), train_loss = 3.34072663, grad/param norm = 1.8104e-01, time/batch = 0.2690s	
937/2700 (epoch 17.352), train_loss = 3.34690319, grad/param norm = 1.7943e-01, time/batch = 0.2700s	
938/2700 (epoch 17.370), train_loss = 3.28798696, grad/param norm = 1.4847e-01, time/batch = 0.2693s	
939/2700 (epoch 17.389), train_loss = 3.25304987, grad/param norm = 1.0632e-01, time/batch = 0.2692s	
940/2700 (epoch 17.407), train_loss = 3.27525137, grad/param norm = 1.0129e-01, time/batch = 0.2698s	
941/2700 (epoch 17.426), train_loss = 3.27814511, grad/param norm = 1.1163e-01, time/batch = 0.2708s	
942/2700 (epoch 17.444), train_loss = 3.20699589, grad/param norm = 9.4676e-02, time/batch = 0.2698s	
943/2700 (epoch 17.463), train_loss = 3.24899284, grad/param norm = 1.1221e-01, time/batch = 0.2699s	
944/2700 (epoch 17.481), train_loss = 3.32587966, grad/param norm = 1.1459e-01, time/batch = 0.2701s	
945/2700 (epoch 17.500), train_loss = 3.37277459, grad/param norm = 1.7651e-01, time/batch = 0.2695s	
946/2700 (epoch 17.519), train_loss = 3.32598897, grad/param norm = 1.6856e-01, time/batch = 0.2698s	
947/2700 (epoch 17.537), train_loss = 3.32927808, grad/param norm = 1.7418e-01, time/batch = 0.2705s	
948/2700 (epoch 17.556), train_loss = 3.26458419, grad/param norm = 1.3952e-01, time/batch = 0.2695s	
949/2700 (epoch 17.574), train_loss = 3.23228226, grad/param norm = 1.3167e-01, time/batch = 0.2697s	
950/2700 (epoch 17.593), train_loss = 3.23212150, grad/param norm = 1.5860e-01, time/batch = 0.2702s	
951/2700 (epoch 17.611), train_loss = 3.17185845, grad/param norm = 1.0518e-01, time/batch = 0.2707s	
952/2700 (epoch 17.630), train_loss = 3.21331269, grad/param norm = 1.2753e-01, time/batch = 0.2698s	
953/2700 (epoch 17.648), train_loss = 3.28591981, grad/param norm = 1.4719e-01, time/batch = 0.2695s	
954/2700 (epoch 17.667), train_loss = 3.21616919, grad/param norm = 1.2656e-01, time/batch = 0.2710s	
955/2700 (epoch 17.685), train_loss = 3.21206255, grad/param norm = 1.1697e-01, time/batch = 0.2709s	
956/2700 (epoch 17.704), train_loss = 3.18440173, grad/param norm = 1.4497e-01, time/batch = 0.2698s	
957/2700 (epoch 17.722), train_loss = 3.17551766, grad/param norm = 1.0142e-01, time/batch = 0.2700s	
958/2700 (epoch 17.741), train_loss = 3.30816983, grad/param norm = 1.2036e-01, time/batch = 0.2692s	
959/2700 (epoch 17.759), train_loss = 3.25792458, grad/param norm = 1.5703e-01, time/batch = 0.2693s	
960/2700 (epoch 17.778), train_loss = 3.25084070, grad/param norm = 1.5635e-01, time/batch = 0.2698s	
961/2700 (epoch 17.796), train_loss = 3.24315693, grad/param norm = 1.5001e-01, time/batch = 0.2712s	
962/2700 (epoch 17.815), train_loss = 3.19504347, grad/param norm = 1.1317e-01, time/batch = 0.2689s	
963/2700 (epoch 17.833), train_loss = 3.23206161, grad/param norm = 1.1442e-01, time/batch = 0.2700s	
964/2700 (epoch 17.852), train_loss = 3.21989229, grad/param norm = 1.3237e-01, time/batch = 0.2701s	
965/2700 (epoch 17.870), train_loss = 3.21527681, grad/param norm = 1.3468e-01, time/batch = 0.2700s	
966/2700 (epoch 17.889), train_loss = 3.25580210, grad/param norm = 1.7368e-01, time/batch = 0.2698s	
967/2700 (epoch 17.907), train_loss = 3.30940722, grad/param norm = 2.2452e-01, time/batch = 0.2697s	
968/2700 (epoch 17.926), train_loss = 3.25807654, grad/param norm = 2.0473e-01, time/batch = 0.2690s	
969/2700 (epoch 17.944), train_loss = 3.26457479, grad/param norm = 1.5336e-01, time/batch = 0.2694s	
970/2700 (epoch 17.963), train_loss = 3.34228188, grad/param norm = 1.3535e-01, time/batch = 0.2695s	
971/2700 (epoch 17.981), train_loss = 3.40303144, grad/param norm = 1.4249e-01, time/batch = 0.2706s	
decayed learning rate by a factor 0.97 to 0.0015204621173091	
972/2700 (epoch 18.000), train_loss = 3.30186613, grad/param norm = 1.4585e-01, time/batch = 0.2691s	
973/2700 (epoch 18.019), train_loss = 3.24297260, grad/param norm = 1.6632e-01, time/batch = 0.2696s	
974/2700 (epoch 18.037), train_loss = 3.25837383, grad/param norm = 1.4053e-01, time/batch = 0.2701s	
975/2700 (epoch 18.056), train_loss = 3.25944739, grad/param norm = 1.0902e-01, time/batch = 0.2719s	
976/2700 (epoch 18.074), train_loss = 3.29123668, grad/param norm = 1.4469e-01, time/batch = 0.2697s	
977/2700 (epoch 18.093), train_loss = 3.29581557, grad/param norm = 1.5302e-01, time/batch = 0.2693s	
978/2700 (epoch 18.111), train_loss = 3.26611767, grad/param norm = 1.2521e-01, time/batch = 0.2690s	
979/2700 (epoch 18.130), train_loss = 3.28218835, grad/param norm = 1.1145e-01, time/batch = 0.2695s	
980/2700 (epoch 18.148), train_loss = 3.24571157, grad/param norm = 1.3424e-01, time/batch = 0.2696s	
981/2700 (epoch 18.167), train_loss = 3.25574474, grad/param norm = 1.6286e-01, time/batch = 0.2698s	
982/2700 (epoch 18.185), train_loss = 3.23988771, grad/param norm = 1.0222e-01, time/batch = 0.2692s	
983/2700 (epoch 18.204), train_loss = 3.17499795, grad/param norm = 1.1466e-01, time/batch = 0.2695s	
984/2700 (epoch 18.222), train_loss = 3.15163661, grad/param norm = 1.6062e-01, time/batch = 0.2696s	
985/2700 (epoch 18.241), train_loss = 3.16955664, grad/param norm = 1.1765e-01, time/batch = 0.2693s	
986/2700 (epoch 18.259), train_loss = 3.20340123, grad/param norm = 1.2073e-01, time/batch = 0.2697s	
987/2700 (epoch 18.278), train_loss = 3.27785725, grad/param norm = 1.3938e-01, time/batch = 0.2697s	
988/2700 (epoch 18.296), train_loss = 3.28239594, grad/param norm = 1.7304e-01, time/batch = 0.2689s	
989/2700 (epoch 18.315), train_loss = 3.25950559, grad/param norm = 1.8070e-01, time/batch = 0.2693s	
990/2700 (epoch 18.333), train_loss = 3.34033991, grad/param norm = 1.7885e-01, time/batch = 0.2695s	
991/2700 (epoch 18.352), train_loss = 3.34629170, grad/param norm = 1.7588e-01, time/batch = 0.2704s	
992/2700 (epoch 18.370), train_loss = 3.28735828, grad/param norm = 1.4502e-01, time/batch = 0.2691s	
993/2700 (epoch 18.389), train_loss = 3.25276458, grad/param norm = 1.0324e-01, time/batch = 0.2695s	
994/2700 (epoch 18.407), train_loss = 3.27496817, grad/param norm = 1.0004e-01, time/batch = 0.2696s	
995/2700 (epoch 18.426), train_loss = 3.27777910, grad/param norm = 1.0962e-01, time/batch = 0.2694s	
996/2700 (epoch 18.444), train_loss = 3.20693101, grad/param norm = 9.3801e-02, time/batch = 0.2695s	
997/2700 (epoch 18.463), train_loss = 3.24881093, grad/param norm = 1.1232e-01, time/batch = 0.2693s	
998/2700 (epoch 18.481), train_loss = 3.32584199, grad/param norm = 1.1668e-01, time/batch = 0.2700s	
999/2700 (epoch 18.500), train_loss = 3.37243193, grad/param norm = 1.7727e-01, time/batch = 0.2699s	
evaluating loss over split index 2	
1/3...	
2/3...	
3/3...	
saving checkpoint to cv/lm_lstm_epoch18.52_3.2203.t7	
1000/2700 (epoch 18.519), train_loss = 3.32553921, grad/param norm = 1.6663e-01, time/batch = 0.2696s	
1001/2700 (epoch 18.537), train_loss = 3.32876676, grad/param norm = 1.7045e-01, time/batch = 0.2709s	
1002/2700 (epoch 18.556), train_loss = 3.26394991, grad/param norm = 1.3461e-01, time/batch = 0.2694s	
1003/2700 (epoch 18.574), train_loss = 3.23206907, grad/param norm = 1.2838e-01, time/batch = 0.2693s	
1004/2700 (epoch 18.593), train_loss = 3.23158079, grad/param norm = 1.5431e-01, time/batch = 0.2702s	
1005/2700 (epoch 18.611), train_loss = 3.17150484, grad/param norm = 1.0021e-01, time/batch = 0.2706s	
1006/2700 (epoch 18.630), train_loss = 3.21292654, grad/param norm = 1.2228e-01, time/batch = 0.2693s	
1007/2700 (epoch 18.648), train_loss = 3.28273852, grad/param norm = 1.3053e-01, time/batch = 0.2696s	
1008/2700 (epoch 18.667), train_loss = 3.21516248, grad/param norm = 1.1180e-01, time/batch = 0.2687s	
1009/2700 (epoch 18.685), train_loss = 3.21151662, grad/param norm = 1.1150e-01, time/batch = 0.2691s	
1010/2700 (epoch 18.704), train_loss = 3.18416232, grad/param norm = 1.4358e-01, time/batch = 0.2691s	
1011/2700 (epoch 18.722), train_loss = 3.17501102, grad/param norm = 9.9059e-02, time/batch = 0.2690s	
1012/2700 (epoch 18.741), train_loss = 3.30818680, grad/param norm = 1.2841e-01, time/batch = 0.2685s	
1013/2700 (epoch 18.759), train_loss = 3.25795130, grad/param norm = 1.6247e-01, time/batch = 0.2691s	
1014/2700 (epoch 18.778), train_loss = 3.25041451, grad/param norm = 1.5585e-01, time/batch = 0.2695s	
1015/2700 (epoch 18.796), train_loss = 3.24308771, grad/param norm = 1.5238e-01, time/batch = 0.2692s	
1016/2700 (epoch 18.815), train_loss = 3.19484724, grad/param norm = 1.1251e-01, time/batch = 0.2688s	
1017/2700 (epoch 18.833), train_loss = 3.23140584, grad/param norm = 1.0650e-01, time/batch = 0.2690s	
1018/2700 (epoch 18.852), train_loss = 3.21847558, grad/param norm = 1.0924e-01, time/batch = 0.2689s	
1019/2700 (epoch 18.870), train_loss = 3.21259650, grad/param norm = 7.7326e-02, time/batch = 0.2694s	
1020/2700 (epoch 18.889), train_loss = 3.25117796, grad/param norm = 1.0135e-01, time/batch = 0.2698s	
1021/2700 (epoch 18.907), train_loss = 3.30520724, grad/param norm = 1.6470e-01, time/batch = 0.2692s	
1022/2700 (epoch 18.926), train_loss = 3.25559713, grad/param norm = 1.8139e-01, time/batch = 0.2688s	
1023/2700 (epoch 18.944), train_loss = 3.26550810, grad/param norm = 1.7689e-01, time/batch = 0.2691s	
1024/2700 (epoch 18.963), train_loss = 3.34456389, grad/param norm = 1.7146e-01, time/batch = 0.2700s	
1025/2700 (epoch 18.981), train_loss = 3.40400396, grad/param norm = 1.6669e-01, time/batch = 0.2701s	
decayed learning rate by a factor 0.97 to 0.0014748482537899	
1026/2700 (epoch 19.000), train_loss = 3.30299044, grad/param norm = 1.7287e-01, time/batch = 0.2690s	
1027/2700 (epoch 19.019), train_loss = 3.24468908, grad/param norm = 1.9165e-01, time/batch = 0.2694s	
1028/2700 (epoch 19.037), train_loss = 3.25905440, grad/param norm = 1.5863e-01, time/batch = 0.2689s	
1029/2700 (epoch 19.056), train_loss = 3.25890207, grad/param norm = 1.0159e-01, time/batch = 0.2690s	
1030/2700 (epoch 19.074), train_loss = 3.28991898, grad/param norm = 1.2380e-01, time/batch = 0.2690s	
1031/2700 (epoch 19.093), train_loss = 3.29470068, grad/param norm = 1.3784e-01, time/batch = 0.2687s	
1032/2700 (epoch 19.111), train_loss = 3.26579379, grad/param norm = 1.1947e-01, time/batch = 0.2684s	
1033/2700 (epoch 19.130), train_loss = 3.28208262, grad/param norm = 1.1245e-01, time/batch = 0.2687s	
1034/2700 (epoch 19.148), train_loss = 3.24582331, grad/param norm = 1.3679e-01, time/batch = 0.2694s	
1035/2700 (epoch 19.167), train_loss = 3.25516072, grad/param norm = 1.5872e-01, time/batch = 0.2689s	
1036/2700 (epoch 19.185), train_loss = 3.23971336, grad/param norm = 1.0219e-01, time/batch = 0.2692s	
1037/2700 (epoch 19.204), train_loss = 3.17482546, grad/param norm = 1.0971e-01, time/batch = 0.2689s	
1038/2700 (epoch 19.222), train_loss = 3.15089878, grad/param norm = 1.5623e-01, time/batch = 0.2691s	
1039/2700 (epoch 19.241), train_loss = 3.16862287, grad/param norm = 1.0209e-01, time/batch = 0.2709s	
1040/2700 (epoch 19.259), train_loss = 3.20223956, grad/param norm = 9.3676e-02, time/batch = 0.2697s	
1041/2700 (epoch 19.278), train_loss = 3.27678640, grad/param norm = 1.2098e-01, time/batch = 0.2692s	
1042/2700 (epoch 19.296), train_loss = 3.27975559, grad/param norm = 1.3410e-01, time/batch = 0.2688s	
1043/2700 (epoch 19.315), train_loss = 3.25604626, grad/param norm = 1.3351e-01, time/batch = 0.2696s	
1044/2700 (epoch 19.333), train_loss = 3.33817304, grad/param norm = 1.4734e-01, time/batch = 0.2697s	
1045/2700 (epoch 19.352), train_loss = 3.34620662, grad/param norm = 1.7945e-01, time/batch = 0.2696s	
1046/2700 (epoch 19.370), train_loss = 3.28876949, grad/param norm = 1.6995e-01, time/batch = 0.2691s	
1047/2700 (epoch 19.389), train_loss = 3.25438442, grad/param norm = 1.3634e-01, time/batch = 0.2696s	
1048/2700 (epoch 19.407), train_loss = 3.27562343, grad/param norm = 1.2330e-01, time/batch = 0.2687s	
1049/2700 (epoch 19.426), train_loss = 3.27880322, grad/param norm = 1.3733e-01, time/batch = 0.2698s	
1050/2700 (epoch 19.444), train_loss = 3.20770110, grad/param norm = 1.1225e-01, time/batch = 0.2694s	
1051/2700 (epoch 19.463), train_loss = 3.24871331, grad/param norm = 1.1498e-01, time/batch = 0.2687s	
1052/2700 (epoch 19.481), train_loss = 3.32536217, grad/param norm = 1.0649e-01, time/batch = 0.2692s	
1053/2700 (epoch 19.500), train_loss = 3.37064318, grad/param norm = 1.5472e-01, time/batch = 0.2693s	
1054/2700 (epoch 19.519), train_loss = 3.32312200, grad/param norm = 1.3058e-01, time/batch = 0.2693s	
1055/2700 (epoch 19.537), train_loss = 3.32705227, grad/param norm = 1.4386e-01, time/batch = 0.2696s	
1056/2700 (epoch 19.556), train_loss = 3.26249603, grad/param norm = 1.1136e-01, time/batch = 0.2691s	
1057/2700 (epoch 19.574), train_loss = 3.23164107, grad/param norm = 1.2210e-01, time/batch = 0.2693s	
1058/2700 (epoch 19.593), train_loss = 3.23111795, grad/param norm = 1.4971e-01, time/batch = 0.2686s	
1059/2700 (epoch 19.611), train_loss = 3.17124689, grad/param norm = 9.6518e-02, time/batch = 0.2696s	
1060/2700 (epoch 19.630), train_loss = 3.21265090, grad/param norm = 1.1823e-01, time/batch = 0.2697s	
1061/2700 (epoch 19.648), train_loss = 3.28221022, grad/param norm = 1.2883e-01, time/batch = 0.2693s	
1062/2700 (epoch 19.667), train_loss = 3.21502520, grad/param norm = 1.1600e-01, time/batch = 0.2690s	
1063/2700 (epoch 19.685), train_loss = 3.21167469, grad/param norm = 1.1780e-01, time/batch = 0.2693s	
1064/2700 (epoch 19.704), train_loss = 3.18423674, grad/param norm = 1.5067e-01, time/batch = 0.2694s	
1065/2700 (epoch 19.722), train_loss = 3.17561888, grad/param norm = 1.1676e-01, time/batch = 0.2691s	
1066/2700 (epoch 19.741), train_loss = 3.30783070, grad/param norm = 1.2024e-01, time/batch = 0.2691s	
1067/2700 (epoch 19.759), train_loss = 3.25813921, grad/param norm = 1.6793e-01, time/batch = 0.2689s	
1068/2700 (epoch 19.778), train_loss = 3.25350540, grad/param norm = 2.0651e-01, time/batch = 0.2687s	
1069/2700 (epoch 19.796), train_loss = 3.24785254, grad/param norm = 2.2524e-01, time/batch = 0.2694s	
1070/2700 (epoch 19.815), train_loss = 3.19923278, grad/param norm = 1.8911e-01, time/batch = 0.2698s	
1071/2700 (epoch 19.833), train_loss = 3.23391969, grad/param norm = 1.5645e-01, time/batch = 0.2687s	
1072/2700 (epoch 19.852), train_loss = 3.22022554, grad/param norm = 1.4598e-01, time/batch = 0.2690s	
1073/2700 (epoch 19.870), train_loss = 3.21302147, grad/param norm = 9.9598e-02, time/batch = 0.2692s	
1074/2700 (epoch 19.889), train_loss = 3.25136724, grad/param norm = 1.0778e-01, time/batch = 0.2695s	
1075/2700 (epoch 19.907), train_loss = 3.30462675, grad/param norm = 1.5244e-01, time/batch = 0.2692s	
1076/2700 (epoch 19.926), train_loss = 3.25289220, grad/param norm = 1.4615e-01, time/batch = 0.2696s	
1077/2700 (epoch 19.944), train_loss = 3.26268755, grad/param norm = 1.3196e-01, time/batch = 0.2691s	
1078/2700 (epoch 19.963), train_loss = 3.34181914, grad/param norm = 1.2836e-01, time/batch = 0.2689s	
1079/2700 (epoch 19.981), train_loss = 3.40344426, grad/param norm = 1.4508e-01, time/batch = 0.2692s	
decayed learning rate by a factor 0.97 to 0.0014306028061762	
1080/2700 (epoch 20.000), train_loss = 3.30125634, grad/param norm = 1.4294e-01, time/batch = 0.2692s	
1081/2700 (epoch 20.019), train_loss = 3.24141844, grad/param norm = 1.5907e-01, time/batch = 0.2688s	
1082/2700 (epoch 20.037), train_loss = 3.25738937, grad/param norm = 1.3337e-01, time/batch = 0.2686s	
1083/2700 (epoch 20.056), train_loss = 3.26003610, grad/param norm = 1.2257e-01, time/batch = 0.2696s	
1084/2700 (epoch 20.074), train_loss = 3.29194276, grad/param norm = 1.6574e-01, time/batch = 0.2699s	
1085/2700 (epoch 20.093), train_loss = 3.29586616, grad/param norm = 1.6393e-01, time/batch = 0.2698s	
1086/2700 (epoch 20.111), train_loss = 3.26555885, grad/param norm = 1.2464e-01, time/batch = 0.2694s	
1087/2700 (epoch 20.130), train_loss = 3.28123539, grad/param norm = 1.0850e-01, time/batch = 0.2694s	
1088/2700 (epoch 20.148), train_loss = 3.24480726, grad/param norm = 1.2497e-01, time/batch = 0.2687s	
1089/2700 (epoch 20.167), train_loss = 3.25419100, grad/param norm = 1.5423e-01, time/batch = 0.2702s	
1090/2700 (epoch 20.185), train_loss = 3.23907010, grad/param norm = 9.6649e-02, time/batch = 0.2699s	
1091/2700 (epoch 20.204), train_loss = 3.17466912, grad/param norm = 1.1175e-01, time/batch = 0.2713s	
1092/2700 (epoch 20.222), train_loss = 3.15086458, grad/param norm = 1.5538e-01, time/batch = 0.2694s	
1093/2700 (epoch 20.241), train_loss = 3.16900629, grad/param norm = 1.1295e-01, time/batch = 0.2691s	
1094/2700 (epoch 20.259), train_loss = 3.20269329, grad/param norm = 1.1359e-01, time/batch = 0.2694s	
1095/2700 (epoch 20.278), train_loss = 3.27677704, grad/param norm = 1.2813e-01, time/batch = 0.2694s	
1096/2700 (epoch 20.296), train_loss = 3.28039773, grad/param norm = 1.5243e-01, time/batch = 0.2694s	
1097/2700 (epoch 20.315), train_loss = 3.25717302, grad/param norm = 1.5811e-01, time/batch = 0.2690s	
1098/2700 (epoch 20.333), train_loss = 3.33879795, grad/param norm = 1.6139e-01, time/batch = 0.2688s	
1099/2700 (epoch 20.352), train_loss = 3.34481368, grad/param norm = 1.6623e-01, time/batch = 0.2695s	
1100/2700 (epoch 20.370), train_loss = 3.28632809, grad/param norm = 1.4102e-01, time/batch = 0.2702s	
1101/2700 (epoch 20.389), train_loss = 3.25237451, grad/param norm = 1.0091e-01, time/batch = 0.2705s	
1102/2700 (epoch 20.407), train_loss = 3.27447506, grad/param norm = 9.8757e-02, time/batch = 0.2689s	
1103/2700 (epoch 20.426), train_loss = 3.27722430, grad/param norm = 1.0839e-01, time/batch = 0.2701s	
1104/2700 (epoch 20.444), train_loss = 3.20684597, grad/param norm = 9.2400e-02, time/batch = 0.2716s	
1105/2700 (epoch 20.463), train_loss = 3.24829499, grad/param norm = 1.0913e-01, time/batch = 0.2696s	
1106/2700 (epoch 20.481), train_loss = 3.32552225, grad/param norm = 1.1489e-01, time/batch = 0.2693s	
1107/2700 (epoch 20.500), train_loss = 3.37157019, grad/param norm = 1.7486e-01, time/batch = 0.2689s	
1108/2700 (epoch 20.519), train_loss = 3.32478580, grad/param norm = 1.6323e-01, time/batch = 0.2702s	
1109/2700 (epoch 20.537), train_loss = 3.32824217, grad/param norm = 1.6931e-01, time/batch = 0.2705s	
1110/2700 (epoch 20.556), train_loss = 3.26328372, grad/param norm = 1.3376e-01, time/batch = 0.2702s	
1111/2700 (epoch 20.574), train_loss = 3.23184639, grad/param norm = 1.2661e-01, time/batch = 0.2712s	
1112/2700 (epoch 20.593), train_loss = 3.23073039, grad/param norm = 1.5007e-01, time/batch = 0.2700s	
1113/2700 (epoch 20.611), train_loss = 3.17108820, grad/param norm = 9.5474e-02, time/batch = 0.2694s	
1114/2700 (epoch 20.630), train_loss = 3.21254320, grad/param norm = 1.1905e-01, time/batch = 0.2701s	
1115/2700 (epoch 20.648), train_loss = 3.28164967, grad/param norm = 1.2619e-01, time/batch = 0.2695s	
1116/2700 (epoch 20.667), train_loss = 3.21439073, grad/param norm = 1.0667e-01, time/batch = 0.2698s	
1117/2700 (epoch 20.685), train_loss = 3.21108510, grad/param norm = 1.0910e-01, time/batch = 0.2693s	
1118/2700 (epoch 20.704), train_loss = 3.18354601, grad/param norm = 1.4109e-01, time/batch = 0.2688s	
1119/2700 (epoch 20.722), train_loss = 3.17439244, grad/param norm = 9.6376e-02, time/batch = 0.2692s	
1120/2700 (epoch 20.741), train_loss = 3.30796699, grad/param norm = 1.3328e-01, time/batch = 0.2690s	
1121/2700 (epoch 20.759), train_loss = 3.25738389, grad/param norm = 1.6344e-01, time/batch = 0.2699s	
1122/2700 (epoch 20.778), train_loss = 3.24940547, grad/param norm = 1.5150e-01, time/batch = 0.2688s	
1123/2700 (epoch 20.796), train_loss = 3.24188972, grad/param norm = 1.4609e-01, time/batch = 0.2691s	
1124/2700 (epoch 20.815), train_loss = 3.19393628, grad/param norm = 1.0674e-01, time/batch = 0.2702s	
1125/2700 (epoch 20.833), train_loss = 3.23072836, grad/param norm = 1.0322e-01, time/batch = 0.2702s	
1126/2700 (epoch 20.852), train_loss = 3.21774570, grad/param norm = 1.0611e-01, time/batch = 0.2698s	
1127/2700 (epoch 20.870), train_loss = 3.21186646, grad/param norm = 7.3049e-02, time/batch = 0.2700s	
1128/2700 (epoch 20.889), train_loss = 3.25077631, grad/param norm = 9.7607e-02, time/batch = 0.2694s	
1129/2700 (epoch 20.907), train_loss = 3.30428580, grad/param norm = 1.5594e-01, time/batch = 0.2704s	
1130/2700 (epoch 20.926), train_loss = 3.25369949, grad/param norm = 1.6543e-01, time/batch = 0.2697s	
1131/2700 (epoch 20.944), train_loss = 3.26373973, grad/param norm = 1.5863e-01, time/batch = 0.2708s	
1132/2700 (epoch 20.963), train_loss = 3.34310379, grad/param norm = 1.5561e-01, time/batch = 0.2691s	
1133/2700 (epoch 20.981), train_loss = 3.40336389, grad/param norm = 1.5759e-01, time/batch = 0.2697s	
decayed learning rate by a factor 0.97 to 0.0013876847219909	
1134/2700 (epoch 21.000), train_loss = 3.30199033, grad/param norm = 1.6732e-01, time/batch = 0.2701s	
1135/2700 (epoch 21.019), train_loss = 3.24353740, grad/param norm = 1.8896e-01, time/batch = 0.2699s	
1136/2700 (epoch 21.037), train_loss = 3.25818088, grad/param norm = 1.5505e-01, time/batch = 0.2699s	
1137/2700 (epoch 21.056), train_loss = 3.25874086, grad/param norm = 1.0086e-01, time/batch = 0.2696s	
1138/2700 (epoch 21.074), train_loss = 3.28922941, grad/param norm = 1.2013e-01, time/batch = 0.2691s	
1139/2700 (epoch 21.093), train_loss = 3.29383244, grad/param norm = 1.3388e-01, time/batch = 0.2696s	
1140/2700 (epoch 21.111), train_loss = 3.26513858, grad/param norm = 1.1534e-01, time/batch = 0.2690s	
1141/2700 (epoch 21.130), train_loss = 3.28136015, grad/param norm = 1.1087e-01, time/batch = 0.2709s	
1142/2700 (epoch 21.148), train_loss = 3.24539107, grad/param norm = 1.3326e-01, time/batch = 0.2686s	
1143/2700 (epoch 21.167), train_loss = 3.25421436, grad/param norm = 1.5456e-01, time/batch = 0.2693s	
1144/2700 (epoch 21.185), train_loss = 3.23906665, grad/param norm = 9.8615e-02, time/batch = 0.2700s	
1145/2700 (epoch 21.204), train_loss = 3.17455338, grad/param norm = 1.0742e-01, time/batch = 0.2694s	
1146/2700 (epoch 21.222), train_loss = 3.15031464, grad/param norm = 1.5291e-01, time/batch = 0.2695s	
1147/2700 (epoch 21.241), train_loss = 3.16823723, grad/param norm = 9.9197e-02, time/batch = 0.2695s	
1148/2700 (epoch 21.259), train_loss = 3.20183906, grad/param norm = 9.3108e-02, time/batch = 0.2690s	
1149/2700 (epoch 21.278), train_loss = 3.27623529, grad/param norm = 1.1864e-01, time/batch = 0.2694s	
1150/2700 (epoch 21.296), train_loss = 3.27866943, grad/param norm = 1.2604e-01, time/batch = 0.2696s	
1151/2700 (epoch 21.315), train_loss = 3.25480550, grad/param norm = 1.2242e-01, time/batch = 0.2703s	
1152/2700 (epoch 21.333), train_loss = 3.33698776, grad/param norm = 1.3240e-01, time/batch = 0.2686s	
1153/2700 (epoch 21.352), train_loss = 3.34449857, grad/param norm = 1.6651e-01, time/batch = 0.2696s	
1154/2700 (epoch 21.370), train_loss = 3.28763041, grad/param norm = 1.6418e-01, time/batch = 0.2701s	
1155/2700 (epoch 21.389), train_loss = 3.25409660, grad/param norm = 1.3572e-01, time/batch = 0.2693s	
1156/2700 (epoch 21.407), train_loss = 3.27535120, grad/param norm = 1.2706e-01, time/batch = 0.2688s	
1157/2700 (epoch 21.426), train_loss = 3.27860346, grad/param norm = 1.4194e-01, time/batch = 0.2691s	
1158/2700 (epoch 21.444), train_loss = 3.20781777, grad/param norm = 1.1568e-01, time/batch = 0.2692s	
1159/2700 (epoch 21.463), train_loss = 3.24834558, grad/param norm = 1.1501e-01, time/batch = 0.2696s	
1160/2700 (epoch 21.481), train_loss = 3.32515630, grad/param norm = 1.0717e-01, time/batch = 0.2697s	
1161/2700 (epoch 21.500), train_loss = 3.36971175, grad/param norm = 1.5170e-01, time/batch = 0.2702s	
1162/2700 (epoch 21.519), train_loss = 3.32223464, grad/param norm = 1.2371e-01, time/batch = 0.2689s	
1163/2700 (epoch 21.537), train_loss = 3.32634938, grad/param norm = 1.3859e-01, time/batch = 0.2695s	
1164/2700 (epoch 21.556), train_loss = 3.26162116, grad/param norm = 1.0566e-01, time/batch = 0.2699s	
1165/2700 (epoch 21.574), train_loss = 3.23139171, grad/param norm = 1.1943e-01, time/batch = 0.2691s	
1166/2700 (epoch 21.593), train_loss = 3.23028202, grad/param norm = 1.4452e-01, time/batch = 0.2691s	
1167/2700 (epoch 21.611), train_loss = 3.17084335, grad/param norm = 9.1732e-02, time/batch = 0.2694s	
1168/2700 (epoch 21.630), train_loss = 3.21209557, grad/param norm = 1.1043e-01, time/batch = 0.2698s	
1169/2700 (epoch 21.648), train_loss = 3.28083912, grad/param norm = 1.1849e-01, time/batch = 0.2696s	
1170/2700 (epoch 21.667), train_loss = 3.21393401, grad/param norm = 1.0023e-01, time/batch = 0.2696s	
1171/2700 (epoch 21.685), train_loss = 3.21130009, grad/param norm = 1.1812e-01, time/batch = 0.2698s	
1172/2700 (epoch 21.704), train_loss = 3.18449119, grad/param norm = 1.6226e-01, time/batch = 0.2690s	
1173/2700 (epoch 21.722), train_loss = 3.17716933, grad/param norm = 1.6069e-01, time/batch = 0.2691s	
1174/2700 (epoch 21.741), train_loss = 3.31485631, grad/param norm = 2.4738e-01, time/batch = 0.2695s	
1175/2700 (epoch 21.759), train_loss = 3.25982586, grad/param norm = 1.9853e-01, time/batch = 0.2706s	
1176/2700 (epoch 21.778), train_loss = 3.24815036, grad/param norm = 1.3926e-01, time/batch = 0.2701s	
1177/2700 (epoch 21.796), train_loss = 3.24085539, grad/param norm = 1.3643e-01, time/batch = 0.2698s	
1178/2700 (epoch 21.815), train_loss = 3.19341299, grad/param norm = 1.0585e-01, time/batch = 0.2686s	
1179/2700 (epoch 21.833), train_loss = 3.23056909, grad/param norm = 1.0620e-01, time/batch = 0.2697s	
1180/2700 (epoch 21.852), train_loss = 3.21763189, grad/param norm = 1.1125e-01, time/batch = 0.2699s	
1181/2700 (epoch 21.870), train_loss = 3.21165483, grad/param norm = 7.7541e-02, time/batch = 0.2700s	
1182/2700 (epoch 21.889), train_loss = 3.25066077, grad/param norm = 9.7590e-02, time/batch = 0.2692s	
1183/2700 (epoch 21.907), train_loss = 3.30354158, grad/param norm = 1.4594e-01, time/batch = 0.2699s	
1184/2700 (epoch 21.926), train_loss = 3.25177111, grad/param norm = 1.3951e-01, time/batch = 0.2696s	
1185/2700 (epoch 21.944), train_loss = 3.26161224, grad/param norm = 1.2586e-01, time/batch = 0.2700s	
1186/2700 (epoch 21.963), train_loss = 3.34127344, grad/param norm = 1.2438e-01, time/batch = 0.2695s	
1187/2700 (epoch 21.981), train_loss = 3.40237957, grad/param norm = 1.3538e-01, time/batch = 0.2698s	
decayed learning rate by a factor 0.97 to 0.0013460541803311	
1188/2700 (epoch 22.000), train_loss = 3.30010356, grad/param norm = 1.3711e-01, time/batch = 0.2691s	
1189/2700 (epoch 22.019), train_loss = 3.24029469, grad/param norm = 1.5346e-01, time/batch = 0.2696s	
1190/2700 (epoch 22.037), train_loss = 3.25640786, grad/param norm = 1.2827e-01, time/batch = 0.2698s	
1191/2700 (epoch 22.056), train_loss = 3.25950141, grad/param norm = 1.1736e-01, time/batch = 0.2701s	
1192/2700 (epoch 22.074), train_loss = 3.29101798, grad/param norm = 1.6132e-01, time/batch = 0.2690s	
1193/2700 (epoch 22.093), train_loss = 3.29552077, grad/param norm = 1.6705e-01, time/batch = 0.2698s	
1194/2700 (epoch 22.111), train_loss = 3.26526631, grad/param norm = 1.2725e-01, time/batch = 0.2697s	
1195/2700 (epoch 22.130), train_loss = 3.28070956, grad/param norm = 1.1024e-01, time/batch = 0.2693s	
1196/2700 (epoch 22.148), train_loss = 3.24476377, grad/param norm = 1.2544e-01, time/batch = 0.2689s	
1197/2700 (epoch 22.167), train_loss = 3.25413003, grad/param norm = 1.5689e-01, time/batch = 0.2693s	
1198/2700 (epoch 22.185), train_loss = 3.23867701, grad/param norm = 9.6124e-02, time/batch = 0.2696s	
1199/2700 (epoch 22.204), train_loss = 3.17475959, grad/param norm = 1.1386e-01, time/batch = 0.2700s	
1200/2700 (epoch 22.222), train_loss = 3.15080444, grad/param norm = 1.5704e-01, time/batch = 0.2697s	
1201/2700 (epoch 22.241), train_loss = 3.16891177, grad/param norm = 1.1365e-01, time/batch = 0.2713s	
1202/2700 (epoch 22.259), train_loss = 3.20228405, grad/param norm = 1.1423e-01, time/batch = 0.2696s	
1203/2700 (epoch 22.278), train_loss = 3.27622365, grad/param norm = 1.2605e-01, time/batch = 0.2700s	
1204/2700 (epoch 22.296), train_loss = 3.27957463, grad/param norm = 1.4871e-01, time/batch = 0.2696s	
1205/2700 (epoch 22.315), train_loss = 3.25631843, grad/param norm = 1.5399e-01, time/batch = 0.2700s	
1206/2700 (epoch 22.333), train_loss = 3.33809608, grad/param norm = 1.5572e-01, time/batch = 0.2694s	
1207/2700 (epoch 22.352), train_loss = 3.34371461, grad/param norm = 1.5938e-01, time/batch = 0.2689s	
1208/2700 (epoch 22.370), train_loss = 3.28530297, grad/param norm = 1.3521e-01, time/batch = 0.2686s	
1209/2700 (epoch 22.389), train_loss = 3.25189028, grad/param norm = 9.5752e-02, time/batch = 0.2691s	
1210/2700 (epoch 22.407), train_loss = 3.27400011, grad/param norm = 9.6935e-02, time/batch = 0.2694s	
1211/2700 (epoch 22.426), train_loss = 3.27658088, grad/param norm = 1.0487e-01, time/batch = 0.2706s	
1212/2700 (epoch 22.444), train_loss = 3.20679519, grad/param norm = 9.1421e-02, time/batch = 0.2692s	
1213/2700 (epoch 22.463), train_loss = 3.24794446, grad/param norm = 1.0933e-01, time/batch = 0.2698s	
1214/2700 (epoch 22.481), train_loss = 3.32545508, grad/param norm = 1.1830e-01, time/batch = 0.2696s	
1215/2700 (epoch 22.500), train_loss = 3.37094224, grad/param norm = 1.7626e-01, time/batch = 0.2698s	
1216/2700 (epoch 22.519), train_loss = 3.32406427, grad/param norm = 1.5948e-01, time/batch = 0.2699s	
1217/2700 (epoch 22.537), train_loss = 3.32734402, grad/param norm = 1.6183e-01, time/batch = 0.2693s	
1218/2700 (epoch 22.556), train_loss = 3.26218495, grad/param norm = 1.2433e-01, time/batch = 0.2690s	
1219/2700 (epoch 22.574), train_loss = 3.23153103, grad/param norm = 1.2184e-01, time/batch = 0.2694s	
1220/2700 (epoch 22.593), train_loss = 3.22982749, grad/param norm = 1.4331e-01, time/batch = 0.2690s	
1221/2700 (epoch 22.611), train_loss = 3.17066349, grad/param norm = 8.9072e-02, time/batch = 0.2703s	
1222/2700 (epoch 22.630), train_loss = 3.21202157, grad/param norm = 1.1207e-01, time/batch = 0.2682s	
1223/2700 (epoch 22.648), train_loss = 3.28042286, grad/param norm = 1.1869e-01, time/batch = 0.2692s	
1224/2700 (epoch 22.667), train_loss = 3.21360053, grad/param norm = 9.8589e-02, time/batch = 0.2693s	
1225/2700 (epoch 22.685), train_loss = 3.21074762, grad/param norm = 1.0854e-01, time/batch = 0.2696s	
1226/2700 (epoch 22.704), train_loss = 3.18317606, grad/param norm = 1.4208e-01, time/batch = 0.2689s	
1227/2700 (epoch 22.722), train_loss = 3.17417121, grad/param norm = 1.0422e-01, time/batch = 0.2690s	
1228/2700 (epoch 22.741), train_loss = 3.30884161, grad/param norm = 1.5891e-01, time/batch = 0.2685s	
1229/2700 (epoch 22.759), train_loss = 3.25752756, grad/param norm = 1.7357e-01, time/batch = 0.2698s	
1230/2700 (epoch 22.778), train_loss = 3.24840883, grad/param norm = 1.4622e-01, time/batch = 0.2694s	
1231/2700 (epoch 22.796), train_loss = 3.24064063, grad/param norm = 1.3745e-01, time/batch = 0.2710s	
1232/2700 (epoch 22.815), train_loss = 3.19302248, grad/param norm = 1.0004e-01, time/batch = 0.2687s	
1233/2700 (epoch 22.833), train_loss = 3.23014790, grad/param norm = 1.0083e-01, time/batch = 0.2692s	
1234/2700 (epoch 22.852), train_loss = 3.21722936, grad/param norm = 1.0677e-01, time/batch = 0.2694s	
1235/2700 (epoch 22.870), train_loss = 3.21144852, grad/param norm = 7.7016e-02, time/batch = 0.2696s	
1236/2700 (epoch 22.889), train_loss = 3.25080026, grad/param norm = 1.0339e-01, time/batch = 0.2692s	
1237/2700 (epoch 22.907), train_loss = 3.30389899, grad/param norm = 1.5600e-01, time/batch = 0.2691s	
1238/2700 (epoch 22.926), train_loss = 3.25237346, grad/param norm = 1.5463e-01, time/batch = 0.2690s	
1239/2700 (epoch 22.944), train_loss = 3.26202355, grad/param norm = 1.3787e-01, time/batch = 0.2693s	
1240/2700 (epoch 22.963), train_loss = 3.34130748, grad/param norm = 1.3065e-01, time/batch = 0.2693s	
1241/2700 (epoch 22.981), train_loss = 3.40225467, grad/param norm = 1.3726e-01, time/batch = 0.2701s	
decayed learning rate by a factor 0.97 to 0.0013056725549212	
1242/2700 (epoch 23.000), train_loss = 3.29998382, grad/param norm = 1.4396e-01, time/batch = 0.2695s	
1243/2700 (epoch 23.019), train_loss = 3.24098484, grad/param norm = 1.6696e-01, time/batch = 0.2692s	
1244/2700 (epoch 23.037), train_loss = 3.25662931, grad/param norm = 1.3802e-01, time/batch = 0.2701s	
1245/2700 (epoch 23.056), train_loss = 3.25866981, grad/param norm = 9.9992e-02, time/batch = 0.2692s	
1246/2700 (epoch 23.074), train_loss = 3.28874537, grad/param norm = 1.2135e-01, time/batch = 0.2691s	
1247/2700 (epoch 23.093), train_loss = 3.29324692, grad/param norm = 1.3595e-01, time/batch = 0.2692s	
1248/2700 (epoch 23.111), train_loss = 3.26466600, grad/param norm = 1.1528e-01, time/batch = 0.2686s	
1249/2700 (epoch 23.130), train_loss = 3.28071247, grad/param norm = 1.1049e-01, time/batch = 0.2693s	
1250/2700 (epoch 23.148), train_loss = 3.24494321, grad/param norm = 1.2799e-01, time/batch = 0.2693s	
1251/2700 (epoch 23.167), train_loss = 3.25351839, grad/param norm = 1.5311e-01, time/batch = 0.2706s	
1252/2700 (epoch 23.185), train_loss = 3.23838748, grad/param norm = 9.3850e-02, time/batch = 0.2693s	
1253/2700 (epoch 23.204), train_loss = 3.17459415, grad/param norm = 1.1387e-01, time/batch = 0.2696s	
1254/2700 (epoch 23.222), train_loss = 3.15039907, grad/param norm = 1.5595e-01, time/batch = 0.2695s	
1255/2700 (epoch 23.241), train_loss = 3.16973756, grad/param norm = 1.3502e-01, time/batch = 0.2694s	
1256/2700 (epoch 23.259), train_loss = 3.20440378, grad/param norm = 1.6338e-01, time/batch = 0.2694s	
1257/2700 (epoch 23.278), train_loss = 3.27808118, grad/param norm = 1.5891e-01, time/batch = 0.2695s	
1258/2700 (epoch 23.296), train_loss = 3.28116077, grad/param norm = 1.7642e-01, time/batch = 0.2685s	
1259/2700 (epoch 23.315), train_loss = 3.25651654, grad/param norm = 1.6002e-01, time/batch = 0.2692s	
1260/2700 (epoch 23.333), train_loss = 3.33714869, grad/param norm = 1.4071e-01, time/batch = 0.2691s	
1261/2700 (epoch 23.352), train_loss = 3.34220827, grad/param norm = 1.4160e-01, time/batch = 0.2705s	
1262/2700 (epoch 23.370), train_loss = 3.28439102, grad/param norm = 1.2710e-01, time/batch = 0.2692s	
1263/2700 (epoch 23.389), train_loss = 3.25162314, grad/param norm = 9.1654e-02, time/batch = 0.2693s	
1264/2700 (epoch 23.407), train_loss = 3.27386095, grad/param norm = 9.8405e-02, time/batch = 0.2700s	
1265/2700 (epoch 23.426), train_loss = 3.27626146, grad/param norm = 1.0244e-01, time/batch = 0.2695s	
1266/2700 (epoch 23.444), train_loss = 3.20686604, grad/param norm = 9.3903e-02, time/batch = 0.2691s	
1267/2700 (epoch 23.463), train_loss = 3.24801840, grad/param norm = 1.1522e-01, time/batch = 0.2705s	
1268/2700 (epoch 23.481), train_loss = 3.32569320, grad/param norm = 1.2500e-01, time/batch = 0.2689s	
1269/2700 (epoch 23.500), train_loss = 3.37043983, grad/param norm = 1.7524e-01, time/batch = 0.2695s	
1270/2700 (epoch 23.519), train_loss = 3.32311248, grad/param norm = 1.4899e-01, time/batch = 0.2692s	
1271/2700 (epoch 23.537), train_loss = 3.32638650, grad/param norm = 1.4867e-01, time/batch = 0.2700s	
1272/2700 (epoch 23.556), train_loss = 3.26127281, grad/param norm = 1.1124e-01, time/batch = 0.2688s	
1273/2700 (epoch 23.574), train_loss = 3.23131508, grad/param norm = 1.1839e-01, time/batch = 0.2691s	
1274/2700 (epoch 23.593), train_loss = 3.22940209, grad/param norm = 1.3938e-01, time/batch = 0.2698s	
1275/2700 (epoch 23.611), train_loss = 3.17049843, grad/param norm = 8.7084e-02, time/batch = 0.2698s	
1276/2700 (epoch 23.630), train_loss = 3.21172455, grad/param norm = 1.0741e-01, time/batch = 0.2691s	
1277/2700 (epoch 23.648), train_loss = 3.27981751, grad/param norm = 1.1469e-01, time/batch = 0.2691s	
1278/2700 (epoch 23.667), train_loss = 3.21327838, grad/param norm = 9.5348e-02, time/batch = 0.2688s	
1279/2700 (epoch 23.685), train_loss = 3.21081757, grad/param norm = 1.1405e-01, time/batch = 0.2695s	
1280/2700 (epoch 23.704), train_loss = 3.18346036, grad/param norm = 1.5035e-01, time/batch = 0.2690s	
1281/2700 (epoch 23.722), train_loss = 3.17465761, grad/param norm = 1.2137e-01, time/batch = 0.2702s	
1282/2700 (epoch 23.741), train_loss = 3.30984060, grad/param norm = 1.8042e-01, time/batch = 0.2687s	
1283/2700 (epoch 23.759), train_loss = 3.25717917, grad/param norm = 1.7312e-01, time/batch = 0.2692s	
1284/2700 (epoch 23.778), train_loss = 3.24759785, grad/param norm = 1.3825e-01, time/batch = 0.2702s	
1285/2700 (epoch 23.796), train_loss = 3.23992577, grad/param norm = 1.3099e-01, time/batch = 0.2702s	
1286/2700 (epoch 23.815), train_loss = 3.19261190, grad/param norm = 9.8172e-02, time/batch = 0.2694s	
1287/2700 (epoch 23.833), train_loss = 3.23002028, grad/param norm = 1.0285e-01, time/batch = 0.2693s	
1288/2700 (epoch 23.852), train_loss = 3.21712402, grad/param norm = 1.0961e-01, time/batch = 0.2700s	
1289/2700 (epoch 23.870), train_loss = 3.21125731, grad/param norm = 7.8739e-02, time/batch = 0.2695s	
1290/2700 (epoch 23.889), train_loss = 3.25064946, grad/param norm = 1.0226e-01, time/batch = 0.2700s	
1291/2700 (epoch 23.907), train_loss = 3.30340470, grad/param norm = 1.5000e-01, time/batch = 0.2705s	
1292/2700 (epoch 23.926), train_loss = 3.25143092, grad/param norm = 1.4364e-01, time/batch = 0.2688s	
1293/2700 (epoch 23.944), train_loss = 3.26127018, grad/param norm = 1.2774e-01, time/batch = 0.2697s	
1294/2700 (epoch 23.963), train_loss = 3.34074030, grad/param norm = 1.2138e-01, time/batch = 0.2702s	
1295/2700 (epoch 23.981), train_loss = 3.40202440, grad/param norm = 1.3220e-01, time/batch = 0.2702s	
decayed learning rate by a factor 0.97 to 0.0012665023782736	
1296/2700 (epoch 24.000), train_loss = 3.29926571, grad/param norm = 1.3512e-01, time/batch = 0.2700s	
1297/2700 (epoch 24.019), train_loss = 3.23964005, grad/param norm = 1.5428e-01, time/batch = 0.2691s	
1298/2700 (epoch 24.037), train_loss = 3.25572487, grad/param norm = 1.2532e-01, time/batch = 0.2699s	
1299/2700 (epoch 24.056), train_loss = 3.25886688, grad/param norm = 1.0538e-01, time/batch = 0.2699s	
1300/2700 (epoch 24.074), train_loss = 3.28951353, grad/param norm = 1.4203e-01, time/batch = 0.2703s	
1301/2700 (epoch 24.093), train_loss = 3.29407427, grad/param norm = 1.5512e-01, time/batch = 0.2707s	
1302/2700 (epoch 24.111), train_loss = 3.26485042, grad/param norm = 1.2515e-01, time/batch = 0.2691s	
1303/2700 (epoch 24.130), train_loss = 3.28046014, grad/param norm = 1.1388e-01, time/batch = 0.2703s	
1304/2700 (epoch 24.148), train_loss = 3.24458930, grad/param norm = 1.2489e-01, time/batch = 0.2694s	
1305/2700 (epoch 24.167), train_loss = 3.25320693, grad/param norm = 1.5348e-01, time/batch = 0.2695s	
1306/2700 (epoch 24.185), train_loss = 3.23817969, grad/param norm = 9.4762e-02, time/batch = 0.2696s	
1307/2700 (epoch 24.204), train_loss = 3.17464940, grad/param norm = 1.1667e-01, time/batch = 0.2693s	
1308/2700 (epoch 24.222), train_loss = 3.15025998, grad/param norm = 1.5499e-01, time/batch = 0.2689s	
1309/2700 (epoch 24.241), train_loss = 3.16920915, grad/param norm = 1.2519e-01, time/batch = 0.2692s	
1310/2700 (epoch 24.259), train_loss = 3.20278855, grad/param norm = 1.3482e-01, time/batch = 0.2688s	
1311/2700 (epoch 24.278), train_loss = 3.27631562, grad/param norm = 1.3460e-01, time/batch = 0.2698s	
1312/2700 (epoch 24.296), train_loss = 3.27947987, grad/param norm = 1.5483e-01, time/batch = 0.2687s	
1313/2700 (epoch 24.315), train_loss = 3.25558832, grad/param norm = 1.5022e-01, time/batch = 0.2694s	
1314/2700 (epoch 24.333), train_loss = 3.33702093, grad/param norm = 1.4165e-01, time/batch = 0.2699s	
1315/2700 (epoch 24.352), train_loss = 3.34208247, grad/param norm = 1.4410e-01, time/batch = 0.2693s	
1316/2700 (epoch 24.370), train_loss = 3.28409978, grad/param norm = 1.2652e-01, time/batch = 0.2691s	
1317/2700 (epoch 24.389), train_loss = 3.25145862, grad/param norm = 9.0345e-02, time/batch = 0.2692s	
1318/2700 (epoch 24.407), train_loss = 3.27363086, grad/param norm = 9.6760e-02, time/batch = 0.2688s	
1319/2700 (epoch 24.426), train_loss = 3.27600601, grad/param norm = 1.0145e-01, time/batch = 0.2695s	
1320/2700 (epoch 24.444), train_loss = 3.20682238, grad/param norm = 9.2513e-02, time/batch = 0.2703s	
1321/2700 (epoch 24.463), train_loss = 3.24776477, grad/param norm = 1.1322e-01, time/batch = 0.2717s	
1322/2700 (epoch 24.481), train_loss = 3.32558591, grad/param norm = 1.2486e-01, time/batch = 0.2694s	
1323/2700 (epoch 24.500), train_loss = 3.37025757, grad/param norm = 1.7709e-01, time/batch = 0.2696s	
1324/2700 (epoch 24.519), train_loss = 3.32312337, grad/param norm = 1.5185e-01, time/batch = 0.2698s	
1325/2700 (epoch 24.537), train_loss = 3.32633460, grad/param norm = 1.5069e-01, time/batch = 0.2698s	
1326/2700 (epoch 24.556), train_loss = 3.26105906, grad/param norm = 1.1227e-01, time/batch = 0.2699s	
1327/2700 (epoch 24.574), train_loss = 3.23124053, grad/param norm = 1.1789e-01, time/batch = 0.2698s	
1328/2700 (epoch 24.593), train_loss = 3.22902763, grad/param norm = 1.3751e-01, time/batch = 0.2690s	
1329/2700 (epoch 24.611), train_loss = 3.17039642, grad/param norm = 8.5411e-02, time/batch = 0.2694s	
1330/2700 (epoch 24.630), train_loss = 3.21160883, grad/param norm = 1.0653e-01, time/batch = 0.2694s	
1331/2700 (epoch 24.648), train_loss = 3.27935288, grad/param norm = 1.1332e-01, time/batch = 0.2703s	
1332/2700 (epoch 24.667), train_loss = 3.21301685, grad/param norm = 9.3884e-02, time/batch = 0.2702s	
1333/2700 (epoch 24.685), train_loss = 3.21064194, grad/param norm = 1.1247e-01, time/batch = 0.2693s	
1334/2700 (epoch 24.704), train_loss = 3.18311740, grad/param norm = 1.4771e-01, time/batch = 0.2699s	
1335/2700 (epoch 24.722), train_loss = 3.17425859, grad/param norm = 1.1654e-01, time/batch = 0.2691s	
1336/2700 (epoch 24.741), train_loss = 3.30937402, grad/param norm = 1.7541e-01, time/batch = 0.2691s	
1337/2700 (epoch 24.759), train_loss = 3.25666371, grad/param norm = 1.6965e-01, time/batch = 0.2691s	
1338/2700 (epoch 24.778), train_loss = 3.24721279, grad/param norm = 1.3607e-01, time/batch = 0.2686s	
1339/2700 (epoch 24.796), train_loss = 3.23952982, grad/param norm = 1.2858e-01, time/batch = 0.2695s	
1340/2700 (epoch 24.815), train_loss = 3.19229931, grad/param norm = 9.6116e-02, time/batch = 0.2697s	
1341/2700 (epoch 24.833), train_loss = 3.22976799, grad/param norm = 1.0187e-01, time/batch = 0.2700s	
1342/2700 (epoch 24.852), train_loss = 3.21684356, grad/param norm = 1.0847e-01, time/batch = 0.2692s	
1343/2700 (epoch 24.870), train_loss = 3.21100749, grad/param norm = 7.7624e-02, time/batch = 0.2696s	
1344/2700 (epoch 24.889), train_loss = 3.25053304, grad/param norm = 1.0195e-01, time/batch = 0.2696s	
1345/2700 (epoch 24.907), train_loss = 3.30318084, grad/param norm = 1.4904e-01, time/batch = 0.2697s	
1346/2700 (epoch 24.926), train_loss = 3.25106923, grad/param norm = 1.4204e-01, time/batch = 0.2694s	
1347/2700 (epoch 24.944), train_loss = 3.26092964, grad/param norm = 1.2612e-01, time/batch = 0.2698s	
1348/2700 (epoch 24.963), train_loss = 3.34048554, grad/param norm = 1.1932e-01, time/batch = 0.2695s	
1349/2700 (epoch 24.981), train_loss = 3.40194505, grad/param norm = 1.3134e-01, time/batch = 0.2702s	
decayed learning rate by a factor 0.97 to 0.0012285073069254	
1350/2700 (epoch 25.000), train_loss = 3.29887452, grad/param norm = 1.3359e-01, time/batch = 0.2695s	
1351/2700 (epoch 25.019), train_loss = 3.23913002, grad/param norm = 1.5268e-01, time/batch = 0.2705s	
1352/2700 (epoch 25.037), train_loss = 3.25535540, grad/param norm = 1.2287e-01, time/batch = 0.2698s	
1353/2700 (epoch 25.056), train_loss = 3.25885092, grad/param norm = 1.0578e-01, time/batch = 0.2699s	
1354/2700 (epoch 25.074), train_loss = 3.28932018, grad/param norm = 1.4274e-01, time/batch = 0.2703s	
1355/2700 (epoch 25.093), train_loss = 3.29372041, grad/param norm = 1.5421e-01, time/batch = 0.2704s	
1356/2700 (epoch 25.111), train_loss = 3.26456042, grad/param norm = 1.2263e-01, time/batch = 0.2701s	
1357/2700 (epoch 25.130), train_loss = 3.28013207, grad/param norm = 1.1219e-01, time/batch = 0.2702s	
1358/2700 (epoch 25.148), train_loss = 3.24437854, grad/param norm = 1.2265e-01, time/batch = 0.2702s	
1359/2700 (epoch 25.167), train_loss = 3.25272539, grad/param norm = 1.5017e-01, time/batch = 0.2707s	
1360/2700 (epoch 25.185), train_loss = 3.23787722, grad/param norm = 9.1934e-02, time/batch = 0.2700s	
1361/2700 (epoch 25.204), train_loss = 3.17444401, grad/param norm = 1.1366e-01, time/batch = 0.2707s	
1362/2700 (epoch 25.222), train_loss = 3.14985830, grad/param norm = 1.5181e-01, time/batch = 0.2693s	
1363/2700 (epoch 25.241), train_loss = 3.16889123, grad/param norm = 1.2076e-01, time/batch = 0.2694s	
1364/2700 (epoch 25.259), train_loss = 3.20243346, grad/param norm = 1.3031e-01, time/batch = 0.2702s	
1365/2700 (epoch 25.278), train_loss = 3.27591242, grad/param norm = 1.3064e-01, time/batch = 0.2697s	
1366/2700 (epoch 25.296), train_loss = 3.27896251, grad/param norm = 1.5023e-01, time/batch = 0.2692s	
1367/2700 (epoch 25.315), train_loss = 3.25513632, grad/param norm = 1.4682e-01, time/batch = 0.2696s	
1368/2700 (epoch 25.333), train_loss = 3.33679461, grad/param norm = 1.4028e-01, time/batch = 0.2693s	
1369/2700 (epoch 25.352), train_loss = 3.34176835, grad/param norm = 1.4323e-01, time/batch = 0.2691s	
1370/2700 (epoch 25.370), train_loss = 3.28373433, grad/param norm = 1.2503e-01, time/batch = 0.2696s	
1371/2700 (epoch 25.389), train_loss = 3.25130212, grad/param norm = 8.8990e-02, time/batch = 0.2704s	
1372/2700 (epoch 25.407), train_loss = 3.27344963, grad/param norm = 9.6336e-02, time/batch = 0.2689s	
1373/2700 (epoch 25.426), train_loss = 3.27575303, grad/param norm = 1.0049e-01, time/batch = 0.2694s	
1374/2700 (epoch 25.444), train_loss = 3.20681185, grad/param norm = 9.2050e-02, time/batch = 0.2696s	
1375/2700 (epoch 25.463), train_loss = 3.24757231, grad/param norm = 1.1246e-01, time/batch = 0.2691s	
1376/2700 (epoch 25.481), train_loss = 3.32551164, grad/param norm = 1.2494e-01, time/batch = 0.2693s	
1377/2700 (epoch 25.500), train_loss = 3.36982143, grad/param norm = 1.7575e-01, time/batch = 0.2694s	
1378/2700 (epoch 25.519), train_loss = 3.32277153, grad/param norm = 1.4917e-01, time/batch = 0.2691s	
1379/2700 (epoch 25.537), train_loss = 3.32606498, grad/param norm = 1.4844e-01, time/batch = 0.2691s	
1380/2700 (epoch 25.556), train_loss = 3.26069896, grad/param norm = 1.0986e-01, time/batch = 0.2695s	
1381/2700 (epoch 25.574), train_loss = 3.23113828, grad/param norm = 1.1697e-01, time/batch = 0.2704s	
1382/2700 (epoch 25.593), train_loss = 3.22866711, grad/param norm = 1.3546e-01, time/batch = 0.2686s	
1383/2700 (epoch 25.611), train_loss = 3.17032398, grad/param norm = 8.4366e-02, time/batch = 0.2693s	
1384/2700 (epoch 25.630), train_loss = 3.21148347, grad/param norm = 1.0519e-01, time/batch = 0.2693s	
1385/2700 (epoch 25.648), train_loss = 3.27888486, grad/param norm = 1.1175e-01, time/batch = 0.2696s	
1386/2700 (epoch 25.667), train_loss = 3.21276898, grad/param norm = 9.2489e-02, time/batch = 0.2694s	
1387/2700 (epoch 25.685), train_loss = 3.21055243, grad/param norm = 1.1289e-01, time/batch = 0.2696s	
1388/2700 (epoch 25.704), train_loss = 3.18295102, grad/param norm = 1.4780e-01, time/batch = 0.2689s	
1389/2700 (epoch 25.722), train_loss = 3.17406207, grad/param norm = 1.1612e-01, time/batch = 0.2696s	
1390/2700 (epoch 25.741), train_loss = 3.30913704, grad/param norm = 1.7427e-01, time/batch = 0.2696s	
1391/2700 (epoch 25.759), train_loss = 3.25613269, grad/param norm = 1.6597e-01, time/batch = 0.2698s	
1392/2700 (epoch 25.778), train_loss = 3.24678901, grad/param norm = 1.3314e-01, time/batch = 0.2686s	
1393/2700 (epoch 25.796), train_loss = 3.23911497, grad/param norm = 1.2584e-01, time/batch = 0.2697s	
1394/2700 (epoch 25.815), train_loss = 3.19200713, grad/param norm = 9.4454e-02, time/batch = 0.2696s	
1395/2700 (epoch 25.833), train_loss = 3.22954436, grad/param norm = 1.0135e-01, time/batch = 0.2699s	
1396/2700 (epoch 25.852), train_loss = 3.21658558, grad/param norm = 1.0751e-01, time/batch = 0.2697s	
1397/2700 (epoch 25.870), train_loss = 3.21075714, grad/param norm = 7.6021e-02, time/batch = 0.2693s	
1398/2700 (epoch 25.889), train_loss = 3.25037549, grad/param norm = 1.0059e-01, time/batch = 0.2694s	
1399/2700 (epoch 25.907), train_loss = 3.30289516, grad/param norm = 1.4675e-01, time/batch = 0.2692s	
1400/2700 (epoch 25.926), train_loss = 3.25064147, grad/param norm = 1.3896e-01, time/batch = 0.2697s	
1401/2700 (epoch 25.944), train_loss = 3.26056756, grad/param norm = 1.2373e-01, time/batch = 0.2699s	
1402/2700 (epoch 25.963), train_loss = 3.34023510, grad/param norm = 1.1690e-01, time/batch = 0.2688s	
1403/2700 (epoch 25.981), train_loss = 3.40189654, grad/param norm = 1.3066e-01, time/batch = 0.2698s	
decayed learning rate by a factor 0.97 to 0.0011916520877176	
1404/2700 (epoch 26.000), train_loss = 3.29847975, grad/param norm = 1.3170e-01, time/batch = 0.2698s	
1405/2700 (epoch 26.019), train_loss = 3.23856353, grad/param norm = 1.5028e-01, time/batch = 0.2697s	
1406/2700 (epoch 26.037), train_loss = 3.25497180, grad/param norm = 1.1988e-01, time/batch = 0.2691s	
1407/2700 (epoch 26.056), train_loss = 3.25893004, grad/param norm = 1.0821e-01, time/batch = 0.2695s	
1408/2700 (epoch 26.074), train_loss = 3.28934311, grad/param norm = 1.4741e-01, time/batch = 0.2692s	
1409/2700 (epoch 26.093), train_loss = 3.29352453, grad/param norm = 1.5554e-01, time/batch = 0.2692s	
1410/2700 (epoch 26.111), train_loss = 3.26427762, grad/param norm = 1.2004e-01, time/batch = 0.2692s	
1411/2700 (epoch 26.130), train_loss = 3.27978136, grad/param norm = 1.0993e-01, time/batch = 0.2704s	
1412/2700 (epoch 26.148), train_loss = 3.24414455, grad/param norm = 1.2007e-01, time/batch = 0.2687s	
1413/2700 (epoch 26.167), train_loss = 3.25223640, grad/param norm = 1.4644e-01, time/batch = 0.2689s	
1414/2700 (epoch 26.185), train_loss = 3.23758104, grad/param norm = 8.8845e-02, time/batch = 0.2692s	
1415/2700 (epoch 26.204), train_loss = 3.17421057, grad/param norm = 1.0976e-01, time/batch = 0.2695s	
1416/2700 (epoch 26.222), train_loss = 3.14943548, grad/param norm = 1.4812e-01, time/batch = 0.2694s	
1417/2700 (epoch 26.241), train_loss = 3.16847433, grad/param norm = 1.1387e-01, time/batch = 0.2694s	
1418/2700 (epoch 26.259), train_loss = 3.20194117, grad/param norm = 1.2205e-01, time/batch = 0.2692s	
1419/2700 (epoch 26.278), train_loss = 3.27540112, grad/param norm = 1.2439e-01, time/batch = 0.2694s	
1420/2700 (epoch 26.296), train_loss = 3.27831076, grad/param norm = 1.4303e-01, time/batch = 0.2695s	
1421/2700 (epoch 26.315), train_loss = 3.25464334, grad/param norm = 1.4254e-01, time/batch = 0.2705s	
1422/2700 (epoch 26.333), train_loss = 3.33663942, grad/param norm = 1.4012e-01, time/batch = 0.2689s	
1423/2700 (epoch 26.352), train_loss = 3.34156369, grad/param norm = 1.4391e-01, time/batch = 0.2698s	
1424/2700 (epoch 26.370), train_loss = 3.28341869, grad/param norm = 1.2405e-01, time/batch = 0.2700s	
1425/2700 (epoch 26.389), train_loss = 3.25115632, grad/param norm = 8.7784e-02, time/batch = 0.2698s	
1426/2700 (epoch 26.407), train_loss = 3.27326931, grad/param norm = 9.5715e-02, time/batch = 0.2692s	
1427/2700 (epoch 26.426), train_loss = 3.27550847, grad/param norm = 9.9596e-02, time/batch = 0.2693s	
1428/2700 (epoch 26.444), train_loss = 3.20679540, grad/param norm = 9.1284e-02, time/batch = 0.2689s	
1429/2700 (epoch 26.463), train_loss = 3.24735735, grad/param norm = 1.1107e-01, time/batch = 0.2697s	
1430/2700 (epoch 26.481), train_loss = 3.32541896, grad/param norm = 1.2458e-01, time/batch = 0.2692s	
1431/2700 (epoch 26.500), train_loss = 3.36940905, grad/param norm = 1.7461e-01, time/batch = 0.2700s	
1432/2700 (epoch 26.519), train_loss = 3.32250455, grad/param norm = 1.4754e-01, time/batch = 0.2690s	
1433/2700 (epoch 26.537), train_loss = 3.32586545, grad/param norm = 1.4718e-01, time/batch = 0.2694s	
1434/2700 (epoch 26.556), train_loss = 3.26038574, grad/param norm = 1.0822e-01, time/batch = 0.2693s	
1435/2700 (epoch 26.574), train_loss = 3.23104228, grad/param norm = 1.1619e-01, time/batch = 0.2692s	
1436/2700 (epoch 26.593), train_loss = 3.22831689, grad/param norm = 1.3351e-01, time/batch = 0.2692s	
1437/2700 (epoch 26.611), train_loss = 3.17026731, grad/param norm = 8.3436e-02, time/batch = 0.2697s	
1438/2700 (epoch 26.630), train_loss = 3.21137274, grad/param norm = 1.0402e-01, time/batch = 0.2687s	
1439/2700 (epoch 26.648), train_loss = 3.27843288, grad/param norm = 1.1031e-01, time/batch = 0.2694s	
1440/2700 (epoch 26.667), train_loss = 3.21253488, grad/param norm = 9.1143e-02, time/batch = 0.2692s	
1441/2700 (epoch 26.685), train_loss = 3.21045538, grad/param norm = 1.1291e-01, time/batch = 0.2703s	
1442/2700 (epoch 26.704), train_loss = 3.18276125, grad/param norm = 1.4728e-01, time/batch = 0.2689s	
1443/2700 (epoch 26.722), train_loss = 3.17382854, grad/param norm = 1.1462e-01, time/batch = 0.2690s	
1444/2700 (epoch 26.741), train_loss = 3.30885544, grad/param norm = 1.7222e-01, time/batch = 0.2694s	
1445/2700 (epoch 26.759), train_loss = 3.25563611, grad/param norm = 1.6249e-01, time/batch = 0.2694s	
1446/2700 (epoch 26.778), train_loss = 3.24640670, grad/param norm = 1.3059e-01, time/batch = 0.2689s	
1447/2700 (epoch 26.796), train_loss = 3.23873352, grad/param norm = 1.2334e-01, time/batch = 0.2692s	
1448/2700 (epoch 26.815), train_loss = 3.19173948, grad/param norm = 9.2916e-02, time/batch = 0.2687s	
1449/2700 (epoch 26.833), train_loss = 3.22932422, grad/param norm = 1.0073e-01, time/batch = 0.2691s	
1450/2700 (epoch 26.852), train_loss = 3.21633421, grad/param norm = 1.0642e-01, time/batch = 0.2692s	
1451/2700 (epoch 26.870), train_loss = 3.21052488, grad/param norm = 7.4483e-02, time/batch = 0.2705s	
1452/2700 (epoch 26.889), train_loss = 3.25022946, grad/param norm = 9.9436e-02, time/batch = 0.2693s	
1453/2700 (epoch 26.907), train_loss = 3.30263597, grad/param norm = 1.4483e-01, time/batch = 0.2691s	
1454/2700 (epoch 26.926), train_loss = 3.25026354, grad/param norm = 1.3643e-01, time/batch = 0.2692s	
1455/2700 (epoch 26.944), train_loss = 3.26022915, grad/param norm = 1.2168e-01, time/batch = 0.2696s	
1456/2700 (epoch 26.963), train_loss = 3.34000436, grad/param norm = 1.1480e-01, time/batch = 0.2696s	
1457/2700 (epoch 26.981), train_loss = 3.40186313, grad/param norm = 1.3019e-01, time/batch = 0.2694s	
decayed learning rate by a factor 0.97 to 0.0011559025250861	
1458/2700 (epoch 27.000), train_loss = 3.29809715, grad/param norm = 1.3003e-01, time/batch = 0.2687s	
1459/2700 (epoch 27.019), train_loss = 3.23803967, grad/param norm = 1.4836e-01, time/batch = 0.2694s	
1460/2700 (epoch 27.037), train_loss = 3.25462292, grad/param norm = 1.1741e-01, time/batch = 0.2692s	
1461/2700 (epoch 27.056), train_loss = 3.25899680, grad/param norm = 1.1018e-01, time/batch = 0.2701s	
1462/2700 (epoch 27.074), train_loss = 3.28928216, grad/param norm = 1.5036e-01, time/batch = 0.2691s	
1463/2700 (epoch 27.093), train_loss = 3.29322682, grad/param norm = 1.5518e-01, time/batch = 0.2698s	
1464/2700 (epoch 27.111), train_loss = 3.26398969, grad/param norm = 1.1692e-01, time/batch = 0.2695s	
1465/2700 (epoch 27.130), train_loss = 3.27947118, grad/param norm = 1.0800e-01, time/batch = 0.2698s	
1466/2700 (epoch 27.148), train_loss = 3.24395033, grad/param norm = 1.1804e-01, time/batch = 0.2693s	
1467/2700 (epoch 27.167), train_loss = 3.25179741, grad/param norm = 1.4321e-01, time/batch = 0.2691s	
1468/2700 (epoch 27.185), train_loss = 3.23732908, grad/param norm = 8.6587e-02, time/batch = 0.2694s	
1469/2700 (epoch 27.204), train_loss = 3.17402933, grad/param norm = 1.0698e-01, time/batch = 0.2694s	
1470/2700 (epoch 27.222), train_loss = 3.14908295, grad/param norm = 1.4534e-01, time/batch = 0.2692s	
1471/2700 (epoch 27.241), train_loss = 3.16818219, grad/param norm = 1.0927e-01, time/batch = 0.2701s	
1472/2700 (epoch 27.259), train_loss = 3.20158492, grad/param norm = 1.1658e-01, time/batch = 0.2697s	
1473/2700 (epoch 27.278), train_loss = 3.27498628, grad/param norm = 1.1950e-01, time/batch = 0.2694s	
1474/2700 (epoch 27.296), train_loss = 3.27771250, grad/param norm = 1.3620e-01, time/batch = 0.2698s	
1475/2700 (epoch 27.315), train_loss = 3.25411520, grad/param norm = 1.3733e-01, time/batch = 0.2701s	
1476/2700 (epoch 27.333), train_loss = 3.33641933, grad/param norm = 1.3843e-01, time/batch = 0.2698s	
1477/2700 (epoch 27.352), train_loss = 3.34129290, grad/param norm = 1.4353e-01, time/batch = 0.2695s	
1478/2700 (epoch 27.370), train_loss = 3.28309528, grad/param norm = 1.2286e-01, time/batch = 0.2696s	
1479/2700 (epoch 27.389), train_loss = 3.25102101, grad/param norm = 8.6632e-02, time/batch = 0.2695s	
1480/2700 (epoch 27.407), train_loss = 3.27310332, grad/param norm = 9.5260e-02, time/batch = 0.2696s	
1481/2700 (epoch 27.426), train_loss = 3.27527136, grad/param norm = 9.8703e-02, time/batch = 0.2711s	
1482/2700 (epoch 27.444), train_loss = 3.20678748, grad/param norm = 9.0715e-02, time/batch = 0.2699s	
1483/2700 (epoch 27.463), train_loss = 3.24715456, grad/param norm = 1.0991e-01, time/batch = 0.2693s	
1484/2700 (epoch 27.481), train_loss = 3.32534127, grad/param norm = 1.2443e-01, time/batch = 0.2696s	
1485/2700 (epoch 27.500), train_loss = 3.36900963, grad/param norm = 1.7356e-01, time/batch = 0.2701s	
1486/2700 (epoch 27.519), train_loss = 3.32224861, grad/param norm = 1.4586e-01, time/batch = 0.2692s	
1487/2700 (epoch 27.537), train_loss = 3.32566360, grad/param norm = 1.4572e-01, time/batch = 0.2694s	
1488/2700 (epoch 27.556), train_loss = 3.26007325, grad/param norm = 1.0637e-01, time/batch = 0.2692s	
1489/2700 (epoch 27.574), train_loss = 3.23095045, grad/param norm = 1.1545e-01, time/batch = 0.2693s	
1490/2700 (epoch 27.593), train_loss = 3.22797597, grad/param norm = 1.3158e-01, time/batch = 0.2692s	
1491/2700 (epoch 27.611), train_loss = 3.17022629, grad/param norm = 8.2679e-02, time/batch = 0.2708s	
1492/2700 (epoch 27.630), train_loss = 3.21126464, grad/param norm = 1.0282e-01, time/batch = 0.2687s	
1493/2700 (epoch 27.648), train_loss = 3.27799450, grad/param norm = 1.0892e-01, time/batch = 0.2695s	
1494/2700 (epoch 27.667), train_loss = 3.21231838, grad/param norm = 8.9889e-02, time/batch = 0.2701s	
1495/2700 (epoch 27.685), train_loss = 3.21036661, grad/param norm = 1.1303e-01, time/batch = 0.2696s	
1496/2700 (epoch 27.704), train_loss = 3.18258035, grad/param norm = 1.4676e-01, time/batch = 0.2696s	
1497/2700 (epoch 27.722), train_loss = 3.17359961, grad/param norm = 1.1299e-01, time/batch = 0.2692s	
1498/2700 (epoch 27.741), train_loss = 3.30856605, grad/param norm = 1.6991e-01, time/batch = 0.2697s	
1499/2700 (epoch 27.759), train_loss = 3.25515215, grad/param norm = 1.5900e-01, time/batch = 0.2703s	
1500/2700 (epoch 27.778), train_loss = 3.24604694, grad/param norm = 1.2818e-01, time/batch = 0.2696s	
1501/2700 (epoch 27.796), train_loss = 3.23837392, grad/param norm = 1.2096e-01, time/batch = 0.2701s	
1502/2700 (epoch 27.815), train_loss = 3.19149271, grad/param norm = 9.1560e-02, time/batch = 0.2687s	
1503/2700 (epoch 27.833), train_loss = 3.22911523, grad/param norm = 1.0013e-01, time/batch = 0.2690s	
1504/2700 (epoch 27.852), train_loss = 3.21609263, grad/param norm = 1.0531e-01, time/batch = 0.2696s	
1505/2700 (epoch 27.870), train_loss = 3.21030822, grad/param norm = 7.2985e-02, time/batch = 0.2690s	
1506/2700 (epoch 27.889), train_loss = 3.25008892, grad/param norm = 9.8320e-02, time/batch = 0.2692s	
1507/2700 (epoch 27.907), train_loss = 3.30238801, grad/param norm = 1.4298e-01, time/batch = 0.2695s	
1508/2700 (epoch 27.926), train_loss = 3.24991199, grad/param norm = 1.3408e-01, time/batch = 0.2687s	
1509/2700 (epoch 27.944), train_loss = 3.25990744, grad/param norm = 1.1977e-01, time/batch = 0.2692s	
1510/2700 (epoch 27.963), train_loss = 3.33979134, grad/param norm = 1.1288e-01, time/batch = 0.2691s	
1511/2700 (epoch 27.981), train_loss = 3.40184229, grad/param norm = 1.2987e-01, time/batch = 0.2700s	
decayed learning rate by a factor 0.97 to 0.0011212254493335	
1512/2700 (epoch 28.000), train_loss = 3.29772141, grad/param norm = 1.2845e-01, time/batch = 0.2691s	
1513/2700 (epoch 28.019), train_loss = 3.23753925, grad/param norm = 1.4663e-01, time/batch = 0.2693s	
1514/2700 (epoch 28.037), train_loss = 3.25429628, grad/param norm = 1.1518e-01, time/batch = 0.2702s	
1515/2700 (epoch 28.056), train_loss = 3.25906250, grad/param norm = 1.1202e-01, time/batch = 0.2699s	
1516/2700 (epoch 28.074), train_loss = 3.28918570, grad/param norm = 1.5253e-01, time/batch = 0.2694s	
1517/2700 (epoch 28.093), train_loss = 3.29288070, grad/param norm = 1.5398e-01, time/batch = 0.2696s	
1518/2700 (epoch 28.111), train_loss = 3.26371359, grad/param norm = 1.1366e-01, time/batch = 0.2695s	
1519/2700 (epoch 28.130), train_loss = 3.27919330, grad/param norm = 1.0639e-01, time/batch = 0.2701s	
1520/2700 (epoch 28.148), train_loss = 3.24377748, grad/param norm = 1.1635e-01, time/batch = 0.2690s	
1521/2700 (epoch 28.167), train_loss = 3.25139431, grad/param norm = 1.4030e-01, time/batch = 0.2700s	
1522/2700 (epoch 28.185), train_loss = 3.23710850, grad/param norm = 8.4845e-02, time/batch = 0.2683s	
1523/2700 (epoch 28.204), train_loss = 3.17387562, grad/param norm = 1.0479e-01, time/batch = 0.2689s	
1524/2700 (epoch 28.222), train_loss = 3.14877359, grad/param norm = 1.4308e-01, time/batch = 0.2699s	
1525/2700 (epoch 28.241), train_loss = 3.16795503, grad/param norm = 1.0584e-01, time/batch = 0.2700s	
1526/2700 (epoch 28.259), train_loss = 3.20130144, grad/param norm = 1.1258e-01, time/batch = 0.2696s	
1527/2700 (epoch 28.278), train_loss = 3.27463008, grad/param norm = 1.1547e-01, time/batch = 0.2701s	
1528/2700 (epoch 28.296), train_loss = 3.27716727, grad/param norm = 1.2984e-01, time/batch = 0.2690s	
1529/2700 (epoch 28.315), train_loss = 3.25358713, grad/param norm = 1.3172e-01, time/batch = 0.2690s	
1530/2700 (epoch 28.333), train_loss = 3.33616197, grad/param norm = 1.3573e-01, time/batch = 0.2694s	
1531/2700 (epoch 28.352), train_loss = 3.34097703, grad/param norm = 1.4241e-01, time/batch = 0.2705s	
1532/2700 (epoch 28.370), train_loss = 3.28277240, grad/param norm = 1.2152e-01, time/batch = 0.2686s	
1533/2700 (epoch 28.389), train_loss = 3.25089279, grad/param norm = 8.5497e-02, time/batch = 0.2693s	
1534/2700 (epoch 28.407), train_loss = 3.27294849, grad/param norm = 9.4908e-02, time/batch = 0.2694s	
1535/2700 (epoch 28.426), train_loss = 3.27504201, grad/param norm = 9.7795e-02, time/batch = 0.2692s	
1536/2700 (epoch 28.444), train_loss = 3.20678545, grad/param norm = 9.0268e-02, time/batch = 0.2690s	
1537/2700 (epoch 28.463), train_loss = 3.24695907, grad/param norm = 1.0892e-01, time/batch = 0.2697s	
1538/2700 (epoch 28.481), train_loss = 3.32527698, grad/param norm = 1.2450e-01, time/batch = 0.2688s	
1539/2700 (epoch 28.500), train_loss = 3.36863011, grad/param norm = 1.7267e-01, time/batch = 0.2688s	
1540/2700 (epoch 28.519), train_loss = 3.32201110, grad/param norm = 1.4427e-01, time/batch = 0.2690s	
1541/2700 (epoch 28.537), train_loss = 3.32546603, grad/param norm = 1.4416e-01, time/batch = 0.2706s	
1542/2700 (epoch 28.556), train_loss = 3.25976828, grad/param norm = 1.0441e-01, time/batch = 0.2687s	
1543/2700 (epoch 28.574), train_loss = 3.23086426, grad/param norm = 1.1478e-01, time/batch = 0.2691s	
1544/2700 (epoch 28.593), train_loss = 3.22764483, grad/param norm = 1.2968e-01, time/batch = 0.2697s	
1545/2700 (epoch 28.611), train_loss = 3.17019901, grad/param norm = 8.2082e-02, time/batch = 0.2699s	
1546/2700 (epoch 28.630), train_loss = 3.21115884, grad/param norm = 1.0160e-01, time/batch = 0.2693s	
1547/2700 (epoch 28.648), train_loss = 3.27756999, grad/param norm = 1.0761e-01, time/batch = 0.2693s	
1548/2700 (epoch 28.667), train_loss = 3.21211826, grad/param norm = 8.8711e-02, time/batch = 0.2701s	
1549/2700 (epoch 28.685), train_loss = 3.21028222, grad/param norm = 1.1316e-01, time/batch = 0.2700s	
1550/2700 (epoch 28.704), train_loss = 3.18240404, grad/param norm = 1.4615e-01, time/batch = 0.2700s	
1551/2700 (epoch 28.722), train_loss = 3.17336870, grad/param norm = 1.1110e-01, time/batch = 0.2707s	
1552/2700 (epoch 28.741), train_loss = 3.30826379, grad/param norm = 1.6731e-01, time/batch = 0.2691s	
1553/2700 (epoch 28.759), train_loss = 3.25468775, grad/param norm = 1.5555e-01, time/batch = 0.2696s	
1554/2700 (epoch 28.778), train_loss = 3.24570758, grad/param norm = 1.2591e-01, time/batch = 0.2701s	
1555/2700 (epoch 28.796), train_loss = 3.23803592, grad/param norm = 1.1868e-01, time/batch = 0.2700s	
1556/2700 (epoch 28.815), train_loss = 3.19126553, grad/param norm = 9.0356e-02, time/batch = 0.2692s	
1557/2700 (epoch 28.833), train_loss = 3.22891378, grad/param norm = 9.9548e-02, time/batch = 0.2691s	
1558/2700 (epoch 28.852), train_loss = 3.21586207, grad/param norm = 1.0420e-01, time/batch = 0.2696s	
1559/2700 (epoch 28.870), train_loss = 3.21010692, grad/param norm = 7.1555e-02, time/batch = 0.2701s	
1560/2700 (epoch 28.889), train_loss = 3.24995378, grad/param norm = 9.7268e-02, time/batch = 0.2693s	
1561/2700 (epoch 28.907), train_loss = 3.30215250, grad/param norm = 1.4124e-01, time/batch = 0.2700s	
1562/2700 (epoch 28.926), train_loss = 3.24958761, grad/param norm = 1.3192e-01, time/batch = 0.2687s	
1563/2700 (epoch 28.944), train_loss = 3.25960047, grad/param norm = 1.1797e-01, time/batch = 0.2700s	
1564/2700 (epoch 28.963), train_loss = 3.33959286, grad/param norm = 1.1112e-01, time/batch = 0.2705s	
1565/2700 (epoch 28.981), train_loss = 3.40182913, grad/param norm = 1.2965e-01, time/batch = 0.2698s	
decayed learning rate by a factor 0.97 to 0.0010875886858535	
1566/2700 (epoch 29.000), train_loss = 3.29735281, grad/param norm = 1.2692e-01, time/batch = 0.2700s	
1567/2700 (epoch 29.019), train_loss = 3.23705710, grad/param norm = 1.4506e-01, time/batch = 0.2702s	
1568/2700 (epoch 29.037), train_loss = 3.25398561, grad/param norm = 1.1314e-01, time/batch = 0.2692s	
1569/2700 (epoch 29.056), train_loss = 3.25912279, grad/param norm = 1.1362e-01, time/batch = 0.2693s	
1570/2700 (epoch 29.074), train_loss = 3.28905127, grad/param norm = 1.5390e-01, time/batch = 0.2703s	
1571/2700 (epoch 29.093), train_loss = 3.29250185, grad/param norm = 1.5211e-01, time/batch = 0.2707s	
1572/2700 (epoch 29.111), train_loss = 3.26345676, grad/param norm = 1.1051e-01, time/batch = 0.2695s	
1573/2700 (epoch 29.130), train_loss = 3.27894589, grad/param norm = 1.0515e-01, time/batch = 0.2694s	
1574/2700 (epoch 29.148), train_loss = 3.24361940, grad/param norm = 1.1489e-01, time/batch = 0.2701s	
1575/2700 (epoch 29.167), train_loss = 3.25102393, grad/param norm = 1.3767e-01, time/batch = 0.2701s	
1576/2700 (epoch 29.185), train_loss = 3.23691253, grad/param norm = 8.3485e-02, time/batch = 0.2700s	
1577/2700 (epoch 29.204), train_loss = 3.17374142, grad/param norm = 1.0306e-01, time/batch = 0.2702s	
1578/2700 (epoch 29.222), train_loss = 3.14849918, grad/param norm = 1.4119e-01, time/batch = 0.2691s	
1579/2700 (epoch 29.241), train_loss = 3.16777788, grad/param norm = 1.0332e-01, time/batch = 0.2703s	
1580/2700 (epoch 29.259), train_loss = 3.20107297, grad/param norm = 1.0981e-01, time/batch = 0.2704s	
1581/2700 (epoch 29.278), train_loss = 3.27432757, grad/param norm = 1.1226e-01, time/batch = 0.2706s	
1582/2700 (epoch 29.296), train_loss = 3.27668971, grad/param norm = 1.2431e-01, time/batch = 0.2695s	
1583/2700 (epoch 29.315), train_loss = 3.25309337, grad/param norm = 1.2626e-01, time/batch = 0.2698s	
1584/2700 (epoch 29.333), train_loss = 3.33588665, grad/param norm = 1.3240e-01, time/batch = 0.2695s	
1585/2700 (epoch 29.352), train_loss = 3.34062371, grad/param norm = 1.4063e-01, time/batch = 0.2692s	
1586/2700 (epoch 29.370), train_loss = 3.28244729, grad/param norm = 1.1998e-01, time/batch = 0.2696s	
1587/2700 (epoch 29.389), train_loss = 3.25077107, grad/param norm = 8.4328e-02, time/batch = 0.2698s	
1588/2700 (epoch 29.407), train_loss = 3.27280474, grad/param norm = 9.4654e-02, time/batch = 0.2693s	
1589/2700 (epoch 29.426), train_loss = 3.27481646, grad/param norm = 9.6834e-02, time/batch = 0.2701s	
1590/2700 (epoch 29.444), train_loss = 3.20679021, grad/param norm = 8.9961e-02, time/batch = 0.2700s	
1591/2700 (epoch 29.463), train_loss = 3.24677719, grad/param norm = 1.0819e-01, time/batch = 0.2708s	
1592/2700 (epoch 29.481), train_loss = 3.32523187, grad/param norm = 1.2487e-01, time/batch = 0.2692s	
1593/2700 (epoch 29.500), train_loss = 3.36827010, grad/param norm = 1.7194e-01, time/batch = 0.2692s	
1594/2700 (epoch 29.519), train_loss = 3.32178248, grad/param norm = 1.4261e-01, time/batch = 0.2698s	
1595/2700 (epoch 29.537), train_loss = 3.32526540, grad/param norm = 1.4237e-01, time/batch = 0.2696s	
1596/2700 (epoch 29.556), train_loss = 3.25946666, grad/param norm = 1.0226e-01, time/batch = 0.2694s	
1597/2700 (epoch 29.574), train_loss = 3.23078428, grad/param norm = 1.1418e-01, time/batch = 0.2699s	
1598/2700 (epoch 29.593), train_loss = 3.22732399, grad/param norm = 1.2781e-01, time/batch = 0.2694s	
1599/2700 (epoch 29.611), train_loss = 3.17018375, grad/param norm = 8.1657e-02, time/batch = 0.2696s	
1600/2700 (epoch 29.630), train_loss = 3.21105585, grad/param norm = 1.0038e-01, time/batch = 0.2697s	
1601/2700 (epoch 29.648), train_loss = 3.27716150, grad/param norm = 1.0637e-01, time/batch = 0.2701s	
1602/2700 (epoch 29.667), train_loss = 3.21193301, grad/param norm = 8.7610e-02, time/batch = 0.2693s	
1603/2700 (epoch 29.685), train_loss = 3.21020270, grad/param norm = 1.1330e-01, time/batch = 0.2693s	
1604/2700 (epoch 29.704), train_loss = 3.18223086, grad/param norm = 1.4544e-01, time/batch = 0.2697s	
1605/2700 (epoch 29.722), train_loss = 3.17313656, grad/param norm = 1.0894e-01, time/batch = 0.2699s	
1606/2700 (epoch 29.741), train_loss = 3.30795278, grad/param norm = 1.6443e-01, time/batch = 0.2695s	
1607/2700 (epoch 29.759), train_loss = 3.25424087, grad/param norm = 1.5216e-01, time/batch = 0.2695s	
1608/2700 (epoch 29.778), train_loss = 3.24538570, grad/param norm = 1.2376e-01, time/batch = 0.2698s	
1609/2700 (epoch 29.796), train_loss = 3.23771585, grad/param norm = 1.1651e-01, time/batch = 0.2700s	
1610/2700 (epoch 29.815), train_loss = 3.19105669, grad/param norm = 8.9291e-02, time/batch = 0.2699s	
1611/2700 (epoch 29.833), train_loss = 3.22872094, grad/param norm = 9.8971e-02, time/batch = 0.2699s	
1612/2700 (epoch 29.852), train_loss = 3.21564105, grad/param norm = 1.0308e-01, time/batch = 0.2689s	
1613/2700 (epoch 29.870), train_loss = 3.20992186, grad/param norm = 7.0200e-02, time/batch = 0.2703s	
1614/2700 (epoch 29.889), train_loss = 3.24982400, grad/param norm = 9.6281e-02, time/batch = 0.2695s	
1615/2700 (epoch 29.907), train_loss = 3.30192837, grad/param norm = 1.3960e-01, time/batch = 0.2695s	
1616/2700 (epoch 29.926), train_loss = 3.24928850, grad/param norm = 1.2994e-01, time/batch = 0.2696s	
1617/2700 (epoch 29.944), train_loss = 3.25930587, grad/param norm = 1.1626e-01, time/batch = 0.2698s	
1618/2700 (epoch 29.963), train_loss = 3.33940840, grad/param norm = 1.0950e-01, time/batch = 0.2696s	
1619/2700 (epoch 29.981), train_loss = 3.40182242, grad/param norm = 1.2950e-01, time/batch = 0.2696s	
decayed learning rate by a factor 0.97 to 0.0010549610252779	
1620/2700 (epoch 30.000), train_loss = 3.29698996, grad/param norm = 1.2541e-01, time/batch = 0.2700s	
1621/2700 (epoch 30.019), train_loss = 3.23659224, grad/param norm = 1.4362e-01, time/batch = 0.2706s	
1622/2700 (epoch 30.037), train_loss = 3.25369065, grad/param norm = 1.1123e-01, time/batch = 0.2693s	
1623/2700 (epoch 30.056), train_loss = 3.25917353, grad/param norm = 1.1497e-01, time/batch = 0.2699s	
1624/2700 (epoch 30.074), train_loss = 3.28888329, grad/param norm = 1.5457e-01, time/batch = 0.2702s	
1625/2700 (epoch 30.093), train_loss = 3.29210422, grad/param norm = 1.4980e-01, time/batch = 0.2697s	
1626/2700 (epoch 30.111), train_loss = 3.26322413, grad/param norm = 1.0757e-01, time/batch = 0.2698s	
1627/2700 (epoch 30.130), train_loss = 3.27872205, grad/param norm = 1.0422e-01, time/batch = 0.2693s	
1628/2700 (epoch 30.148), train_loss = 3.24347261, grad/param norm = 1.1357e-01, time/batch = 0.2694s	
1629/2700 (epoch 30.167), train_loss = 3.25067915, grad/param norm = 1.3526e-01, time/batch = 0.2702s	
1630/2700 (epoch 30.185), train_loss = 3.23673529, grad/param norm = 8.2395e-02, time/batch = 0.2702s	
1631/2700 (epoch 30.204), train_loss = 3.17362192, grad/param norm = 1.0166e-01, time/batch = 0.2711s	
1632/2700 (epoch 30.222), train_loss = 3.14825241, grad/param norm = 1.3959e-01, time/batch = 0.2700s	
1633/2700 (epoch 30.241), train_loss = 3.16763375, grad/param norm = 1.0147e-01, time/batch = 0.2699s	
1634/2700 (epoch 30.259), train_loss = 3.20088292, grad/param norm = 1.0791e-01, time/batch = 0.2697s	
1635/2700 (epoch 30.278), train_loss = 3.27406407, grad/param norm = 1.0969e-01, time/batch = 0.2698s	
1636/2700 (epoch 30.296), train_loss = 3.27627329, grad/param norm = 1.1963e-01, time/batch = 0.2696s	
1637/2700 (epoch 30.315), train_loss = 3.25264666, grad/param norm = 1.2127e-01, time/batch = 0.2694s	
1638/2700 (epoch 30.333), train_loss = 3.33561553, grad/param norm = 1.2886e-01, time/batch = 0.2701s	
1639/2700 (epoch 30.352), train_loss = 3.34024977, grad/param norm = 1.3841e-01, time/batch = 0.2703s	
1640/2700 (epoch 30.370), train_loss = 3.28212185, grad/param norm = 1.1826e-01, time/batch = 0.2698s	
1641/2700 (epoch 30.389), train_loss = 3.25065254, grad/param norm = 8.3114e-02, time/batch = 0.2700s	
1642/2700 (epoch 30.407), train_loss = 3.27267161, grad/param norm = 9.4487e-02, time/batch = 0.2693s	
1643/2700 (epoch 30.426), train_loss = 3.27459486, grad/param norm = 9.5804e-02, time/batch = 0.2695s	
1644/2700 (epoch 30.444), train_loss = 3.20680057, grad/param norm = 8.9798e-02, time/batch = 0.2697s	
1645/2700 (epoch 30.463), train_loss = 3.24661002, grad/param norm = 1.0777e-01, time/batch = 0.2696s	
1646/2700 (epoch 30.481), train_loss = 3.32520778, grad/param norm = 1.2557e-01, time/batch = 0.2701s	
1647/2700 (epoch 30.500), train_loss = 3.36792877, grad/param norm = 1.7135e-01, time/batch = 0.2699s	
1648/2700 (epoch 30.519), train_loss = 3.32155494, grad/param norm = 1.4076e-01, time/batch = 0.2695s	
1649/2700 (epoch 30.537), train_loss = 3.32505858, grad/param norm = 1.4030e-01, time/batch = 0.2704s	
1650/2700 (epoch 30.556), train_loss = 3.25916849, grad/param norm = 9.9936e-02, time/batch = 0.2703s	
1651/2700 (epoch 30.574), train_loss = 3.23071056, grad/param norm = 1.1368e-01, time/batch = 0.2706s	
1652/2700 (epoch 30.593), train_loss = 3.22701277, grad/param norm = 1.2600e-01, time/batch = 0.2691s	
1653/2700 (epoch 30.611), train_loss = 3.17017986, grad/param norm = 8.1401e-02, time/batch = 0.2694s	
1654/2700 (epoch 30.630), train_loss = 3.21095526, grad/param norm = 9.9147e-02, time/batch = 0.2702s	
1655/2700 (epoch 30.648), train_loss = 3.27676760, grad/param norm = 1.0519e-01, time/batch = 0.2700s	
1656/2700 (epoch 30.667), train_loss = 3.21176127, grad/param norm = 8.6576e-02, time/batch = 0.2694s	
1657/2700 (epoch 30.685), train_loss = 3.21012560, grad/param norm = 1.1343e-01, time/batch = 0.2698s	
1658/2700 (epoch 30.704), train_loss = 3.18205822, grad/param norm = 1.4460e-01, time/batch = 0.2692s	
1659/2700 (epoch 30.722), train_loss = 3.17290298, grad/param norm = 1.0650e-01, time/batch = 0.2693s	
1660/2700 (epoch 30.741), train_loss = 3.30763532, grad/param norm = 1.6134e-01, time/batch = 0.2691s	
1661/2700 (epoch 30.759), train_loss = 3.25381467, grad/param norm = 1.4887e-01, time/batch = 0.2701s	
1662/2700 (epoch 30.778), train_loss = 3.24508125, grad/param norm = 1.2171e-01, time/batch = 0.2686s	
1663/2700 (epoch 30.796), train_loss = 3.23741465, grad/param norm = 1.1442e-01, time/batch = 0.2691s	
1664/2700 (epoch 30.815), train_loss = 3.19086304, grad/param norm = 8.8348e-02, time/batch = 0.2698s	
1665/2700 (epoch 30.833), train_loss = 3.22853595, grad/param norm = 9.8397e-02, time/batch = 0.2700s	
1666/2700 (epoch 30.852), train_loss = 3.21543099, grad/param norm = 1.0197e-01, time/batch = 0.2691s	
1667/2700 (epoch 30.870), train_loss = 3.20975204, grad/param norm = 6.8929e-02, time/batch = 0.2692s	
1668/2700 (epoch 30.889), train_loss = 3.24969969, grad/param norm = 9.5357e-02, time/batch = 0.2692s	
1669/2700 (epoch 30.907), train_loss = 3.30171563, grad/param norm = 1.3805e-01, time/batch = 0.2692s	
1670/2700 (epoch 30.926), train_loss = 3.24901296, grad/param norm = 1.2812e-01, time/batch = 0.2695s	
1671/2700 (epoch 30.944), train_loss = 3.25902260, grad/param norm = 1.1463e-01, time/batch = 0.2712s	
1672/2700 (epoch 30.963), train_loss = 3.33923571, grad/param norm = 1.0800e-01, time/batch = 0.2690s	
1673/2700 (epoch 30.981), train_loss = 3.40181942, grad/param norm = 1.2938e-01, time/batch = 0.2701s	
decayed learning rate by a factor 0.97 to 0.0010233121945196	
1674/2700 (epoch 31.000), train_loss = 3.29663420, grad/param norm = 1.2392e-01, time/batch = 0.2706s	
1675/2700 (epoch 31.019), train_loss = 3.23614151, grad/param norm = 1.4227e-01, time/batch = 0.2700s	
1676/2700 (epoch 31.037), train_loss = 3.25340743, grad/param norm = 1.0942e-01, time/batch = 0.2695s	
1677/2700 (epoch 31.056), train_loss = 3.25921436, grad/param norm = 1.1606e-01, time/batch = 0.2701s	
1678/2700 (epoch 31.074), train_loss = 3.28868862, grad/param norm = 1.5468e-01, time/batch = 0.2701s	
1679/2700 (epoch 31.093), train_loss = 3.29170108, grad/param norm = 1.4723e-01, time/batch = 0.2697s	
1680/2700 (epoch 31.111), train_loss = 3.26301339, grad/param norm = 1.0490e-01, time/batch = 0.2699s	
1681/2700 (epoch 31.130), train_loss = 3.27851861, grad/param norm = 1.0352e-01, time/batch = 0.2709s	
1682/2700 (epoch 31.148), train_loss = 3.24333163, grad/param norm = 1.1234e-01, time/batch = 0.2691s	
1683/2700 (epoch 31.167), train_loss = 3.25035764, grad/param norm = 1.3303e-01, time/batch = 0.2702s	
1684/2700 (epoch 31.185), train_loss = 3.23657510, grad/param norm = 8.1493e-02, time/batch = 0.2701s	
1685/2700 (epoch 31.204), train_loss = 3.17351195, grad/param norm = 1.0050e-01, time/batch = 0.2700s	
1686/2700 (epoch 31.222), train_loss = 3.14802670, grad/param norm = 1.3819e-01, time/batch = 0.2702s	
1687/2700 (epoch 31.241), train_loss = 3.16751636, grad/param norm = 1.0007e-01, time/batch = 0.2700s	
1688/2700 (epoch 31.259), train_loss = 3.20071974, grad/param norm = 1.0658e-01, time/batch = 0.2695s	
1689/2700 (epoch 31.278), train_loss = 3.27382963, grad/param norm = 1.0757e-01, time/batch = 0.2702s	
1690/2700 (epoch 31.296), train_loss = 3.27590737, grad/param norm = 1.1565e-01, time/batch = 0.2698s	
1691/2700 (epoch 31.315), train_loss = 3.25224647, grad/param norm = 1.1682e-01, time/batch = 0.2712s	
1692/2700 (epoch 31.333), train_loss = 3.33536062, grad/param norm = 1.2541e-01, time/batch = 0.2695s	
1693/2700 (epoch 31.352), train_loss = 3.33987087, grad/param norm = 1.3597e-01, time/batch = 0.2700s	
1694/2700 (epoch 31.370), train_loss = 3.28179947, grad/param norm = 1.1642e-01, time/batch = 0.2698s	
1695/2700 (epoch 31.389), train_loss = 3.25053903, grad/param norm = 8.1885e-02, time/batch = 0.2705s	
1696/2700 (epoch 31.407), train_loss = 3.27254832, grad/param norm = 9.4398e-02, time/batch = 0.2703s	
1697/2700 (epoch 31.426), train_loss = 3.27437543, grad/param norm = 9.4706e-02, time/batch = 0.2699s	
1698/2700 (epoch 31.444), train_loss = 3.20681608, grad/param norm = 8.9777e-02, time/batch = 0.2697s	
1699/2700 (epoch 31.463), train_loss = 3.24645890, grad/param norm = 1.0768e-01, time/batch = 0.2703s	
1700/2700 (epoch 31.481), train_loss = 3.32520122, grad/param norm = 1.2654e-01, time/batch = 0.2693s	
1701/2700 (epoch 31.500), train_loss = 3.36759959, grad/param norm = 1.7077e-01, time/batch = 0.2705s	
1702/2700 (epoch 31.519), train_loss = 3.32132145, grad/param norm = 1.3859e-01, time/batch = 0.2691s	
1703/2700 (epoch 31.537), train_loss = 3.32484684, grad/param norm = 1.3797e-01, time/batch = 0.2694s	
1704/2700 (epoch 31.556), train_loss = 3.25887643, grad/param norm = 9.7502e-02, time/batch = 0.2704s	
1705/2700 (epoch 31.574), train_loss = 3.23064373, grad/param norm = 1.1329e-01, time/batch = 0.2698s	
1706/2700 (epoch 31.593), train_loss = 3.22671194, grad/param norm = 1.2426e-01, time/batch = 0.2700s	
1707/2700 (epoch 31.611), train_loss = 3.17018559, grad/param norm = 8.1298e-02, time/batch = 0.2696s	
1708/2700 (epoch 31.630), train_loss = 3.21085819, grad/param norm = 9.7924e-02, time/batch = 0.2693s	
1709/2700 (epoch 31.648), train_loss = 3.27639025, grad/param norm = 1.0410e-01, time/batch = 0.2701s	
1710/2700 (epoch 31.667), train_loss = 3.21160269, grad/param norm = 8.5590e-02, time/batch = 0.2703s	
1711/2700 (epoch 31.685), train_loss = 3.21004704, grad/param norm = 1.1349e-01, time/batch = 0.2709s	
1712/2700 (epoch 31.704), train_loss = 3.18188404, grad/param norm = 1.4359e-01, time/batch = 0.2692s	
1713/2700 (epoch 31.722), train_loss = 3.17266915, grad/param norm = 1.0378e-01, time/batch = 0.2699s	
1714/2700 (epoch 31.741), train_loss = 3.30731705, grad/param norm = 1.5810e-01, time/batch = 0.2702s	
1715/2700 (epoch 31.759), train_loss = 3.25341232, grad/param norm = 1.4572e-01, time/batch = 0.2702s	
1716/2700 (epoch 31.778), train_loss = 3.24479104, grad/param norm = 1.1976e-01, time/batch = 0.2696s	
1717/2700 (epoch 31.796), train_loss = 3.23713089, grad/param norm = 1.1241e-01, time/batch = 0.2700s	
1718/2700 (epoch 31.815), train_loss = 3.19068476, grad/param norm = 8.7509e-02, time/batch = 0.2689s	
1719/2700 (epoch 31.833), train_loss = 3.22835737, grad/param norm = 9.7821e-02, time/batch = 0.2694s	
1720/2700 (epoch 31.852), train_loss = 3.21523029, grad/param norm = 1.0087e-01, time/batch = 0.2697s	
1721/2700 (epoch 31.870), train_loss = 3.20959600, grad/param norm = 6.7748e-02, time/batch = 0.2710s	
1722/2700 (epoch 31.889), train_loss = 3.24957970, grad/param norm = 9.4497e-02, time/batch = 0.2697s	
1723/2700 (epoch 31.907), train_loss = 3.30151174, grad/param norm = 1.3659e-01, time/batch = 0.2697s	
1724/2700 (epoch 31.926), train_loss = 3.24875836, grad/param norm = 1.2644e-01, time/batch = 0.2700s	
1725/2700 (epoch 31.944), train_loss = 3.25874947, grad/param norm = 1.1306e-01, time/batch = 0.2699s	
1726/2700 (epoch 31.963), train_loss = 3.33907460, grad/param norm = 1.0660e-01, time/batch = 0.2698s	
1727/2700 (epoch 31.981), train_loss = 3.40181729, grad/param norm = 1.2929e-01, time/batch = 0.2695s	
decayed learning rate by a factor 0.97 to 0.00099261282868397	
1728/2700 (epoch 32.000), train_loss = 3.29628492, grad/param norm = 1.2244e-01, time/batch = 0.2694s	
1729/2700 (epoch 32.019), train_loss = 3.23570505, grad/param norm = 1.4100e-01, time/batch = 0.2697s	
1730/2700 (epoch 32.037), train_loss = 3.25313605, grad/param norm = 1.0768e-01, time/batch = 0.2701s	
1731/2700 (epoch 32.056), train_loss = 3.25924519, grad/param norm = 1.1691e-01, time/batch = 0.2714s	
1732/2700 (epoch 32.074), train_loss = 3.28847509, grad/param norm = 1.5434e-01, time/batch = 0.2701s	
1733/2700 (epoch 32.093), train_loss = 3.29130145, grad/param norm = 1.4451e-01, time/batch = 0.2699s	
1734/2700 (epoch 32.111), train_loss = 3.26282422, grad/param norm = 1.0251e-01, time/batch = 0.2702s	
1735/2700 (epoch 32.130), train_loss = 3.27832784, grad/param norm = 1.0298e-01, time/batch = 0.2705s	
1736/2700 (epoch 32.148), train_loss = 3.24319736, grad/param norm = 1.1118e-01, time/batch = 0.2699s	
1737/2700 (epoch 32.167), train_loss = 3.25005781, grad/param norm = 1.3093e-01, time/batch = 0.2700s	
1738/2700 (epoch 32.185), train_loss = 3.23642649, grad/param norm = 8.0728e-02, time/batch = 0.2698s	
1739/2700 (epoch 32.204), train_loss = 3.17341030, grad/param norm = 9.9499e-02, time/batch = 0.2703s	
1740/2700 (epoch 32.222), train_loss = 3.14781812, grad/param norm = 1.3694e-01, time/batch = 0.2707s	
1741/2700 (epoch 32.241), train_loss = 3.16741532, grad/param norm = 9.8991e-02, time/batch = 0.2710s	
1742/2700 (epoch 32.259), train_loss = 3.20057473, grad/param norm = 1.0562e-01, time/batch = 0.2697s	
1743/2700 (epoch 32.278), train_loss = 3.27361821, grad/param norm = 1.0579e-01, time/batch = 0.2702s	
1744/2700 (epoch 32.296), train_loss = 3.27558284, grad/param norm = 1.1224e-01, time/batch = 0.2702s	
1745/2700 (epoch 32.315), train_loss = 3.25188780, grad/param norm = 1.1290e-01, time/batch = 0.2695s	
1746/2700 (epoch 32.333), train_loss = 3.33512920, grad/param norm = 1.2221e-01, time/batch = 0.2691s	
1747/2700 (epoch 32.352), train_loss = 3.33949750, grad/param norm = 1.3348e-01, time/batch = 0.2695s	
1748/2700 (epoch 32.370), train_loss = 3.28148348, grad/param norm = 1.1451e-01, time/batch = 0.2700s	
1749/2700 (epoch 32.389), train_loss = 3.25043228, grad/param norm = 8.0682e-02, time/batch = 0.2707s	
1750/2700 (epoch 32.407), train_loss = 3.27243394, grad/param norm = 9.4383e-02, time/batch = 0.2703s	
1751/2700 (epoch 32.426), train_loss = 3.27415920, grad/param norm = 9.3560e-02, time/batch = 0.2708s	
1752/2700 (epoch 32.444), train_loss = 3.20683805, grad/param norm = 8.9894e-02, time/batch = 0.2692s	
1753/2700 (epoch 32.463), train_loss = 3.24632290, grad/param norm = 1.0789e-01, time/batch = 0.2704s	
1754/2700 (epoch 32.481), train_loss = 3.32520862, grad/param norm = 1.2769e-01, time/batch = 0.2701s	
1755/2700 (epoch 32.500), train_loss = 3.36727365, grad/param norm = 1.7008e-01, time/batch = 0.2692s	
1756/2700 (epoch 32.519), train_loss = 3.32107666, grad/param norm = 1.3603e-01, time/batch = 0.2692s	
1757/2700 (epoch 32.537), train_loss = 3.32463543, grad/param norm = 1.3546e-01, time/batch = 0.2689s	
1758/2700 (epoch 32.556), train_loss = 3.25859579, grad/param norm = 9.5068e-02, time/batch = 0.2698s	
1759/2700 (epoch 32.574), train_loss = 3.23058461, grad/param norm = 1.1303e-01, time/batch = 0.2696s	
1760/2700 (epoch 32.593), train_loss = 3.22642044, grad/param norm = 1.2257e-01, time/batch = 0.2693s	
1761/2700 (epoch 32.611), train_loss = 3.17020148, grad/param norm = 8.1316e-02, time/batch = 0.2701s	
1762/2700 (epoch 32.630), train_loss = 3.21076444, grad/param norm = 9.6714e-02, time/batch = 0.2695s	
1763/2700 (epoch 32.648), train_loss = 3.27602703, grad/param norm = 1.0306e-01, time/batch = 0.2701s	
1764/2700 (epoch 32.667), train_loss = 3.21145485, grad/param norm = 8.4633e-02, time/batch = 0.2704s	
1765/2700 (epoch 32.685), train_loss = 3.20996554, grad/param norm = 1.1346e-01, time/batch = 0.2700s	
1766/2700 (epoch 32.704), train_loss = 3.18170790, grad/param norm = 1.4242e-01, time/batch = 0.2692s	
1767/2700 (epoch 32.722), train_loss = 3.17243844, grad/param norm = 1.0085e-01, time/batch = 0.2692s	
1768/2700 (epoch 32.741), train_loss = 3.30700267, grad/param norm = 1.5485e-01, time/batch = 0.2696s	
1769/2700 (epoch 32.759), train_loss = 3.25303286, grad/param norm = 1.4276e-01, time/batch = 0.2698s	
1770/2700 (epoch 32.778), train_loss = 3.24451377, grad/param norm = 1.1788e-01, time/batch = 0.2693s	
1771/2700 (epoch 32.796), train_loss = 3.23686289, grad/param norm = 1.1048e-01, time/batch = 0.2699s	
1772/2700 (epoch 32.815), train_loss = 3.19051985, grad/param norm = 8.6763e-02, time/batch = 0.2697s	
1773/2700 (epoch 32.833), train_loss = 3.22818473, grad/param norm = 9.7239e-02, time/batch = 0.2696s	
1774/2700 (epoch 32.852), train_loss = 3.21503892, grad/param norm = 9.9780e-02, time/batch = 0.2703s	
1775/2700 (epoch 32.870), train_loss = 3.20945307, grad/param norm = 6.6647e-02, time/batch = 0.2701s	
1776/2700 (epoch 32.889), train_loss = 3.24946406, grad/param norm = 9.3689e-02, time/batch = 0.2699s	
1777/2700 (epoch 32.907), train_loss = 3.30131752, grad/param norm = 1.3519e-01, time/batch = 0.2704s	
1778/2700 (epoch 32.926), train_loss = 3.24852373, grad/param norm = 1.2490e-01, time/batch = 0.2696s	
1779/2700 (epoch 32.944), train_loss = 3.25848550, grad/param norm = 1.1153e-01, time/batch = 0.2700s	
1780/2700 (epoch 32.963), train_loss = 3.33892308, grad/param norm = 1.0530e-01, time/batch = 0.2701s	
1781/2700 (epoch 32.981), train_loss = 3.40181727, grad/param norm = 1.2920e-01, time/batch = 0.2715s	
decayed learning rate by a factor 0.97 to 0.00096283444382345	
1782/2700 (epoch 33.000), train_loss = 3.29594351, grad/param norm = 1.2096e-01, time/batch = 0.2698s	
1783/2700 (epoch 33.019), train_loss = 3.23527896, grad/param norm = 1.3980e-01, time/batch = 0.2694s	
1784/2700 (epoch 33.037), train_loss = 3.25287523, grad/param norm = 1.0601e-01, time/batch = 0.2693s	
1785/2700 (epoch 33.056), train_loss = 3.25926759, grad/param norm = 1.1757e-01, time/batch = 0.2703s	
1786/2700 (epoch 33.074), train_loss = 3.28825007, grad/param norm = 1.5366e-01, time/batch = 0.2702s	
1787/2700 (epoch 33.093), train_loss = 3.29091090, grad/param norm = 1.4174e-01, time/batch = 0.2694s	
1788/2700 (epoch 33.111), train_loss = 3.26265519, grad/param norm = 1.0036e-01, time/batch = 0.2695s	
1789/2700 (epoch 33.130), train_loss = 3.27814873, grad/param norm = 1.0255e-01, time/batch = 0.2701s	
1790/2700 (epoch 33.148), train_loss = 3.24306834, grad/param norm = 1.1008e-01, time/batch = 0.2695s	
1791/2700 (epoch 33.167), train_loss = 3.24977559, grad/param norm = 1.2895e-01, time/batch = 0.2711s	
1792/2700 (epoch 33.185), train_loss = 3.23628978, grad/param norm = 8.0067e-02, time/batch = 0.2694s	
1793/2700 (epoch 33.204), train_loss = 3.17331398, grad/param norm = 9.8623e-02, time/batch = 0.2694s	
1794/2700 (epoch 33.222), train_loss = 3.14762226, grad/param norm = 1.3582e-01, time/batch = 0.2698s	
1795/2700 (epoch 33.241), train_loss = 3.16732825, grad/param norm = 9.8136e-02, time/batch = 0.2697s	
1796/2700 (epoch 33.259), train_loss = 3.20044257, grad/param norm = 1.0489e-01, time/batch = 0.2697s	
1797/2700 (epoch 33.278), train_loss = 3.27342408, grad/param norm = 1.0424e-01, time/batch = 0.2698s	
1798/2700 (epoch 33.296), train_loss = 3.27528941, grad/param norm = 1.0924e-01, time/batch = 0.2693s	
1799/2700 (epoch 33.315), train_loss = 3.25156312, grad/param norm = 1.0942e-01, time/batch = 0.2700s	
1800/2700 (epoch 33.333), train_loss = 3.33492161, grad/param norm = 1.1931e-01, time/batch = 0.2698s	
1801/2700 (epoch 33.352), train_loss = 3.33913500, grad/param norm = 1.3102e-01, time/batch = 0.2708s	
1802/2700 (epoch 33.370), train_loss = 3.28117692, grad/param norm = 1.1259e-01, time/batch = 0.2689s	
1803/2700 (epoch 33.389), train_loss = 3.25033176, grad/param norm = 7.9549e-02, time/batch = 0.2690s	
1804/2700 (epoch 33.407), train_loss = 3.27232778, grad/param norm = 9.4439e-02, time/batch = 0.2695s	
1805/2700 (epoch 33.426), train_loss = 3.27394708, grad/param norm = 9.2396e-02, time/batch = 0.2695s	
1806/2700 (epoch 33.444), train_loss = 3.20686508, grad/param norm = 9.0140e-02, time/batch = 0.2689s	
1807/2700 (epoch 33.463), train_loss = 3.24619932, grad/param norm = 1.0833e-01, time/batch = 0.2702s	
1808/2700 (epoch 33.481), train_loss = 3.32522192, grad/param norm = 1.2887e-01, time/batch = 0.2699s	
1809/2700 (epoch 33.500), train_loss = 3.36694386, grad/param norm = 1.6912e-01, time/batch = 0.2700s	
1810/2700 (epoch 33.519), train_loss = 3.32082222, grad/param norm = 1.3307e-01, time/batch = 0.2698s	
1811/2700 (epoch 33.537), train_loss = 3.32442957, grad/param norm = 1.3288e-01, time/batch = 0.2696s	
1812/2700 (epoch 33.556), train_loss = 3.25832966, grad/param norm = 9.2736e-02, time/batch = 0.2688s	
1813/2700 (epoch 33.574), train_loss = 3.23053158, grad/param norm = 1.1287e-01, time/batch = 0.2689s	
1814/2700 (epoch 33.593), train_loss = 3.22613677, grad/param norm = 1.2092e-01, time/batch = 0.2698s	
1815/2700 (epoch 33.611), train_loss = 3.17022224, grad/param norm = 8.1418e-02, time/batch = 0.2693s	
1816/2700 (epoch 33.630), train_loss = 3.21067314, grad/param norm = 9.5519e-02, time/batch = 0.2694s	
1817/2700 (epoch 33.648), train_loss = 3.27568030, grad/param norm = 1.0208e-01, time/batch = 0.2696s	
1818/2700 (epoch 33.667), train_loss = 3.21131803, grad/param norm = 8.3688e-02, time/batch = 0.2692s	
1819/2700 (epoch 33.685), train_loss = 3.20988129, grad/param norm = 1.1332e-01, time/batch = 0.2697s	
1820/2700 (epoch 33.704), train_loss = 3.18153126, grad/param norm = 1.4111e-01, time/batch = 0.2694s	
1821/2700 (epoch 33.722), train_loss = 3.17221464, grad/param norm = 9.7806e-02, time/batch = 0.2703s	
1822/2700 (epoch 33.741), train_loss = 3.30669885, grad/param norm = 1.5170e-01, time/batch = 0.2689s	
1823/2700 (epoch 33.759), train_loss = 3.25267717, grad/param norm = 1.3999e-01, time/batch = 0.2696s	
1824/2700 (epoch 33.778), train_loss = 3.24424920, grad/param norm = 1.1608e-01, time/batch = 0.2696s	
1825/2700 (epoch 33.796), train_loss = 3.23661400, grad/param norm = 1.0864e-01, time/batch = 0.2696s	
1826/2700 (epoch 33.815), train_loss = 3.19036726, grad/param norm = 8.6096e-02, time/batch = 0.2692s	
1827/2700 (epoch 33.833), train_loss = 3.22801842, grad/param norm = 9.6647e-02, time/batch = 0.2691s	
1828/2700 (epoch 33.852), train_loss = 3.21485692, grad/param norm = 9.8699e-02, time/batch = 0.2690s	
1829/2700 (epoch 33.870), train_loss = 3.20932154, grad/param norm = 6.5617e-02, time/batch = 0.2699s	
1830/2700 (epoch 33.889), train_loss = 3.24935201, grad/param norm = 9.2920e-02, time/batch = 0.2697s	
1831/2700 (epoch 33.907), train_loss = 3.30113127, grad/param norm = 1.3385e-01, time/batch = 0.2704s	
1832/2700 (epoch 33.926), train_loss = 3.24830687, grad/param norm = 1.2346e-01, time/batch = 0.2685s	
1833/2700 (epoch 33.944), train_loss = 3.25823061, grad/param norm = 1.1004e-01, time/batch = 0.2693s	
1834/2700 (epoch 33.963), train_loss = 3.33878054, grad/param norm = 1.0409e-01, time/batch = 0.2699s	
1835/2700 (epoch 33.981), train_loss = 3.40181543, grad/param norm = 1.2911e-01, time/batch = 0.2693s	
decayed learning rate by a factor 0.97 to 0.00093394941050874	
1836/2700 (epoch 34.000), train_loss = 3.29560988, grad/param norm = 1.1949e-01, time/batch = 0.2689s	
1837/2700 (epoch 34.019), train_loss = 3.23486442, grad/param norm = 1.3864e-01, time/batch = 0.2689s	
1838/2700 (epoch 34.037), train_loss = 3.25262456, grad/param norm = 1.0439e-01, time/batch = 0.2687s	
1839/2700 (epoch 34.056), train_loss = 3.25928130, grad/param norm = 1.1806e-01, time/batch = 0.2692s	
1840/2700 (epoch 34.074), train_loss = 3.28801715, grad/param norm = 1.5274e-01, time/batch = 0.2694s	
1841/2700 (epoch 34.093), train_loss = 3.29053195, grad/param norm = 1.3897e-01, time/batch = 0.2699s	
1842/2700 (epoch 34.111), train_loss = 3.26250153, grad/param norm = 9.8443e-02, time/batch = 0.2687s	
1843/2700 (epoch 34.130), train_loss = 3.27797731, grad/param norm = 1.0219e-01, time/batch = 0.2694s	
1844/2700 (epoch 34.148), train_loss = 3.24294405, grad/param norm = 1.0902e-01, time/batch = 0.2697s	
1845/2700 (epoch 34.167), train_loss = 3.24951108, grad/param norm = 1.2708e-01, time/batch = 0.2694s	
1846/2700 (epoch 34.185), train_loss = 3.23616293, grad/param norm = 7.9488e-02, time/batch = 0.2688s	
1847/2700 (epoch 34.204), train_loss = 3.17322166, grad/param norm = 9.7842e-02, time/batch = 0.2695s	
1848/2700 (epoch 34.222), train_loss = 3.14743755, grad/param norm = 1.3479e-01, time/batch = 0.2687s	
1849/2700 (epoch 34.241), train_loss = 3.16725049, grad/param norm = 9.7440e-02, time/batch = 0.2691s	
1850/2700 (epoch 34.259), train_loss = 3.20031994, grad/param norm = 1.0429e-01, time/batch = 0.2690s	
1851/2700 (epoch 34.278), train_loss = 3.27324459, grad/param norm = 1.0287e-01, time/batch = 0.2701s	
1852/2700 (epoch 34.296), train_loss = 3.27502042, grad/param norm = 1.0657e-01, time/batch = 0.2690s	
1853/2700 (epoch 34.315), train_loss = 3.25126796, grad/param norm = 1.0631e-01, time/batch = 0.2696s	
1854/2700 (epoch 34.333), train_loss = 3.33473633, grad/param norm = 1.1670e-01, time/batch = 0.2695s	
1855/2700 (epoch 34.352), train_loss = 3.33878452, grad/param norm = 1.2866e-01, time/batch = 0.2697s	
1856/2700 (epoch 34.370), train_loss = 3.28088211, grad/param norm = 1.1071e-01, time/batch = 0.2694s	
1857/2700 (epoch 34.389), train_loss = 3.25024025, grad/param norm = 7.8517e-02, time/batch = 0.2699s	
1858/2700 (epoch 34.407), train_loss = 3.27222853, grad/param norm = 9.4557e-02, time/batch = 0.2695s	
1859/2700 (epoch 34.426), train_loss = 3.27373975, grad/param norm = 9.1237e-02, time/batch = 0.2703s	
1860/2700 (epoch 34.444), train_loss = 3.20689617, grad/param norm = 9.0494e-02, time/batch = 0.2699s	
1861/2700 (epoch 34.463), train_loss = 3.24608476, grad/param norm = 1.0891e-01, time/batch = 0.2708s	
1862/2700 (epoch 34.481), train_loss = 3.32523349, grad/param norm = 1.2996e-01, time/batch = 0.2693s	
1863/2700 (epoch 34.500), train_loss = 3.36660471, grad/param norm = 1.6781e-01, time/batch = 0.2689s	
1864/2700 (epoch 34.519), train_loss = 3.32056251, grad/param norm = 1.2978e-01, time/batch = 0.2692s	
1865/2700 (epoch 34.537), train_loss = 3.32423516, grad/param norm = 1.3035e-01, time/batch = 0.2691s	
1866/2700 (epoch 34.556), train_loss = 3.25807934, grad/param norm = 9.0578e-02, time/batch = 0.2694s	
1867/2700 (epoch 34.574), train_loss = 3.23048417, grad/param norm = 1.1281e-01, time/batch = 0.2692s	
1868/2700 (epoch 34.593), train_loss = 3.22585965, grad/param norm = 1.1932e-01, time/batch = 0.2689s	
1869/2700 (epoch 34.611), train_loss = 3.17024818, grad/param norm = 8.1574e-02, time/batch = 0.2696s	
1870/2700 (epoch 34.630), train_loss = 3.21058541, grad/param norm = 9.4337e-02, time/batch = 0.2697s	
1871/2700 (epoch 34.648), train_loss = 3.27534715, grad/param norm = 1.0115e-01, time/batch = 0.2710s	
1872/2700 (epoch 34.667), train_loss = 3.21119086, grad/param norm = 8.2753e-02, time/batch = 0.2690s	
1873/2700 (epoch 34.685), train_loss = 3.20979461, grad/param norm = 1.1310e-01, time/batch = 0.2697s	
1874/2700 (epoch 34.704), train_loss = 3.18135635, grad/param norm = 1.3972e-01, time/batch = 0.2694s	
1875/2700 (epoch 34.722), train_loss = 3.17200108, grad/param norm = 9.4763e-02, time/batch = 0.2699s	
1876/2700 (epoch 34.741), train_loss = 3.30640998, grad/param norm = 1.4874e-01, time/batch = 0.2691s	
1877/2700 (epoch 34.759), train_loss = 3.25234426, grad/param norm = 1.3742e-01, time/batch = 0.2689s	
1878/2700 (epoch 34.778), train_loss = 3.24399457, grad/param norm = 1.1433e-01, time/batch = 0.2691s	
1879/2700 (epoch 34.796), train_loss = 3.23637944, grad/param norm = 1.0688e-01, time/batch = 0.2693s	
1880/2700 (epoch 34.815), train_loss = 3.19022690, grad/param norm = 8.5498e-02, time/batch = 0.2696s	
1881/2700 (epoch 34.833), train_loss = 3.22785859, grad/param norm = 9.6043e-02, time/batch = 0.2710s	
1882/2700 (epoch 34.852), train_loss = 3.21468157, grad/param norm = 9.7623e-02, time/batch = 0.2692s	
1883/2700 (epoch 34.870), train_loss = 3.20920187, grad/param norm = 6.4641e-02, time/batch = 0.2695s	
1884/2700 (epoch 34.889), train_loss = 3.24924213, grad/param norm = 9.2175e-02, time/batch = 0.2706s	
1885/2700 (epoch 34.907), train_loss = 3.30095089, grad/param norm = 1.3256e-01, time/batch = 0.2698s	
1886/2700 (epoch 34.926), train_loss = 3.24810702, grad/param norm = 1.2211e-01, time/batch = 0.2697s	
1887/2700 (epoch 34.944), train_loss = 3.25798338, grad/param norm = 1.0858e-01, time/batch = 0.2697s	
1888/2700 (epoch 34.963), train_loss = 3.33864725, grad/param norm = 1.0297e-01, time/batch = 0.2695s	
1889/2700 (epoch 34.981), train_loss = 3.40181397, grad/param norm = 1.2901e-01, time/batch = 0.2696s	
decayed learning rate by a factor 0.97 to 0.00090593092819348	
1890/2700 (epoch 35.000), train_loss = 3.29528574, grad/param norm = 1.1803e-01, time/batch = 0.2694s	
1891/2700 (epoch 35.019), train_loss = 3.23445761, grad/param norm = 1.3753e-01, time/batch = 0.2709s	
1892/2700 (epoch 35.037), train_loss = 3.25238482, grad/param norm = 1.0282e-01, time/batch = 0.2690s	
1893/2700 (epoch 35.056), train_loss = 3.25928806, grad/param norm = 1.1842e-01, time/batch = 0.2693s	
1894/2700 (epoch 35.074), train_loss = 3.28778172, grad/param norm = 1.5163e-01, time/batch = 0.2700s	
1895/2700 (epoch 35.093), train_loss = 3.29016651, grad/param norm = 1.3623e-01, time/batch = 0.2698s	
1896/2700 (epoch 35.111), train_loss = 3.26236378, grad/param norm = 9.6719e-02, time/batch = 0.2701s	
1897/2700 (epoch 35.130), train_loss = 3.27781236, grad/param norm = 1.0187e-01, time/batch = 0.2692s	
1898/2700 (epoch 35.148), train_loss = 3.24282565, grad/param norm = 1.0801e-01, time/batch = 0.2686s	
1899/2700 (epoch 35.167), train_loss = 3.24926117, grad/param norm = 1.2529e-01, time/batch = 0.2695s	
1900/2700 (epoch 35.185), train_loss = 3.23604396, grad/param norm = 7.8977e-02, time/batch = 0.2696s	
1901/2700 (epoch 35.204), train_loss = 3.17313392, grad/param norm = 9.7133e-02, time/batch = 0.2701s	
1902/2700 (epoch 35.222), train_loss = 3.14726172, grad/param norm = 1.3384e-01, time/batch = 0.2689s	
1903/2700 (epoch 35.241), train_loss = 3.16718088, grad/param norm = 9.6862e-02, time/batch = 0.2689s	
1904/2700 (epoch 35.259), train_loss = 3.20020300, grad/param norm = 1.0377e-01, time/batch = 0.2693s	
1905/2700 (epoch 35.278), train_loss = 3.27307890, grad/param norm = 1.0165e-01, time/batch = 0.2691s	
1906/2700 (epoch 35.296), train_loss = 3.27477326, grad/param norm = 1.0414e-01, time/batch = 0.2690s	
1907/2700 (epoch 35.315), train_loss = 3.25099534, grad/param norm = 1.0351e-01, time/batch = 0.2694s	
1908/2700 (epoch 35.333), train_loss = 3.33457007, grad/param norm = 1.1438e-01, time/batch = 0.2687s	
1909/2700 (epoch 35.352), train_loss = 3.33844881, grad/param norm = 1.2640e-01, time/batch = 0.2697s	
1910/2700 (epoch 35.370), train_loss = 3.28059974, grad/param norm = 1.0887e-01, time/batch = 0.2691s	
1911/2700 (epoch 35.389), train_loss = 3.25015714, grad/param norm = 7.7602e-02, time/batch = 0.2705s	
1912/2700 (epoch 35.407), train_loss = 3.27213712, grad/param norm = 9.4729e-02, time/batch = 0.2692s	
1913/2700 (epoch 35.426), train_loss = 3.27353874, grad/param norm = 9.0101e-02, time/batch = 0.2689s	
1914/2700 (epoch 35.444), train_loss = 3.20693052, grad/param norm = 9.0934e-02, time/batch = 0.2696s	
1915/2700 (epoch 35.463), train_loss = 3.24597622, grad/param norm = 1.0956e-01, time/batch = 0.2692s	
1916/2700 (epoch 35.481), train_loss = 3.32523870, grad/param norm = 1.3083e-01, time/batch = 0.2690s	
1917/2700 (epoch 35.500), train_loss = 3.36625444, grad/param norm = 1.6611e-01, time/batch = 0.2691s	
1918/2700 (epoch 35.519), train_loss = 3.32030447, grad/param norm = 1.2627e-01, time/batch = 0.2689s	
1919/2700 (epoch 35.537), train_loss = 3.32405558, grad/param norm = 1.2795e-01, time/batch = 0.2691s	
1920/2700 (epoch 35.556), train_loss = 3.25784683, grad/param norm = 8.8628e-02, time/batch = 0.2696s	
1921/2700 (epoch 35.574), train_loss = 3.23043970, grad/param norm = 1.1281e-01, time/batch = 0.2705s	
1922/2700 (epoch 35.593), train_loss = 3.22559043, grad/param norm = 1.1774e-01, time/batch = 0.2690s	
1923/2700 (epoch 35.611), train_loss = 3.17027727, grad/param norm = 8.1761e-02, time/batch = 0.2689s	
1924/2700 (epoch 35.630), train_loss = 3.21050092, grad/param norm = 9.3165e-02, time/batch = 0.2704s	
1925/2700 (epoch 35.648), train_loss = 3.27502946, grad/param norm = 1.0026e-01, time/batch = 0.2698s	
1926/2700 (epoch 35.667), train_loss = 3.21107258, grad/param norm = 8.1833e-02, time/batch = 0.2692s	
1927/2700 (epoch 35.685), train_loss = 3.20970538, grad/param norm = 1.1283e-01, time/batch = 0.2695s	
1928/2700 (epoch 35.704), train_loss = 3.18118540, grad/param norm = 1.3828e-01, time/batch = 0.2687s	
1929/2700 (epoch 35.722), train_loss = 3.17180209, grad/param norm = 9.1792e-02, time/batch = 0.2693s	
1930/2700 (epoch 35.741), train_loss = 3.30613607, grad/param norm = 1.4600e-01, time/batch = 0.2696s	
1931/2700 (epoch 35.759), train_loss = 3.25203080, grad/param norm = 1.3504e-01, time/batch = 0.2702s	
1932/2700 (epoch 35.778), train_loss = 3.24375023, grad/param norm = 1.1263e-01, time/batch = 0.2690s	
1933/2700 (epoch 35.796), train_loss = 3.23616119, grad/param norm = 1.0520e-01, time/batch = 0.2691s	
1934/2700 (epoch 35.815), train_loss = 3.19009616, grad/param norm = 8.4959e-02, time/batch = 0.2698s	
1935/2700 (epoch 35.833), train_loss = 3.22770351, grad/param norm = 9.5424e-02, time/batch = 0.2696s	
1936/2700 (epoch 35.852), train_loss = 3.21451509, grad/param norm = 9.6550e-02, time/batch = 0.2693s	
1937/2700 (epoch 35.870), train_loss = 3.20909208, grad/param norm = 6.3710e-02, time/batch = 0.2691s	
1938/2700 (epoch 35.889), train_loss = 3.24913501, grad/param norm = 9.1445e-02, time/batch = 0.2686s	
1939/2700 (epoch 35.907), train_loss = 3.30077775, grad/param norm = 1.3129e-01, time/batch = 0.2694s	
1940/2700 (epoch 35.926), train_loss = 3.24792128, grad/param norm = 1.2083e-01, time/batch = 0.2695s	
1941/2700 (epoch 35.944), train_loss = 3.25774427, grad/param norm = 1.0715e-01, time/batch = 0.2699s	
1942/2700 (epoch 35.963), train_loss = 3.33852257, grad/param norm = 1.0193e-01, time/batch = 0.2686s	
1943/2700 (epoch 35.981), train_loss = 3.40181147, grad/param norm = 1.2892e-01, time/batch = 0.2690s	
decayed learning rate by a factor 0.97 to 0.00087875300034768	
1944/2700 (epoch 36.000), train_loss = 3.29497007, grad/param norm = 1.1658e-01, time/batch = 0.2694s	
1945/2700 (epoch 36.019), train_loss = 3.23405990, grad/param norm = 1.3645e-01, time/batch = 0.2696s	
1946/2700 (epoch 36.037), train_loss = 3.25215374, grad/param norm = 1.0131e-01, time/batch = 0.2688s	
1947/2700 (epoch 36.056), train_loss = 3.25928705, grad/param norm = 1.1866e-01, time/batch = 0.2694s	
1948/2700 (epoch 36.074), train_loss = 3.28754467, grad/param norm = 1.5038e-01, time/batch = 0.2690s	
1949/2700 (epoch 36.093), train_loss = 3.28981552, grad/param norm = 1.3353e-01, time/batch = 0.2690s	
1950/2700 (epoch 36.111), train_loss = 3.26223794, grad/param norm = 9.5166e-02, time/batch = 0.2690s	
1951/2700 (epoch 36.130), train_loss = 3.27765243, grad/param norm = 1.0156e-01, time/batch = 0.2707s	
1952/2700 (epoch 36.148), train_loss = 3.24271245, grad/param norm = 1.0706e-01, time/batch = 0.2689s	
1953/2700 (epoch 36.167), train_loss = 3.24902500, grad/param norm = 1.2358e-01, time/batch = 0.2690s	
1954/2700 (epoch 36.185), train_loss = 3.23593283, grad/param norm = 7.8523e-02, time/batch = 0.2693s	
1955/2700 (epoch 36.204), train_loss = 3.17304963, grad/param norm = 9.6486e-02, time/batch = 0.2694s	
1956/2700 (epoch 36.222), train_loss = 3.14709379, grad/param norm = 1.3295e-01, time/batch = 0.2691s	
1957/2700 (epoch 36.241), train_loss = 3.16711696, grad/param norm = 9.6371e-02, time/batch = 0.2689s	
1958/2700 (epoch 36.259), train_loss = 3.20009322, grad/param norm = 1.0329e-01, time/batch = 0.2688s	
1959/2700 (epoch 36.278), train_loss = 3.27292352, grad/param norm = 1.0056e-01, time/batch = 0.2701s	
1960/2700 (epoch 36.296), train_loss = 3.27454313, grad/param norm = 1.0192e-01, time/batch = 0.2696s	
1961/2700 (epoch 36.315), train_loss = 3.25074334, grad/param norm = 1.0097e-01, time/batch = 0.2707s	
1962/2700 (epoch 36.333), train_loss = 3.33442381, grad/param norm = 1.1231e-01, time/batch = 0.2696s	
1963/2700 (epoch 36.352), train_loss = 3.33812550, grad/param norm = 1.2426e-01, time/batch = 0.2695s	
1964/2700 (epoch 36.370), train_loss = 3.28032926, grad/param norm = 1.0711e-01, time/batch = 0.2700s	
1965/2700 (epoch 36.389), train_loss = 3.25008329, grad/param norm = 7.6815e-02, time/batch = 0.2693s	
1966/2700 (epoch 36.407), train_loss = 3.27205012, grad/param norm = 9.4942e-02, time/batch = 0.2694s	
1967/2700 (epoch 36.426), train_loss = 3.27334173, grad/param norm = 8.8998e-02, time/batch = 0.2692s	
1968/2700 (epoch 36.444), train_loss = 3.20696708, grad/param norm = 9.1426e-02, time/batch = 0.2694s	
1969/2700 (epoch 36.463), train_loss = 3.24587084, grad/param norm = 1.1017e-01, time/batch = 0.2695s	
1970/2700 (epoch 36.481), train_loss = 3.32522873, grad/param norm = 1.3139e-01, time/batch = 0.2696s	
1971/2700 (epoch 36.500), train_loss = 3.36589485, grad/param norm = 1.6402e-01, time/batch = 0.2702s	
1972/2700 (epoch 36.519), train_loss = 3.32005571, grad/param norm = 1.2265e-01, time/batch = 0.2689s	
1973/2700 (epoch 36.537), train_loss = 3.32389155, grad/param norm = 1.2572e-01, time/batch = 0.2694s	
1974/2700 (epoch 36.556), train_loss = 3.25763076, grad/param norm = 8.6888e-02, time/batch = 0.2704s	
1975/2700 (epoch 36.574), train_loss = 3.23039886, grad/param norm = 1.1284e-01, time/batch = 0.2690s	
1976/2700 (epoch 36.593), train_loss = 3.22532730, grad/param norm = 1.1617e-01, time/batch = 0.2695s	
1977/2700 (epoch 36.611), train_loss = 3.17030882, grad/param norm = 8.1969e-02, time/batch = 0.2707s	
1978/2700 (epoch 36.630), train_loss = 3.21041881, grad/param norm = 9.2002e-02, time/batch = 0.2691s	
1979/2700 (epoch 36.648), train_loss = 3.27472489, grad/param norm = 9.9423e-02, time/batch = 0.2700s	
1980/2700 (epoch 36.667), train_loss = 3.21096513, grad/param norm = 8.0932e-02, time/batch = 0.2695s	
1981/2700 (epoch 36.685), train_loss = 3.20961586, grad/param norm = 1.1253e-01, time/batch = 0.2703s	
1982/2700 (epoch 36.704), train_loss = 3.18101866, grad/param norm = 1.3684e-01, time/batch = 0.2696s	
1983/2700 (epoch 36.722), train_loss = 3.17161814, grad/param norm = 8.8953e-02, time/batch = 0.2690s	
1984/2700 (epoch 36.741), train_loss = 3.30587734, grad/param norm = 1.4351e-01, time/batch = 0.2707s	
1985/2700 (epoch 36.759), train_loss = 3.25173604, grad/param norm = 1.3281e-01, time/batch = 0.2692s	
1986/2700 (epoch 36.778), train_loss = 3.24351608, grad/param norm = 1.1099e-01, time/batch = 0.2699s	
1987/2700 (epoch 36.796), train_loss = 3.23595821, grad/param norm = 1.0361e-01, time/batch = 0.2694s	
1988/2700 (epoch 36.815), train_loss = 3.18997537, grad/param norm = 8.4469e-02, time/batch = 0.2692s	
1989/2700 (epoch 36.833), train_loss = 3.22755442, grad/param norm = 9.4790e-02, time/batch = 0.2696s	
1990/2700 (epoch 36.852), train_loss = 3.21435462, grad/param norm = 9.5478e-02, time/batch = 0.2692s	
1991/2700 (epoch 36.870), train_loss = 3.20899145, grad/param norm = 6.2819e-02, time/batch = 0.2700s	
1992/2700 (epoch 36.889), train_loss = 3.24902934, grad/param norm = 9.0729e-02, time/batch = 0.2694s	
1993/2700 (epoch 36.907), train_loss = 3.30060942, grad/param norm = 1.3006e-01, time/batch = 0.2692s	
1994/2700 (epoch 36.926), train_loss = 3.24775003, grad/param norm = 1.1963e-01, time/batch = 0.2694s	
1995/2700 (epoch 36.944), train_loss = 3.25751404, grad/param norm = 1.0576e-01, time/batch = 0.2693s	
1996/2700 (epoch 36.963), train_loss = 3.33840525, grad/param norm = 1.0097e-01, time/batch = 0.2692s	
1997/2700 (epoch 36.981), train_loss = 3.40180669, grad/param norm = 1.2881e-01, time/batch = 0.2694s	
decayed learning rate by a factor 0.97 to 0.00085239041033725	
1998/2700 (epoch 37.000), train_loss = 3.29466558, grad/param norm = 1.1515e-01, time/batch = 0.2687s	
1999/2700 (epoch 37.019), train_loss = 3.23367045, grad/param norm = 1.3541e-01, time/batch = 0.2700s	
evaluating loss over split index 2	
1/3...	
2/3...	
3/3...	
saving checkpoint to cv/lm_lstm_epoch37.04_3.2048.t7	
2000/2700 (epoch 37.037), train_loss = 3.25193289, grad/param norm = 9.9856e-02, time/batch = 0.2699s	
2001/2700 (epoch 37.056), train_loss = 3.25928008, grad/param norm = 1.1878e-01, time/batch = 0.2707s	
2002/2700 (epoch 37.074), train_loss = 3.28730996, grad/param norm = 1.4901e-01, time/batch = 0.2698s	
2003/2700 (epoch 37.093), train_loss = 3.28947903, grad/param norm = 1.3089e-01, time/batch = 0.2691s	
2004/2700 (epoch 37.111), train_loss = 3.26212357, grad/param norm = 9.3764e-02, time/batch = 0.2699s	
2005/2700 (epoch 37.130), train_loss = 3.27749721, grad/param norm = 1.0126e-01, time/batch = 0.2697s	
2006/2700 (epoch 37.148), train_loss = 3.24260447, grad/param norm = 1.0616e-01, time/batch = 0.2692s	
2007/2700 (epoch 37.167), train_loss = 3.24880144, grad/param norm = 1.2194e-01, time/batch = 0.2701s	
2008/2700 (epoch 37.185), train_loss = 3.23582918, grad/param norm = 7.8120e-02, time/batch = 0.2695s	
2009/2700 (epoch 37.204), train_loss = 3.17296922, grad/param norm = 9.5890e-02, time/batch = 0.2694s	
2010/2700 (epoch 37.222), train_loss = 3.14693264, grad/param norm = 1.3212e-01, time/batch = 0.2692s	
2011/2700 (epoch 37.241), train_loss = 3.16705707, grad/param norm = 9.5950e-02, time/batch = 0.2698s	
2012/2700 (epoch 37.259), train_loss = 3.19998734, grad/param norm = 1.0282e-01, time/batch = 0.2694s	
2013/2700 (epoch 37.278), train_loss = 3.27277913, grad/param norm = 9.9578e-02, time/batch = 0.2693s	
2014/2700 (epoch 37.296), train_loss = 3.27432844, grad/param norm = 9.9875e-02, time/batch = 0.2696s	
2015/2700 (epoch 37.315), train_loss = 3.25050761, grad/param norm = 9.8658e-02, time/batch = 0.2697s	
2016/2700 (epoch 37.333), train_loss = 3.33429366, grad/param norm = 1.1047e-01, time/batch = 0.2690s	
2017/2700 (epoch 37.352), train_loss = 3.33781608, grad/param norm = 1.2223e-01, time/batch = 0.2694s	
2018/2700 (epoch 37.370), train_loss = 3.28007146, grad/param norm = 1.0542e-01, time/batch = 0.2688s	
2019/2700 (epoch 37.389), train_loss = 3.25001651, grad/param norm = 7.6153e-02, time/batch = 0.2693s	
2020/2700 (epoch 37.407), train_loss = 3.27196838, grad/param norm = 9.5178e-02, time/batch = 0.2701s	
2021/2700 (epoch 37.426), train_loss = 3.27315175, grad/param norm = 8.7932e-02, time/batch = 0.2692s	
2022/2700 (epoch 37.444), train_loss = 3.20700333, grad/param norm = 9.1932e-02, time/batch = 0.2690s	
2023/2700 (epoch 37.463), train_loss = 3.24576331, grad/param norm = 1.1067e-01, time/batch = 0.2690s	
2024/2700 (epoch 37.481), train_loss = 3.32520305, grad/param norm = 1.3159e-01, time/batch = 0.2701s	
2025/2700 (epoch 37.500), train_loss = 3.36553064, grad/param norm = 1.6163e-01, time/batch = 0.2692s	
2026/2700 (epoch 37.519), train_loss = 3.31982080, grad/param norm = 1.1905e-01, time/batch = 0.2695s	
2027/2700 (epoch 37.537), train_loss = 3.32374310, grad/param norm = 1.2368e-01, time/batch = 0.2692s	
2028/2700 (epoch 37.556), train_loss = 3.25743012, grad/param norm = 8.5342e-02, time/batch = 0.2693s	
2029/2700 (epoch 37.574), train_loss = 3.23035868, grad/param norm = 1.1290e-01, time/batch = 0.2690s	
2030/2700 (epoch 37.593), train_loss = 3.22506991, grad/param norm = 1.1461e-01, time/batch = 0.2693s	
2031/2700 (epoch 37.611), train_loss = 3.17034087, grad/param norm = 8.2190e-02, time/batch = 0.2692s	
2032/2700 (epoch 37.630), train_loss = 3.21033892, grad/param norm = 9.0848e-02, time/batch = 0.2689s	
2033/2700 (epoch 37.648), train_loss = 3.27443475, grad/param norm = 9.8628e-02, time/batch = 0.2698s	
2034/2700 (epoch 37.667), train_loss = 3.21086591, grad/param norm = 8.0062e-02, time/batch = 0.2703s	
2035/2700 (epoch 37.685), train_loss = 3.20952594, grad/param norm = 1.1222e-01, time/batch = 0.2702s	
2036/2700 (epoch 37.704), train_loss = 3.18085873, grad/param norm = 1.3541e-01, time/batch = 0.2692s	
2037/2700 (epoch 37.722), train_loss = 3.17144851, grad/param norm = 8.6265e-02, time/batch = 0.2698s	
2038/2700 (epoch 37.741), train_loss = 3.30563280, grad/param norm = 1.4125e-01, time/batch = 0.2696s	
2039/2700 (epoch 37.759), train_loss = 3.25145786, grad/param norm = 1.3074e-01, time/batch = 0.2692s	
2040/2700 (epoch 37.778), train_loss = 3.24329095, grad/param norm = 1.0940e-01, time/batch = 0.2700s	
2041/2700 (epoch 37.796), train_loss = 3.23576939, grad/param norm = 1.0209e-01, time/batch = 0.2702s	
2042/2700 (epoch 37.815), train_loss = 3.18986311, grad/param norm = 8.4019e-02, time/batch = 0.2686s	
2043/2700 (epoch 37.833), train_loss = 3.22740979, grad/param norm = 9.4142e-02, time/batch = 0.2693s	
2044/2700 (epoch 37.852), train_loss = 3.21420218, grad/param norm = 9.4409e-02, time/batch = 0.2693s	
2045/2700 (epoch 37.870), train_loss = 3.20889984, grad/param norm = 6.1966e-02, time/batch = 0.2691s	
2046/2700 (epoch 37.889), train_loss = 3.24892677, grad/param norm = 9.0026e-02, time/batch = 0.2693s	
2047/2700 (epoch 37.907), train_loss = 3.30044580, grad/param norm = 1.2886e-01, time/batch = 0.2692s	
2048/2700 (epoch 37.926), train_loss = 3.24759167, grad/param norm = 1.1848e-01, time/batch = 0.2687s	
2049/2700 (epoch 37.944), train_loss = 3.25729240, grad/param norm = 1.0440e-01, time/batch = 0.2690s	
2050/2700 (epoch 37.963), train_loss = 3.33829512, grad/param norm = 1.0009e-01, time/batch = 0.2693s	
2051/2700 (epoch 37.981), train_loss = 3.40180015, grad/param norm = 1.2869e-01, time/batch = 0.2703s	
decayed learning rate by a factor 0.97 to 0.00082681869802713	
2052/2700 (epoch 38.000), train_loss = 3.29437031, grad/param norm = 1.1375e-01, time/batch = 0.2690s	
2053/2700 (epoch 38.019), train_loss = 3.23328763, grad/param norm = 1.3440e-01, time/batch = 0.2692s	
2054/2700 (epoch 38.037), train_loss = 3.25172128, grad/param norm = 9.8453e-02, time/batch = 0.2694s	
2055/2700 (epoch 38.056), train_loss = 3.25926663, grad/param norm = 1.1881e-01, time/batch = 0.2693s	
2056/2700 (epoch 38.074), train_loss = 3.28707757, grad/param norm = 1.4756e-01, time/batch = 0.2704s	
2057/2700 (epoch 38.093), train_loss = 3.28915631, grad/param norm = 1.2832e-01, time/batch = 0.2697s	
2058/2700 (epoch 38.111), train_loss = 3.26201827, grad/param norm = 9.2493e-02, time/batch = 0.2690s	
2059/2700 (epoch 38.130), train_loss = 3.27734643, grad/param norm = 1.0095e-01, time/batch = 0.2696s	
2060/2700 (epoch 38.148), train_loss = 3.24250241, grad/param norm = 1.0532e-01, time/batch = 0.2699s	
2061/2700 (epoch 38.167), train_loss = 3.24858915, grad/param norm = 1.2036e-01, time/batch = 0.2704s	
2062/2700 (epoch 38.185), train_loss = 3.23573169, grad/param norm = 7.7762e-02, time/batch = 0.2686s	
2063/2700 (epoch 38.204), train_loss = 3.17289194, grad/param norm = 9.5340e-02, time/batch = 0.2691s	
2064/2700 (epoch 38.222), train_loss = 3.14677641, grad/param norm = 1.3133e-01, time/batch = 0.2697s	
2065/2700 (epoch 38.241), train_loss = 3.16700202, grad/param norm = 9.5583e-02, time/batch = 0.2704s	
2066/2700 (epoch 38.259), train_loss = 3.19988531, grad/param norm = 1.0236e-01, time/batch = 0.2703s	
2067/2700 (epoch 38.278), train_loss = 3.27264598, grad/param norm = 9.8706e-02, time/batch = 0.2693s	
2068/2700 (epoch 38.296), train_loss = 3.27412777, grad/param norm = 9.7978e-02, time/batch = 0.2689s	
2069/2700 (epoch 38.315), train_loss = 3.25028773, grad/param norm = 9.6544e-02, time/batch = 0.2693s	
2070/2700 (epoch 38.333), train_loss = 3.33417764, grad/param norm = 1.0885e-01, time/batch = 0.2693s	
2071/2700 (epoch 38.352), train_loss = 3.33751783, grad/param norm = 1.2032e-01, time/batch = 0.2703s	
2072/2700 (epoch 38.370), train_loss = 3.27982593, grad/param norm = 1.0381e-01, time/batch = 0.2685s	
2073/2700 (epoch 38.389), train_loss = 3.24995758, grad/param norm = 7.5611e-02, time/batch = 0.2698s	
2074/2700 (epoch 38.407), train_loss = 3.27189042, grad/param norm = 9.5424e-02, time/batch = 0.2697s	
2075/2700 (epoch 38.426), train_loss = 3.27296718, grad/param norm = 8.6899e-02, time/batch = 0.2701s	
2076/2700 (epoch 38.444), train_loss = 3.20703771, grad/param norm = 9.2416e-02, time/batch = 0.2691s	
2077/2700 (epoch 38.463), train_loss = 3.24565198, grad/param norm = 1.1098e-01, time/batch = 0.2694s	
2078/2700 (epoch 38.481), train_loss = 3.32515872, grad/param norm = 1.3143e-01, time/batch = 0.2697s	
2079/2700 (epoch 38.500), train_loss = 3.36516745, grad/param norm = 1.5902e-01, time/batch = 0.2696s	
2080/2700 (epoch 38.519), train_loss = 3.31960459, grad/param norm = 1.1556e-01, time/batch = 0.2697s	
2081/2700 (epoch 38.537), train_loss = 3.32361017, grad/param norm = 1.2182e-01, time/batch = 0.2702s	
2082/2700 (epoch 38.556), train_loss = 3.25724507, grad/param norm = 8.3968e-02, time/batch = 0.2691s	
2083/2700 (epoch 38.574), train_loss = 3.23031915, grad/param norm = 1.1297e-01, time/batch = 0.2706s	
2084/2700 (epoch 38.593), train_loss = 3.22481882, grad/param norm = 1.1306e-01, time/batch = 0.2708s	
2085/2700 (epoch 38.611), train_loss = 3.17037376, grad/param norm = 8.2422e-02, time/batch = 0.2700s	
2086/2700 (epoch 38.630), train_loss = 3.21026253, grad/param norm = 8.9703e-02, time/batch = 0.2704s	
2087/2700 (epoch 38.648), train_loss = 3.27415839, grad/param norm = 9.7876e-02, time/batch = 0.2702s	
2088/2700 (epoch 38.667), train_loss = 3.21077670, grad/param norm = 7.9225e-02, time/batch = 0.2687s	
2089/2700 (epoch 38.685), train_loss = 3.20943595, grad/param norm = 1.1192e-01, time/batch = 0.2695s	
2090/2700 (epoch 38.704), train_loss = 3.18070487, grad/param norm = 1.3401e-01, time/batch = 0.2698s	
2091/2700 (epoch 38.722), train_loss = 3.17129323, grad/param norm = 8.3735e-02, time/batch = 0.2698s	
2092/2700 (epoch 38.741), train_loss = 3.30540072, grad/param norm = 1.3920e-01, time/batch = 0.2690s	
2093/2700 (epoch 38.759), train_loss = 3.25119507, grad/param norm = 1.2878e-01, time/batch = 0.2693s	
2094/2700 (epoch 38.778), train_loss = 3.24307519, grad/param norm = 1.0786e-01, time/batch = 0.2704s	
2095/2700 (epoch 38.796), train_loss = 3.23559360, grad/param norm = 1.0066e-01, time/batch = 0.2698s	
2096/2700 (epoch 38.815), train_loss = 3.18975848, grad/param norm = 8.3604e-02, time/batch = 0.2698s	
2097/2700 (epoch 38.833), train_loss = 3.22727043, grad/param norm = 9.3483e-02, time/batch = 0.2696s	
2098/2700 (epoch 38.852), train_loss = 3.21405608, grad/param norm = 9.3345e-02, time/batch = 0.2693s	
2099/2700 (epoch 38.870), train_loss = 3.20881713, grad/param norm = 6.1155e-02, time/batch = 0.2700s	
2100/2700 (epoch 38.889), train_loss = 3.24882589, grad/param norm = 8.9342e-02, time/batch = 0.2696s	
2101/2700 (epoch 38.907), train_loss = 3.30028884, grad/param norm = 1.2769e-01, time/batch = 0.2698s	
2102/2700 (epoch 38.926), train_loss = 3.24744518, grad/param norm = 1.1740e-01, time/batch = 0.2692s	
2103/2700 (epoch 38.944), train_loss = 3.25707801, grad/param norm = 1.0307e-01, time/batch = 0.2695s	
2104/2700 (epoch 38.963), train_loss = 3.33819133, grad/param norm = 9.9278e-02, time/batch = 0.2698s	
2105/2700 (epoch 38.981), train_loss = 3.40179119, grad/param norm = 1.2855e-01, time/batch = 0.2699s	
decayed learning rate by a factor 0.97 to 0.00080201413708631	
2106/2700 (epoch 39.000), train_loss = 3.29408472, grad/param norm = 1.1236e-01, time/batch = 0.2698s	
2107/2700 (epoch 39.019), train_loss = 3.23291318, grad/param norm = 1.3341e-01, time/batch = 0.2688s	
2108/2700 (epoch 39.037), train_loss = 3.25151811, grad/param norm = 9.7102e-02, time/batch = 0.2695s	
2109/2700 (epoch 39.056), train_loss = 3.25924722, grad/param norm = 1.1874e-01, time/batch = 0.2700s	
2110/2700 (epoch 39.074), train_loss = 3.28684911, grad/param norm = 1.4603e-01, time/batch = 0.2693s	
2111/2700 (epoch 39.093), train_loss = 3.28884871, grad/param norm = 1.2583e-01, time/batch = 0.2705s	
2112/2700 (epoch 39.111), train_loss = 3.26192270, grad/param norm = 9.1338e-02, time/batch = 0.2688s	
2113/2700 (epoch 39.130), train_loss = 3.27719804, grad/param norm = 1.0064e-01, time/batch = 0.2699s	
2114/2700 (epoch 39.148), train_loss = 3.24240559, grad/param norm = 1.0452e-01, time/batch = 0.2695s	
2115/2700 (epoch 39.167), train_loss = 3.24838730, grad/param norm = 1.1884e-01, time/batch = 0.2693s	
2116/2700 (epoch 39.185), train_loss = 3.23564070, grad/param norm = 7.7445e-02, time/batch = 0.2698s	
2117/2700 (epoch 39.204), train_loss = 3.17281723, grad/param norm = 9.4829e-02, time/batch = 0.2694s	
2118/2700 (epoch 39.222), train_loss = 3.14662448, grad/param norm = 1.3057e-01, time/batch = 0.2689s	
2119/2700 (epoch 39.241), train_loss = 3.16694960, grad/param norm = 9.5261e-02, time/batch = 0.2693s	
2120/2700 (epoch 39.259), train_loss = 3.19978706, grad/param norm = 1.0188e-01, time/batch = 0.2693s	
2121/2700 (epoch 39.278), train_loss = 3.27252078, grad/param norm = 9.7933e-02, time/batch = 0.2703s	
2122/2700 (epoch 39.296), train_loss = 3.27393965, grad/param norm = 9.6213e-02, time/batch = 0.2690s	
2123/2700 (epoch 39.315), train_loss = 3.25008175, grad/param norm = 9.4605e-02, time/batch = 0.2693s	
2124/2700 (epoch 39.333), train_loss = 3.33407446, grad/param norm = 1.0741e-01, time/batch = 0.2694s	
2125/2700 (epoch 39.352), train_loss = 3.33723000, grad/param norm = 1.1851e-01, time/batch = 0.2697s	
2126/2700 (epoch 39.370), train_loss = 3.27959254, grad/param norm = 1.0228e-01, time/batch = 0.2696s	
2127/2700 (epoch 39.389), train_loss = 3.24990615, grad/param norm = 7.5175e-02, time/batch = 0.2690s	
2128/2700 (epoch 39.407), train_loss = 3.27181531, grad/param norm = 9.5662e-02, time/batch = 0.2692s	
2129/2700 (epoch 39.426), train_loss = 3.27278779, grad/param norm = 8.5896e-02, time/batch = 0.2698s	
2130/2700 (epoch 39.444), train_loss = 3.20707019, grad/param norm = 9.2848e-02, time/batch = 0.2692s	
2131/2700 (epoch 39.463), train_loss = 3.24553555, grad/param norm = 1.1108e-01, time/batch = 0.2701s	
2132/2700 (epoch 39.481), train_loss = 3.32509845, grad/param norm = 1.3093e-01, time/batch = 0.2697s	
2133/2700 (epoch 39.500), train_loss = 3.36481101, grad/param norm = 1.5629e-01, time/batch = 0.2693s	
2134/2700 (epoch 39.519), train_loss = 3.31940804, grad/param norm = 1.1225e-01, time/batch = 0.2696s	
2135/2700 (epoch 39.537), train_loss = 3.32349044, grad/param norm = 1.2014e-01, time/batch = 0.2694s	
2136/2700 (epoch 39.556), train_loss = 3.25707223, grad/param norm = 8.2739e-02, time/batch = 0.2697s	
2137/2700 (epoch 39.574), train_loss = 3.23028130, grad/param norm = 1.1304e-01, time/batch = 0.2689s	
2138/2700 (epoch 39.593), train_loss = 3.22457503, grad/param norm = 1.1153e-01, time/batch = 0.2688s	
2139/2700 (epoch 39.611), train_loss = 3.17040581, grad/param norm = 8.2664e-02, time/batch = 0.2693s	
2140/2700 (epoch 39.630), train_loss = 3.21018759, grad/param norm = 8.8567e-02, time/batch = 0.2691s	
2141/2700 (epoch 39.648), train_loss = 3.27389499, grad/param norm = 9.7166e-02, time/batch = 0.2707s	
2142/2700 (epoch 39.667), train_loss = 3.21069627, grad/param norm = 7.8426e-02, time/batch = 0.2696s	
2143/2700 (epoch 39.685), train_loss = 3.20934613, grad/param norm = 1.1163e-01, time/batch = 0.2701s	
2144/2700 (epoch 39.704), train_loss = 3.18055717, grad/param norm = 1.3263e-01, time/batch = 0.2704s	
2145/2700 (epoch 39.722), train_loss = 3.17115173, grad/param norm = 8.1353e-02, time/batch = 0.2697s	
2146/2700 (epoch 39.741), train_loss = 3.30517834, grad/param norm = 1.3733e-01, time/batch = 0.2693s	
2147/2700 (epoch 39.759), train_loss = 3.25094511, grad/param norm = 1.2694e-01, time/batch = 0.2697s	
2148/2700 (epoch 39.778), train_loss = 3.24286757, grad/param norm = 1.0638e-01, time/batch = 0.2689s	
2149/2700 (epoch 39.796), train_loss = 3.23543103, grad/param norm = 9.9305e-02, time/batch = 0.2701s	
2150/2700 (epoch 39.815), train_loss = 3.18966282, grad/param norm = 8.3217e-02, time/batch = 0.2699s	
2151/2700 (epoch 39.833), train_loss = 3.22713598, grad/param norm = 9.2816e-02, time/batch = 0.2704s	
2152/2700 (epoch 39.852), train_loss = 3.21391599, grad/param norm = 9.2289e-02, time/batch = 0.2687s	
2153/2700 (epoch 39.870), train_loss = 3.20874231, grad/param norm = 6.0387e-02, time/batch = 0.2690s	
2154/2700 (epoch 39.889), train_loss = 3.24872759, grad/param norm = 8.8680e-02, time/batch = 0.2693s	
2155/2700 (epoch 39.907), train_loss = 3.30013564, grad/param norm = 1.2655e-01, time/batch = 0.2697s	
2156/2700 (epoch 39.926), train_loss = 3.24730976, grad/param norm = 1.1638e-01, time/batch = 0.2690s	
2157/2700 (epoch 39.944), train_loss = 3.25687241, grad/param norm = 1.0179e-01, time/batch = 0.2697s	
2158/2700 (epoch 39.963), train_loss = 3.33809404, grad/param norm = 9.8535e-02, time/batch = 0.2690s	
2159/2700 (epoch 39.981), train_loss = 3.40177950, grad/param norm = 1.2838e-01, time/batch = 0.2692s	
decayed learning rate by a factor 0.97 to 0.00077795371297373	
2160/2700 (epoch 40.000), train_loss = 3.29381077, grad/param norm = 1.1101e-01, time/batch = 0.2692s	
2161/2700 (epoch 40.019), train_loss = 3.23254420, grad/param norm = 1.3244e-01, time/batch = 0.2704s	
2162/2700 (epoch 40.037), train_loss = 3.25132510, grad/param norm = 9.5801e-02, time/batch = 0.2688s	
2163/2700 (epoch 40.056), train_loss = 3.25922182, grad/param norm = 1.1859e-01, time/batch = 0.2692s	
2164/2700 (epoch 40.074), train_loss = 3.28662554, grad/param norm = 1.4445e-01, time/batch = 0.2694s	
2165/2700 (epoch 40.093), train_loss = 3.28855469, grad/param norm = 1.2341e-01, time/batch = 0.2695s	
2166/2700 (epoch 40.111), train_loss = 3.26183431, grad/param norm = 9.0285e-02, time/batch = 0.2691s	
2167/2700 (epoch 40.130), train_loss = 3.27705374, grad/param norm = 1.0031e-01, time/batch = 0.2690s	
2168/2700 (epoch 40.148), train_loss = 3.24231378, grad/param norm = 1.0378e-01, time/batch = 0.2693s	
2169/2700 (epoch 40.167), train_loss = 3.24819399, grad/param norm = 1.1736e-01, time/batch = 0.2698s	
2170/2700 (epoch 40.185), train_loss = 3.23555464, grad/param norm = 7.7163e-02, time/batch = 0.2690s	
2171/2700 (epoch 40.204), train_loss = 3.17274644, grad/param norm = 9.4356e-02, time/batch = 0.2700s	
2172/2700 (epoch 40.222), train_loss = 3.14647679, grad/param norm = 1.2983e-01, time/batch = 0.2685s	
2173/2700 (epoch 40.241), train_loss = 3.16690113, grad/param norm = 9.4978e-02, time/batch = 0.2695s	
2174/2700 (epoch 40.259), train_loss = 3.19969184, grad/param norm = 1.0139e-01, time/batch = 0.2696s	
2175/2700 (epoch 40.278), train_loss = 3.27240448, grad/param norm = 9.7252e-02, time/batch = 0.2691s	
2176/2700 (epoch 40.296), train_loss = 3.27376245, grad/param norm = 9.4568e-02, time/batch = 0.2694s	
2177/2700 (epoch 40.315), train_loss = 3.24988581, grad/param norm = 9.2824e-02, time/batch = 0.2690s	
2178/2700 (epoch 40.333), train_loss = 3.33398392, grad/param norm = 1.0615e-01, time/batch = 0.2695s	
2179/2700 (epoch 40.352), train_loss = 3.33695242, grad/param norm = 1.1680e-01, time/batch = 0.2691s	
2180/2700 (epoch 40.370), train_loss = 3.27937008, grad/param norm = 1.0083e-01, time/batch = 0.2694s	
2181/2700 (epoch 40.389), train_loss = 3.24986062, grad/param norm = 7.4834e-02, time/batch = 0.2701s	
2182/2700 (epoch 40.407), train_loss = 3.27174257, grad/param norm = 9.5877e-02, time/batch = 0.2685s	
2183/2700 (epoch 40.426), train_loss = 3.27261531, grad/param norm = 8.4913e-02, time/batch = 0.2696s	
2184/2700 (epoch 40.444), train_loss = 3.20709763, grad/param norm = 9.3200e-02, time/batch = 0.2693s	
2185/2700 (epoch 40.463), train_loss = 3.24541345, grad/param norm = 1.1093e-01, time/batch = 0.2696s	
2186/2700 (epoch 40.481), train_loss = 3.32502398, grad/param norm = 1.3014e-01, time/batch = 0.2692s	
2187/2700 (epoch 40.500), train_loss = 3.36446643, grad/param norm = 1.5352e-01, time/batch = 0.2700s	
2188/2700 (epoch 40.519), train_loss = 3.31923400, grad/param norm = 1.0914e-01, time/batch = 0.2689s	
2189/2700 (epoch 40.537), train_loss = 3.32338274, grad/param norm = 1.1860e-01, time/batch = 0.2691s	
2190/2700 (epoch 40.556), train_loss = 3.25691287, grad/param norm = 8.1635e-02, time/batch = 0.2693s	
2191/2700 (epoch 40.574), train_loss = 3.23024368, grad/param norm = 1.1311e-01, time/batch = 0.2701s	
2192/2700 (epoch 40.593), train_loss = 3.22433748, grad/param norm = 1.1001e-01, time/batch = 0.2689s	
2193/2700 (epoch 40.611), train_loss = 3.17043678, grad/param norm = 8.2915e-02, time/batch = 0.2692s	
2194/2700 (epoch 40.630), train_loss = 3.21011520, grad/param norm = 8.7441e-02, time/batch = 0.2698s	
2195/2700 (epoch 40.648), train_loss = 3.27364398, grad/param norm = 9.6497e-02, time/batch = 0.2697s	
2196/2700 (epoch 40.667), train_loss = 3.21062504, grad/param norm = 7.7665e-02, time/batch = 0.2695s	
2197/2700 (epoch 40.685), train_loss = 3.20925616, grad/param norm = 1.1136e-01, time/batch = 0.2691s	
2198/2700 (epoch 40.704), train_loss = 3.18041434, grad/param norm = 1.3128e-01, time/batch = 0.2687s	
2199/2700 (epoch 40.722), train_loss = 3.17102248, grad/param norm = 7.9114e-02, time/batch = 0.2693s	
2200/2700 (epoch 40.741), train_loss = 3.30496462, grad/param norm = 1.3562e-01, time/batch = 0.2693s	
2201/2700 (epoch 40.759), train_loss = 3.25070717, grad/param norm = 1.2520e-01, time/batch = 0.2701s	
2202/2700 (epoch 40.778), train_loss = 3.24266841, grad/param norm = 1.0495e-01, time/batch = 0.2686s	
2203/2700 (epoch 40.796), train_loss = 3.23528018, grad/param norm = 9.8021e-02, time/batch = 0.2696s	
2204/2700 (epoch 40.815), train_loss = 3.18957312, grad/param norm = 8.2857e-02, time/batch = 0.2698s	
2205/2700 (epoch 40.833), train_loss = 3.22700747, grad/param norm = 9.2145e-02, time/batch = 0.2694s	
2206/2700 (epoch 40.852), train_loss = 3.21378212, grad/param norm = 9.1245e-02, time/batch = 0.2698s	
2207/2700 (epoch 40.870), train_loss = 3.20867496, grad/param norm = 5.9666e-02, time/batch = 0.2693s	
2208/2700 (epoch 40.889), train_loss = 3.24863128, grad/param norm = 8.8042e-02, time/batch = 0.2689s	
2209/2700 (epoch 40.907), train_loss = 3.29998765, grad/param norm = 1.2545e-01, time/batch = 0.2694s	
2210/2700 (epoch 40.926), train_loss = 3.24718524, grad/param norm = 1.1541e-01, time/batch = 0.2692s	
2211/2700 (epoch 40.944), train_loss = 3.25667444, grad/param norm = 1.0054e-01, time/batch = 0.2699s	
2212/2700 (epoch 40.963), train_loss = 3.33800174, grad/param norm = 9.7853e-02, time/batch = 0.2686s	
2213/2700 (epoch 40.981), train_loss = 3.40176331, grad/param norm = 1.2820e-01, time/batch = 0.2691s	
decayed learning rate by a factor 0.97 to 0.00075461510158451	
2214/2700 (epoch 41.000), train_loss = 3.29354724, grad/param norm = 1.0969e-01, time/batch = 0.2694s	
2215/2700 (epoch 41.019), train_loss = 3.23218292, grad/param norm = 1.3150e-01, time/batch = 0.2691s	
2216/2700 (epoch 41.037), train_loss = 3.25113979, grad/param norm = 9.4552e-02, time/batch = 0.2692s	
2217/2700 (epoch 41.056), train_loss = 3.25919027, grad/param norm = 1.1837e-01, time/batch = 0.2690s	
2218/2700 (epoch 41.074), train_loss = 3.28640741, grad/param norm = 1.4283e-01, time/batch = 0.2687s	
2219/2700 (epoch 41.093), train_loss = 3.28827457, grad/param norm = 1.2108e-01, time/batch = 0.2693s	
2220/2700 (epoch 41.111), train_loss = 3.26175265, grad/param norm = 8.9322e-02, time/batch = 0.2692s	
2221/2700 (epoch 41.130), train_loss = 3.27691165, grad/param norm = 9.9974e-02, time/batch = 0.2701s	
2222/2700 (epoch 41.148), train_loss = 3.24222740, grad/param norm = 1.0309e-01, time/batch = 0.2689s	
2223/2700 (epoch 41.167), train_loss = 3.24800939, grad/param norm = 1.1592e-01, time/batch = 0.2691s	
2224/2700 (epoch 41.185), train_loss = 3.23547398, grad/param norm = 7.6915e-02, time/batch = 0.2694s	
2225/2700 (epoch 41.204), train_loss = 3.17267828, grad/param norm = 9.3917e-02, time/batch = 0.2691s	
2226/2700 (epoch 41.222), train_loss = 3.14633347, grad/param norm = 1.2912e-01, time/batch = 0.2690s	
2227/2700 (epoch 41.241), train_loss = 3.16685410, grad/param norm = 9.4728e-02, time/batch = 0.2691s	
2228/2700 (epoch 41.259), train_loss = 3.19960004, grad/param norm = 1.0088e-01, time/batch = 0.2686s	
2229/2700 (epoch 41.278), train_loss = 3.27229724, grad/param norm = 9.6658e-02, time/batch = 0.2691s	
2230/2700 (epoch 41.296), train_loss = 3.27359641, grad/param norm = 9.3032e-02, time/batch = 0.2693s	
2231/2700 (epoch 41.315), train_loss = 3.24970158, grad/param norm = 9.1183e-02, time/batch = 0.2699s	
2232/2700 (epoch 41.333), train_loss = 3.33390426, grad/param norm = 1.0505e-01, time/batch = 0.2688s	
2233/2700 (epoch 41.352), train_loss = 3.33668426, grad/param norm = 1.1518e-01, time/batch = 0.2691s	
2234/2700 (epoch 41.370), train_loss = 3.27915834, grad/param norm = 9.9458e-02, time/batch = 0.2696s	
2235/2700 (epoch 41.389), train_loss = 3.24982100, grad/param norm = 7.4570e-02, time/batch = 0.2696s	
2236/2700 (epoch 41.407), train_loss = 3.27167029, grad/param norm = 9.6057e-02, time/batch = 0.2690s	
2237/2700 (epoch 41.426), train_loss = 3.27244742, grad/param norm = 8.3945e-02, time/batch = 0.2690s	
2238/2700 (epoch 41.444), train_loss = 3.20711993, grad/param norm = 9.3454e-02, time/batch = 0.2686s	
2239/2700 (epoch 41.463), train_loss = 3.24528493, grad/param norm = 1.1054e-01, time/batch = 0.2692s	
2240/2700 (epoch 41.481), train_loss = 3.32493926, grad/param norm = 1.2912e-01, time/batch = 0.2692s	
2241/2700 (epoch 41.500), train_loss = 3.36413615, grad/param norm = 1.5077e-01, time/batch = 0.2702s	
2242/2700 (epoch 41.519), train_loss = 3.31908054, grad/param norm = 1.0628e-01, time/batch = 0.2690s	
2243/2700 (epoch 41.537), train_loss = 3.32328550, grad/param norm = 1.1720e-01, time/batch = 0.2690s	
2244/2700 (epoch 41.556), train_loss = 3.25676377, grad/param norm = 8.0636e-02, time/batch = 0.2697s	
2245/2700 (epoch 41.574), train_loss = 3.23020623, grad/param norm = 1.1318e-01, time/batch = 0.2694s	
2246/2700 (epoch 41.593), train_loss = 3.22410737, grad/param norm = 1.0851e-01, time/batch = 0.2690s	
2247/2700 (epoch 41.611), train_loss = 3.17046681, grad/param norm = 8.3174e-02, time/batch = 0.2692s	
2248/2700 (epoch 41.630), train_loss = 3.21004445, grad/param norm = 8.6327e-02, time/batch = 0.2688s	
2249/2700 (epoch 41.648), train_loss = 3.27340546, grad/param norm = 9.5866e-02, time/batch = 0.2690s	
2250/2700 (epoch 41.667), train_loss = 3.21056191, grad/param norm = 7.6943e-02, time/batch = 0.2695s	
2251/2700 (epoch 41.685), train_loss = 3.20916599, grad/param norm = 1.1110e-01, time/batch = 0.2699s	
2252/2700 (epoch 41.704), train_loss = 3.18027831, grad/param norm = 1.2996e-01, time/batch = 0.2686s	
2253/2700 (epoch 41.722), train_loss = 3.17090532, grad/param norm = 7.7003e-02, time/batch = 0.2693s	
2254/2700 (epoch 41.741), train_loss = 3.30475836, grad/param norm = 1.3403e-01, time/batch = 0.2691s	
2255/2700 (epoch 41.759), train_loss = 3.25048057, grad/param norm = 1.2354e-01, time/batch = 0.2690s	
2256/2700 (epoch 41.778), train_loss = 3.24247705, grad/param norm = 1.0358e-01, time/batch = 0.2689s	
2257/2700 (epoch 41.796), train_loss = 3.23513945, grad/param norm = 9.6803e-02, time/batch = 0.2693s	
2258/2700 (epoch 41.815), train_loss = 3.18949047, grad/param norm = 8.2517e-02, time/batch = 0.2687s	
2259/2700 (epoch 41.833), train_loss = 3.22688416, grad/param norm = 9.1474e-02, time/batch = 0.2701s	
2260/2700 (epoch 41.852), train_loss = 3.21365505, grad/param norm = 9.0215e-02, time/batch = 0.2701s	
2261/2700 (epoch 41.870), train_loss = 3.20861460, grad/param norm = 5.8993e-02, time/batch = 0.2704s	
2262/2700 (epoch 41.889), train_loss = 3.24853799, grad/param norm = 8.7434e-02, time/batch = 0.2688s	
2263/2700 (epoch 41.907), train_loss = 3.29984491, grad/param norm = 1.2438e-01, time/batch = 0.2695s	
2264/2700 (epoch 41.926), train_loss = 3.24707210, grad/param norm = 1.1449e-01, time/batch = 0.2697s	
2265/2700 (epoch 41.944), train_loss = 3.25648656, grad/param norm = 9.9336e-02, time/batch = 0.2696s	
2266/2700 (epoch 41.963), train_loss = 3.33791426, grad/param norm = 9.7229e-02, time/batch = 0.2694s	
2267/2700 (epoch 41.981), train_loss = 3.40174424, grad/param norm = 1.2799e-01, time/batch = 0.2694s	
decayed learning rate by a factor 0.97 to 0.00073197664853698	
2268/2700 (epoch 42.000), train_loss = 3.29329391, grad/param norm = 1.0841e-01, time/batch = 0.2690s	
2269/2700 (epoch 42.019), train_loss = 3.23182742, grad/param norm = 1.3058e-01, time/batch = 0.2697s	
2270/2700 (epoch 42.037), train_loss = 3.25096264, grad/param norm = 9.3354e-02, time/batch = 0.2697s	
2271/2700 (epoch 42.056), train_loss = 3.25915453, grad/param norm = 1.1808e-01, time/batch = 0.2700s	
2272/2700 (epoch 42.074), train_loss = 3.28619504, grad/param norm = 1.4118e-01, time/batch = 0.2687s	
2273/2700 (epoch 42.093), train_loss = 3.28800897, grad/param norm = 1.1883e-01, time/batch = 0.2696s	
2274/2700 (epoch 42.111), train_loss = 3.26167767, grad/param norm = 8.8436e-02, time/batch = 0.2698s	
2275/2700 (epoch 42.130), train_loss = 3.27677349, grad/param norm = 9.9618e-02, time/batch = 0.2700s	
2276/2700 (epoch 42.148), train_loss = 3.24214609, grad/param norm = 1.0245e-01, time/batch = 0.2691s	
2277/2700 (epoch 42.167), train_loss = 3.24783136, grad/param norm = 1.1453e-01, time/batch = 0.2689s	
2278/2700 (epoch 42.185), train_loss = 3.23539790, grad/param norm = 7.6697e-02, time/batch = 0.2691s	
2279/2700 (epoch 42.204), train_loss = 3.17261349, grad/param norm = 9.3508e-02, time/batch = 0.2697s	
2280/2700 (epoch 42.222), train_loss = 3.14619184, grad/param norm = 1.2842e-01, time/batch = 0.2691s	
2281/2700 (epoch 42.241), train_loss = 3.16680944, grad/param norm = 9.4505e-02, time/batch = 0.2699s	
2282/2700 (epoch 42.259), train_loss = 3.19951038, grad/param norm = 1.0035e-01, time/batch = 0.2686s	
2283/2700 (epoch 42.278), train_loss = 3.27219581, grad/param norm = 9.6146e-02, time/batch = 0.2694s	
2284/2700 (epoch 42.296), train_loss = 3.27343963, grad/param norm = 9.1598e-02, time/batch = 0.2693s	
2285/2700 (epoch 42.315), train_loss = 3.24952717, grad/param norm = 8.9668e-02, time/batch = 0.2693s	
2286/2700 (epoch 42.333), train_loss = 3.33383300, grad/param norm = 1.0409e-01, time/batch = 0.2690s	
2287/2700 (epoch 42.352), train_loss = 3.33642526, grad/param norm = 1.1365e-01, time/batch = 0.2689s	
2288/2700 (epoch 42.370), train_loss = 3.27895688, grad/param norm = 9.8157e-02, time/batch = 0.2690s	
2289/2700 (epoch 42.389), train_loss = 3.24978564, grad/param norm = 7.4369e-02, time/batch = 0.2692s	
2290/2700 (epoch 42.407), train_loss = 3.27160034, grad/param norm = 9.6191e-02, time/batch = 0.2691s	
2291/2700 (epoch 42.426), train_loss = 3.27228457, grad/param norm = 8.2988e-02, time/batch = 0.2706s	
2292/2700 (epoch 42.444), train_loss = 3.20713633, grad/param norm = 9.3605e-02, time/batch = 0.2689s	
2293/2700 (epoch 42.463), train_loss = 3.24515187, grad/param norm = 1.0993e-01, time/batch = 0.2691s	
2294/2700 (epoch 42.481), train_loss = 3.32484649, grad/param norm = 1.2793e-01, time/batch = 0.2694s	
2295/2700 (epoch 42.500), train_loss = 3.36382299, grad/param norm = 1.4808e-01, time/batch = 0.2695s	
2296/2700 (epoch 42.519), train_loss = 3.31894623, grad/param norm = 1.0364e-01, time/batch = 0.2695s	
2297/2700 (epoch 42.537), train_loss = 3.32319750, grad/param norm = 1.1590e-01, time/batch = 0.2692s	
2298/2700 (epoch 42.556), train_loss = 3.25662660, grad/param norm = 7.9727e-02, time/batch = 0.2689s	
2299/2700 (epoch 42.574), train_loss = 3.23016940, grad/param norm = 1.1325e-01, time/batch = 0.2693s	
2300/2700 (epoch 42.593), train_loss = 3.22388378, grad/param norm = 1.0703e-01, time/batch = 0.2699s	
2301/2700 (epoch 42.611), train_loss = 3.17049531, grad/param norm = 8.3437e-02, time/batch = 0.2704s	
2302/2700 (epoch 42.630), train_loss = 3.20997498, grad/param norm = 8.5226e-02, time/batch = 0.2690s	
2303/2700 (epoch 42.648), train_loss = 3.27317923, grad/param norm = 9.5271e-02, time/batch = 0.2689s	
2304/2700 (epoch 42.667), train_loss = 3.21050741, grad/param norm = 7.6260e-02, time/batch = 0.2693s	
2305/2700 (epoch 42.685), train_loss = 3.20907570, grad/param norm = 1.1084e-01, time/batch = 0.2691s	
2306/2700 (epoch 42.704), train_loss = 3.18014661, grad/param norm = 1.2867e-01, time/batch = 0.2692s	
2307/2700 (epoch 42.722), train_loss = 3.17079854, grad/param norm = 7.5012e-02, time/batch = 0.2689s	
2308/2700 (epoch 42.741), train_loss = 3.30455812, grad/param norm = 1.3257e-01, time/batch = 0.2687s	
2309/2700 (epoch 42.759), train_loss = 3.25026441, grad/param norm = 1.2197e-01, time/batch = 0.2696s	
2310/2700 (epoch 42.778), train_loss = 3.24229444, grad/param norm = 1.0226e-01, time/batch = 0.2693s	
2311/2700 (epoch 42.796), train_loss = 3.23500886, grad/param norm = 9.5649e-02, time/batch = 0.2700s	
2312/2700 (epoch 42.815), train_loss = 3.18941340, grad/param norm = 8.2195e-02, time/batch = 0.2695s	
2313/2700 (epoch 42.833), train_loss = 3.22676624, grad/param norm = 9.0803e-02, time/batch = 0.2699s	
2314/2700 (epoch 42.852), train_loss = 3.21353289, grad/param norm = 8.9201e-02, time/batch = 0.2702s	
2315/2700 (epoch 42.870), train_loss = 3.20856137, grad/param norm = 5.8368e-02, time/batch = 0.2692s	
2316/2700 (epoch 42.889), train_loss = 3.24844699, grad/param norm = 8.6855e-02, time/batch = 0.2697s	
2317/2700 (epoch 42.907), train_loss = 3.29970639, grad/param norm = 1.2335e-01, time/batch = 0.2700s	
2318/2700 (epoch 42.926), train_loss = 3.24696723, grad/param norm = 1.1362e-01, time/batch = 0.2693s	
2319/2700 (epoch 42.944), train_loss = 3.25630529, grad/param norm = 9.8172e-02, time/batch = 0.2700s	
2320/2700 (epoch 42.963), train_loss = 3.33782986, grad/param norm = 9.6657e-02, time/batch = 0.2700s	
2321/2700 (epoch 42.981), train_loss = 3.40172080, grad/param norm = 1.2775e-01, time/batch = 0.2703s	
decayed learning rate by a factor 0.97 to 0.00071001734908087	
2322/2700 (epoch 43.000), train_loss = 3.29305126, grad/param norm = 1.0716e-01, time/batch = 0.2695s	
2323/2700 (epoch 43.019), train_loss = 3.23147837, grad/param norm = 1.2968e-01, time/batch = 0.2698s	
2324/2700 (epoch 43.037), train_loss = 3.25079411, grad/param norm = 9.2207e-02, time/batch = 0.2701s	
2325/2700 (epoch 43.056), train_loss = 3.25911442, grad/param norm = 1.1773e-01, time/batch = 0.2700s	
2326/2700 (epoch 43.074), train_loss = 3.28598923, grad/param norm = 1.3953e-01, time/batch = 0.2698s	
2327/2700 (epoch 43.093), train_loss = 3.28775501, grad/param norm = 1.1667e-01, time/batch = 0.2701s	
2328/2700 (epoch 43.111), train_loss = 3.26160816, grad/param norm = 8.7619e-02, time/batch = 0.2695s	
2329/2700 (epoch 43.130), train_loss = 3.27663757, grad/param norm = 9.9247e-02, time/batch = 0.2700s	
2330/2700 (epoch 43.148), train_loss = 3.24206868, grad/param norm = 1.0186e-01, time/batch = 0.2695s	
2331/2700 (epoch 43.167), train_loss = 3.24766091, grad/param norm = 1.1316e-01, time/batch = 0.2704s	
2332/2700 (epoch 43.185), train_loss = 3.23532557, grad/param norm = 7.6506e-02, time/batch = 0.2686s	
2333/2700 (epoch 43.204), train_loss = 3.17255218, grad/param norm = 9.3130e-02, time/batch = 0.2692s	
2334/2700 (epoch 43.222), train_loss = 3.14605371, grad/param norm = 1.2774e-01, time/batch = 0.2693s	
2335/2700 (epoch 43.241), train_loss = 3.16676671, grad/param norm = 9.4309e-02, time/batch = 0.2692s	
2336/2700 (epoch 43.259), train_loss = 3.19942345, grad/param norm = 9.9795e-02, time/batch = 0.2690s	
2337/2700 (epoch 43.278), train_loss = 3.27210314, grad/param norm = 9.5707e-02, time/batch = 0.2690s	
2338/2700 (epoch 43.296), train_loss = 3.27329232, grad/param norm = 9.0257e-02, time/batch = 0.2688s	
2339/2700 (epoch 43.315), train_loss = 3.24935959, grad/param norm = 8.8264e-02, time/batch = 0.2693s	
2340/2700 (epoch 43.333), train_loss = 3.33377165, grad/param norm = 1.0325e-01, time/batch = 0.2692s	
2341/2700 (epoch 43.352), train_loss = 3.33617378, grad/param norm = 1.1219e-01, time/batch = 0.2701s	
2342/2700 (epoch 43.370), train_loss = 3.27876460, grad/param norm = 9.6925e-02, time/batch = 0.2688s	
2343/2700 (epoch 43.389), train_loss = 3.24975482, grad/param norm = 7.4214e-02, time/batch = 0.2694s	
2344/2700 (epoch 43.407), train_loss = 3.27152941, grad/param norm = 9.6272e-02, time/batch = 0.2695s	
2345/2700 (epoch 43.426), train_loss = 3.27212635, grad/param norm = 8.2039e-02, time/batch = 0.2695s	
2346/2700 (epoch 43.444), train_loss = 3.20714710, grad/param norm = 9.3655e-02, time/batch = 0.2692s	
2347/2700 (epoch 43.463), train_loss = 3.24501584, grad/param norm = 1.0914e-01, time/batch = 0.2691s	
2348/2700 (epoch 43.481), train_loss = 3.32474883, grad/param norm = 1.2662e-01, time/batch = 0.2689s	
2349/2700 (epoch 43.500), train_loss = 3.36352747, grad/param norm = 1.4550e-01, time/batch = 0.2693s	
2350/2700 (epoch 43.519), train_loss = 3.31882965, grad/param norm = 1.0123e-01, time/batch = 0.2693s	
2351/2700 (epoch 43.537), train_loss = 3.32311698, grad/param norm = 1.1470e-01, time/batch = 0.2703s	
2352/2700 (epoch 43.556), train_loss = 3.25649851, grad/param norm = 7.8896e-02, time/batch = 0.2689s	
2353/2700 (epoch 43.574), train_loss = 3.23013362, grad/param norm = 1.1332e-01, time/batch = 0.2690s	
2354/2700 (epoch 43.593), train_loss = 3.22366798, grad/param norm = 1.0557e-01, time/batch = 0.2693s	
2355/2700 (epoch 43.611), train_loss = 3.17052134, grad/param norm = 8.3705e-02, time/batch = 0.2694s	
2356/2700 (epoch 43.630), train_loss = 3.20990778, grad/param norm = 8.4141e-02, time/batch = 0.2690s	
2357/2700 (epoch 43.648), train_loss = 3.27296381, grad/param norm = 9.4711e-02, time/batch = 0.2691s	
2358/2700 (epoch 43.667), train_loss = 3.21046166, grad/param norm = 7.5616e-02, time/batch = 0.2684s	
2359/2700 (epoch 43.685), train_loss = 3.20898482, grad/param norm = 1.1060e-01, time/batch = 0.2695s	
2360/2700 (epoch 43.704), train_loss = 3.18001916, grad/param norm = 1.2739e-01, time/batch = 0.2691s	
2361/2700 (epoch 43.722), train_loss = 3.17070159, grad/param norm = 7.3129e-02, time/batch = 0.2700s	
2362/2700 (epoch 43.741), train_loss = 3.30436226, grad/param norm = 1.3120e-01, time/batch = 0.2686s	
2363/2700 (epoch 43.759), train_loss = 3.25005786, grad/param norm = 1.2046e-01, time/batch = 0.2693s	
2364/2700 (epoch 43.778), train_loss = 3.24211905, grad/param norm = 1.0099e-01, time/batch = 0.2695s	
2365/2700 (epoch 43.796), train_loss = 3.23488699, grad/param norm = 9.4553e-02, time/batch = 0.2696s	
2366/2700 (epoch 43.815), train_loss = 3.18934333, grad/param norm = 8.1890e-02, time/batch = 0.2691s	
2367/2700 (epoch 43.833), train_loss = 3.22665323, grad/param norm = 9.0138e-02, time/batch = 0.2693s	
2368/2700 (epoch 43.852), train_loss = 3.21341746, grad/param norm = 8.8204e-02, time/batch = 0.2693s	
2369/2700 (epoch 43.870), train_loss = 3.20851414, grad/param norm = 5.7792e-02, time/batch = 0.2694s	
2370/2700 (epoch 43.889), train_loss = 3.24835819, grad/param norm = 8.6309e-02, time/batch = 0.2692s	
2371/2700 (epoch 43.907), train_loss = 3.29957054, grad/param norm = 1.2236e-01, time/batch = 0.2709s	
2372/2700 (epoch 43.926), train_loss = 3.24687067, grad/param norm = 1.1280e-01, time/batch = 0.2689s	
2373/2700 (epoch 43.944), train_loss = 3.25613330, grad/param norm = 9.7052e-02, time/batch = 0.2695s	
2374/2700 (epoch 43.963), train_loss = 3.33775084, grad/param norm = 9.6134e-02, time/batch = 0.2697s	
2375/2700 (epoch 43.981), train_loss = 3.40169335, grad/param norm = 1.2749e-01, time/batch = 0.2693s	
decayed learning rate by a factor 0.97 to 0.00068871682860844	
2376/2700 (epoch 44.000), train_loss = 3.29281972, grad/param norm = 1.0595e-01, time/batch = 0.2697s	
2377/2700 (epoch 44.019), train_loss = 3.23113488, grad/param norm = 1.2879e-01, time/batch = 0.2700s	
2378/2700 (epoch 44.037), train_loss = 3.25063362, grad/param norm = 9.1111e-02, time/batch = 0.2693s	
2379/2700 (epoch 44.056), train_loss = 3.25907046, grad/param norm = 1.1733e-01, time/batch = 0.2705s	
2380/2700 (epoch 44.074), train_loss = 3.28578995, grad/param norm = 1.3786e-01, time/batch = 0.2704s	
2381/2700 (epoch 44.093), train_loss = 3.28751447, grad/param norm = 1.1460e-01, time/batch = 0.2706s	
2382/2700 (epoch 44.111), train_loss = 3.26154417, grad/param norm = 8.6862e-02, time/batch = 0.2700s	
2383/2700 (epoch 44.130), train_loss = 3.27650513, grad/param norm = 9.8861e-02, time/batch = 0.2696s	
2384/2700 (epoch 44.148), train_loss = 3.24199645, grad/param norm = 1.0132e-01, time/batch = 0.2698s	
2385/2700 (epoch 44.167), train_loss = 3.24749649, grad/param norm = 1.1183e-01, time/batch = 0.2702s	
2386/2700 (epoch 44.185), train_loss = 3.23525794, grad/param norm = 7.6340e-02, time/batch = 0.2698s	
2387/2700 (epoch 44.204), train_loss = 3.17249421, grad/param norm = 9.2779e-02, time/batch = 0.2695s	
2388/2700 (epoch 44.222), train_loss = 3.14591702, grad/param norm = 1.2706e-01, time/batch = 0.2692s	
2389/2700 (epoch 44.241), train_loss = 3.16672598, grad/param norm = 9.4136e-02, time/batch = 0.2696s	
2390/2700 (epoch 44.259), train_loss = 3.19933882, grad/param norm = 9.9218e-02, time/batch = 0.2703s	
2391/2700 (epoch 44.278), train_loss = 3.27201580, grad/param norm = 9.5338e-02, time/batch = 0.2710s	
2392/2700 (epoch 44.296), train_loss = 3.27315400, grad/param norm = 8.9002e-02, time/batch = 0.2690s	
2393/2700 (epoch 44.315), train_loss = 3.24920029, grad/param norm = 8.6958e-02, time/batch = 0.2693s	
2394/2700 (epoch 44.333), train_loss = 3.33371681, grad/param norm = 1.0251e-01, time/batch = 0.2700s	
2395/2700 (epoch 44.352), train_loss = 3.33592926, grad/param norm = 1.1079e-01, time/batch = 0.2693s	
2396/2700 (epoch 44.370), train_loss = 3.27858067, grad/param norm = 9.5759e-02, time/batch = 0.2694s	
2397/2700 (epoch 44.389), train_loss = 3.24972713, grad/param norm = 7.4093e-02, time/batch = 0.2689s	
2398/2700 (epoch 44.407), train_loss = 3.27145957, grad/param norm = 9.6295e-02, time/batch = 0.2687s	
2399/2700 (epoch 44.426), train_loss = 3.27197375, grad/param norm = 8.1098e-02, time/batch = 0.2695s	
2400/2700 (epoch 44.444), train_loss = 3.20715199, grad/param norm = 9.3610e-02, time/batch = 0.2693s	
2401/2700 (epoch 44.463), train_loss = 3.24487718, grad/param norm = 1.0820e-01, time/batch = 0.2700s	
2402/2700 (epoch 44.481), train_loss = 3.32464919, grad/param norm = 1.2523e-01, time/batch = 0.2688s	
2403/2700 (epoch 44.500), train_loss = 3.36324947, grad/param norm = 1.4302e-01, time/batch = 0.2691s	
2404/2700 (epoch 44.519), train_loss = 3.31872920, grad/param norm = 9.9027e-02, time/batch = 0.2691s	
2405/2700 (epoch 44.537), train_loss = 3.32304457, grad/param norm = 1.1357e-01, time/batch = 0.2696s	
2406/2700 (epoch 44.556), train_loss = 3.25638082, grad/param norm = 7.8132e-02, time/batch = 0.2695s	
2407/2700 (epoch 44.574), train_loss = 3.23009767, grad/param norm = 1.1339e-01, time/batch = 0.2689s	
2408/2700 (epoch 44.593), train_loss = 3.22345894, grad/param norm = 1.0414e-01, time/batch = 0.2687s	
2409/2700 (epoch 44.611), train_loss = 3.17054446, grad/param norm = 8.3973e-02, time/batch = 0.2696s	
2410/2700 (epoch 44.630), train_loss = 3.20984241, grad/param norm = 8.3072e-02, time/batch = 0.2692s	
2411/2700 (epoch 44.648), train_loss = 3.27275936, grad/param norm = 9.4181e-02, time/batch = 0.2701s	
2412/2700 (epoch 44.667), train_loss = 3.21042281, grad/param norm = 7.5007e-02, time/batch = 0.2690s	
2413/2700 (epoch 44.685), train_loss = 3.20889276, grad/param norm = 1.1037e-01, time/batch = 0.2690s	
2414/2700 (epoch 44.704), train_loss = 3.17989652, grad/param norm = 1.2614e-01, time/batch = 0.2692s	
2415/2700 (epoch 44.722), train_loss = 3.17061300, grad/param norm = 7.1345e-02, time/batch = 0.2693s	
2416/2700 (epoch 44.741), train_loss = 3.30417220, grad/param norm = 1.2991e-01, time/batch = 0.2692s	
2417/2700 (epoch 44.759), train_loss = 3.24986053, grad/param norm = 1.1902e-01, time/batch = 0.2689s	
2418/2700 (epoch 44.778), train_loss = 3.24195023, grad/param norm = 9.9780e-02, time/batch = 0.2687s	
2419/2700 (epoch 44.796), train_loss = 3.23477399, grad/param norm = 9.3510e-02, time/batch = 0.2694s	
2420/2700 (epoch 44.815), train_loss = 3.18927781, grad/param norm = 8.1600e-02, time/batch = 0.2692s	
2421/2700 (epoch 44.833), train_loss = 3.22654559, grad/param norm = 8.9479e-02, time/batch = 0.2702s	
2422/2700 (epoch 44.852), train_loss = 3.21330664, grad/param norm = 8.7227e-02, time/batch = 0.2689s	
2423/2700 (epoch 44.870), train_loss = 3.20847331, grad/param norm = 5.7263e-02, time/batch = 0.2693s	
2424/2700 (epoch 44.889), train_loss = 3.24827120, grad/param norm = 8.5793e-02, time/batch = 0.2695s	
2425/2700 (epoch 44.907), train_loss = 3.29943937, grad/param norm = 1.2140e-01, time/batch = 0.2694s	
2426/2700 (epoch 44.926), train_loss = 3.24678141, grad/param norm = 1.1202e-01, time/batch = 0.2692s	
2427/2700 (epoch 44.944), train_loss = 3.25596938, grad/param norm = 9.5975e-02, time/batch = 0.2691s	
2428/2700 (epoch 44.963), train_loss = 3.33767402, grad/param norm = 9.5655e-02, time/batch = 0.2689s	
2429/2700 (epoch 44.981), train_loss = 3.40166153, grad/param norm = 1.2720e-01, time/batch = 0.2695s	
decayed learning rate by a factor 0.97 to 0.00066805532375019	
2430/2700 (epoch 45.000), train_loss = 3.29259779, grad/param norm = 1.0479e-01, time/batch = 0.2694s	
2431/2700 (epoch 45.019), train_loss = 3.23079715, grad/param norm = 1.2793e-01, time/batch = 0.2697s	
2432/2700 (epoch 45.037), train_loss = 3.25047966, grad/param norm = 9.0066e-02, time/batch = 0.2687s	
2433/2700 (epoch 45.056), train_loss = 3.25902322, grad/param norm = 1.1690e-01, time/batch = 0.2692s	
2434/2700 (epoch 45.074), train_loss = 3.28559723, grad/param norm = 1.3620e-01, time/batch = 0.2694s	
2435/2700 (epoch 45.093), train_loss = 3.28728516, grad/param norm = 1.1261e-01, time/batch = 0.2700s	
2436/2700 (epoch 45.111), train_loss = 3.26148491, grad/param norm = 8.6159e-02, time/batch = 0.2693s	
2437/2700 (epoch 45.130), train_loss = 3.27637560, grad/param norm = 9.8460e-02, time/batch = 0.2691s	
2438/2700 (epoch 45.148), train_loss = 3.24192804, grad/param norm = 1.0081e-01, time/batch = 0.2688s	
2439/2700 (epoch 45.167), train_loss = 3.24733845, grad/param norm = 1.1052e-01, time/batch = 0.2693s	
2440/2700 (epoch 45.185), train_loss = 3.23519256, grad/param norm = 7.6196e-02, time/batch = 0.2691s	
2441/2700 (epoch 45.204), train_loss = 3.17244023, grad/param norm = 9.2454e-02, time/batch = 0.2699s	
2442/2700 (epoch 45.222), train_loss = 3.14578290, grad/param norm = 1.2639e-01, time/batch = 0.2693s	
2443/2700 (epoch 45.241), train_loss = 3.16668693, grad/param norm = 9.3986e-02, time/batch = 0.2690s	
2444/2700 (epoch 45.259), train_loss = 3.19925711, grad/param norm = 9.8620e-02, time/batch = 0.2694s	
2445/2700 (epoch 45.278), train_loss = 3.27193454, grad/param norm = 9.5030e-02, time/batch = 0.2693s	
2446/2700 (epoch 45.296), train_loss = 3.27302361, grad/param norm = 8.7832e-02, time/batch = 0.2692s	
2447/2700 (epoch 45.315), train_loss = 3.24904649, grad/param norm = 8.5744e-02, time/batch = 0.2690s	
2448/2700 (epoch 45.333), train_loss = 3.33366951, grad/param norm = 1.0188e-01, time/batch = 0.2686s	
2449/2700 (epoch 45.352), train_loss = 3.33569252, grad/param norm = 1.0946e-01, time/batch = 0.2694s	
2450/2700 (epoch 45.370), train_loss = 3.27840636, grad/param norm = 9.4656e-02, time/batch = 0.2690s	
2451/2700 (epoch 45.389), train_loss = 3.24970254, grad/param norm = 7.3992e-02, time/batch = 0.2703s	
2452/2700 (epoch 45.407), train_loss = 3.27138844, grad/param norm = 9.6260e-02, time/batch = 0.2689s	
2453/2700 (epoch 45.426), train_loss = 3.27182547, grad/param norm = 8.0169e-02, time/batch = 0.2692s	
2454/2700 (epoch 45.444), train_loss = 3.20715235, grad/param norm = 9.3482e-02, time/batch = 0.2694s	
2455/2700 (epoch 45.463), train_loss = 3.24473909, grad/param norm = 1.0715e-01, time/batch = 0.2694s	
2456/2700 (epoch 45.481), train_loss = 3.32454881, grad/param norm = 1.2381e-01, time/batch = 0.2700s	
2457/2700 (epoch 45.500), train_loss = 3.36298831, grad/param norm = 1.4067e-01, time/batch = 0.2695s	
2458/2700 (epoch 45.519), train_loss = 3.31864379, grad/param norm = 9.7013e-02, time/batch = 0.2688s	
2459/2700 (epoch 45.537), train_loss = 3.32297788, grad/param norm = 1.1250e-01, time/batch = 0.2694s	
2460/2700 (epoch 45.556), train_loss = 3.25627178, grad/param norm = 7.7427e-02, time/batch = 0.2689s	
2461/2700 (epoch 45.574), train_loss = 3.23006242, grad/param norm = 1.1346e-01, time/batch = 0.2704s	
2462/2700 (epoch 45.593), train_loss = 3.22325785, grad/param norm = 1.0273e-01, time/batch = 0.2685s	
2463/2700 (epoch 45.611), train_loss = 3.17056512, grad/param norm = 8.4240e-02, time/batch = 0.2688s	
2464/2700 (epoch 45.630), train_loss = 3.20977842, grad/param norm = 8.2022e-02, time/batch = 0.2692s	
2465/2700 (epoch 45.648), train_loss = 3.27256490, grad/param norm = 9.3682e-02, time/batch = 0.2691s	
2466/2700 (epoch 45.667), train_loss = 3.21039145, grad/param norm = 7.4435e-02, time/batch = 0.2698s	
2467/2700 (epoch 45.685), train_loss = 3.20880032, grad/param norm = 1.1014e-01, time/batch = 0.2691s	
2468/2700 (epoch 45.704), train_loss = 3.17977808, grad/param norm = 1.2490e-01, time/batch = 0.2691s	
2469/2700 (epoch 45.722), train_loss = 3.17053340, grad/param norm = 6.9657e-02, time/batch = 0.2705s	
2470/2700 (epoch 45.741), train_loss = 3.30398545, grad/param norm = 1.2869e-01, time/batch = 0.2697s	
2471/2700 (epoch 45.759), train_loss = 3.24967110, grad/param norm = 1.1764e-01, time/batch = 0.2704s	
2472/2700 (epoch 45.778), train_loss = 3.24178824, grad/param norm = 9.8622e-02, time/batch = 0.2689s	
2473/2700 (epoch 45.796), train_loss = 3.23466876, grad/param norm = 9.2519e-02, time/batch = 0.2690s	
2474/2700 (epoch 45.815), train_loss = 3.18921712, grad/param norm = 8.1322e-02, time/batch = 0.2703s	
2475/2700 (epoch 45.833), train_loss = 3.22644262, grad/param norm = 8.8829e-02, time/batch = 0.2695s	
2476/2700 (epoch 45.852), train_loss = 3.21320162, grad/param norm = 8.6271e-02, time/batch = 0.2689s	
2477/2700 (epoch 45.870), train_loss = 3.20843730, grad/param norm = 5.6781e-02, time/batch = 0.2691s	
2478/2700 (epoch 45.889), train_loss = 3.24818667, grad/param norm = 8.5309e-02, time/batch = 0.2692s	
2479/2700 (epoch 45.907), train_loss = 3.29931149, grad/param norm = 1.2048e-01, time/batch = 0.2693s	
2480/2700 (epoch 45.926), train_loss = 3.24670065, grad/param norm = 1.1127e-01, time/batch = 0.2691s	
2481/2700 (epoch 45.944), train_loss = 3.25581278, grad/param norm = 9.4941e-02, time/batch = 0.2707s	
2482/2700 (epoch 45.963), train_loss = 3.33760079, grad/param norm = 9.5217e-02, time/batch = 0.2686s	
2483/2700 (epoch 45.981), train_loss = 3.40162635, grad/param norm = 1.2689e-01, time/batch = 0.2689s	
decayed learning rate by a factor 0.97 to 0.00064801366403768	
2484/2700 (epoch 46.000), train_loss = 3.29238639, grad/param norm = 1.0368e-01, time/batch = 0.2694s	
2485/2700 (epoch 46.019), train_loss = 3.23046667, grad/param norm = 1.2708e-01, time/batch = 0.2693s	
2486/2700 (epoch 46.037), train_loss = 3.25033401, grad/param norm = 8.9073e-02, time/batch = 0.2689s	
2487/2700 (epoch 46.056), train_loss = 3.25897312, grad/param norm = 1.1643e-01, time/batch = 0.2692s	
2488/2700 (epoch 46.074), train_loss = 3.28541088, grad/param norm = 1.3454e-01, time/batch = 0.2690s	
2489/2700 (epoch 46.093), train_loss = 3.28706775, grad/param norm = 1.1071e-01, time/batch = 0.2691s	
2490/2700 (epoch 46.111), train_loss = 3.26142979, grad/param norm = 8.5502e-02, time/batch = 0.2690s	
2491/2700 (epoch 46.130), train_loss = 3.27624913, grad/param norm = 9.8047e-02, time/batch = 0.2705s	
2492/2700 (epoch 46.148), train_loss = 3.24186357, grad/param norm = 1.0035e-01, time/batch = 0.2686s	
2493/2700 (epoch 46.167), train_loss = 3.24718508, grad/param norm = 1.0925e-01, time/batch = 0.2690s	
2494/2700 (epoch 46.185), train_loss = 3.23513238, grad/param norm = 7.6071e-02, time/batch = 0.2701s	
2495/2700 (epoch 46.204), train_loss = 3.17238873, grad/param norm = 9.2155e-02, time/batch = 0.2699s	
2496/2700 (epoch 46.222), train_loss = 3.14565113, grad/param norm = 1.2572e-01, time/batch = 0.2691s	
2497/2700 (epoch 46.241), train_loss = 3.16664880, grad/param norm = 9.3857e-02, time/batch = 0.2688s	
2498/2700 (epoch 46.259), train_loss = 3.19917709, grad/param norm = 9.8001e-02, time/batch = 0.2686s	
2499/2700 (epoch 46.278), train_loss = 3.27185857, grad/param norm = 9.4778e-02, time/batch = 0.2691s	
2500/2700 (epoch 46.296), train_loss = 3.27290091, grad/param norm = 8.6739e-02, time/batch = 0.2694s	
2501/2700 (epoch 46.315), train_loss = 3.24890004, grad/param norm = 8.4607e-02, time/batch = 0.2703s	
2502/2700 (epoch 46.333), train_loss = 3.33362856, grad/param norm = 1.0132e-01, time/batch = 0.2686s	
2503/2700 (epoch 46.352), train_loss = 3.33546117, grad/param norm = 1.0818e-01, time/batch = 0.2689s	
2504/2700 (epoch 46.370), train_loss = 3.27823868, grad/param norm = 9.3612e-02, time/batch = 0.2699s	
2505/2700 (epoch 46.389), train_loss = 3.24967984, grad/param norm = 7.3899e-02, time/batch = 0.2695s	
2506/2700 (epoch 46.407), train_loss = 3.27131896, grad/param norm = 9.6166e-02, time/batch = 0.2696s	
2507/2700 (epoch 46.426), train_loss = 3.27168200, grad/param norm = 7.9256e-02, time/batch = 0.2691s	
2508/2700 (epoch 46.444), train_loss = 3.20714713, grad/param norm = 9.3284e-02, time/batch = 0.2687s	
2509/2700 (epoch 46.463), train_loss = 3.24460151, grad/param norm = 1.0603e-01, time/batch = 0.2691s	
2510/2700 (epoch 46.481), train_loss = 3.32444924, grad/param norm = 1.2237e-01, time/batch = 0.2699s	
2511/2700 (epoch 46.500), train_loss = 3.36274447, grad/param norm = 1.3842e-01, time/batch = 0.2704s	
2512/2700 (epoch 46.519), train_loss = 3.31857059, grad/param norm = 9.5172e-02, time/batch = 0.2686s	
2513/2700 (epoch 46.537), train_loss = 3.32291648, grad/param norm = 1.1148e-01, time/batch = 0.2693s	
2514/2700 (epoch 46.556), train_loss = 3.25617200, grad/param norm = 7.6776e-02, time/batch = 0.2697s	
2515/2700 (epoch 46.574), train_loss = 3.23002853, grad/param norm = 1.1354e-01, time/batch = 0.2693s	
2516/2700 (epoch 46.593), train_loss = 3.22306323, grad/param norm = 1.0135e-01, time/batch = 0.2691s	
2517/2700 (epoch 46.611), train_loss = 3.17058319, grad/param norm = 8.4506e-02, time/batch = 0.2687s	
2518/2700 (epoch 46.630), train_loss = 3.20971574, grad/param norm = 8.0992e-02, time/batch = 0.2686s	
2519/2700 (epoch 46.648), train_loss = 3.27238093, grad/param norm = 9.3210e-02, time/batch = 0.2690s	
2520/2700 (epoch 46.667), train_loss = 3.21036726, grad/param norm = 7.3898e-02, time/batch = 0.2692s	
2521/2700 (epoch 46.685), train_loss = 3.20870692, grad/param norm = 1.0992e-01, time/batch = 0.2703s	
2522/2700 (epoch 46.704), train_loss = 3.17966379, grad/param norm = 1.2369e-01, time/batch = 0.2687s	
2523/2700 (epoch 46.722), train_loss = 3.17046029, grad/param norm = 6.8053e-02, time/batch = 0.2691s	
2524/2700 (epoch 46.741), train_loss = 3.30380177, grad/param norm = 1.2754e-01, time/batch = 0.2695s	
2525/2700 (epoch 46.759), train_loss = 3.24948918, grad/param norm = 1.1631e-01, time/batch = 0.2693s	
2526/2700 (epoch 46.778), train_loss = 3.24163354, grad/param norm = 9.7515e-02, time/batch = 0.2692s	
2527/2700 (epoch 46.796), train_loss = 3.23457070, grad/param norm = 9.1573e-02, time/batch = 0.2692s	
2528/2700 (epoch 46.815), train_loss = 3.18916245, grad/param norm = 8.1056e-02, time/batch = 0.2688s	
2529/2700 (epoch 46.833), train_loss = 3.22634472, grad/param norm = 8.8190e-02, time/batch = 0.2691s	
2530/2700 (epoch 46.852), train_loss = 3.21310071, grad/param norm = 8.5334e-02, time/batch = 0.2693s	
2531/2700 (epoch 46.870), train_loss = 3.20840712, grad/param norm = 5.6342e-02, time/batch = 0.2698s	
2532/2700 (epoch 46.889), train_loss = 3.24810591, grad/param norm = 8.4857e-02, time/batch = 0.2685s	
2533/2700 (epoch 46.907), train_loss = 3.29918679, grad/param norm = 1.1959e-01, time/batch = 0.2696s	
2534/2700 (epoch 46.926), train_loss = 3.24662590, grad/param norm = 1.1056e-01, time/batch = 0.2696s	
2535/2700 (epoch 46.944), train_loss = 3.25566322, grad/param norm = 9.3952e-02, time/batch = 0.2694s	
2536/2700 (epoch 46.963), train_loss = 3.33753018, grad/param norm = 9.4818e-02, time/batch = 0.2691s	
2537/2700 (epoch 46.981), train_loss = 3.40158628, grad/param norm = 1.2654e-01, time/batch = 0.2695s	
decayed learning rate by a factor 0.97 to 0.00062857325411655	
2538/2700 (epoch 47.000), train_loss = 3.29218447, grad/param norm = 1.0260e-01, time/batch = 0.2686s	
2539/2700 (epoch 47.019), train_loss = 3.23014051, grad/param norm = 1.2624e-01, time/batch = 0.2691s	
2540/2700 (epoch 47.037), train_loss = 3.25019536, grad/param norm = 8.8129e-02, time/batch = 0.2694s	
2541/2700 (epoch 47.056), train_loss = 3.25892070, grad/param norm = 1.1594e-01, time/batch = 0.2701s	
2542/2700 (epoch 47.074), train_loss = 3.28523128, grad/param norm = 1.3290e-01, time/batch = 0.2688s	
2543/2700 (epoch 47.093), train_loss = 3.28686153, grad/param norm = 1.0889e-01, time/batch = 0.2692s	
2544/2700 (epoch 47.111), train_loss = 3.26137824, grad/param norm = 8.4886e-02, time/batch = 0.2694s	
2545/2700 (epoch 47.130), train_loss = 3.27612494, grad/param norm = 9.7623e-02, time/batch = 0.2692s	
2546/2700 (epoch 47.148), train_loss = 3.24180290, grad/param norm = 9.9932e-02, time/batch = 0.2691s	
2547/2700 (epoch 47.167), train_loss = 3.24703662, grad/param norm = 1.0799e-01, time/batch = 0.2691s	
2548/2700 (epoch 47.185), train_loss = 3.23507380, grad/param norm = 7.5964e-02, time/batch = 0.2689s	
2549/2700 (epoch 47.204), train_loss = 3.17234094, grad/param norm = 9.1880e-02, time/batch = 0.2691s	
2550/2700 (epoch 47.222), train_loss = 3.14552030, grad/param norm = 1.2505e-01, time/batch = 0.2692s	
2551/2700 (epoch 47.241), train_loss = 3.16661297, grad/param norm = 9.3747e-02, time/batch = 0.2706s	
2552/2700 (epoch 47.259), train_loss = 3.19909989, grad/param norm = 9.7361e-02, time/batch = 0.2687s	
2553/2700 (epoch 47.278), train_loss = 3.27178784, grad/param norm = 9.4576e-02, time/batch = 0.2690s	
2554/2700 (epoch 47.296), train_loss = 3.27278562, grad/param norm = 8.5721e-02, time/batch = 0.2693s	
2555/2700 (epoch 47.315), train_loss = 3.24875893, grad/param norm = 8.3542e-02, time/batch = 0.2694s	
2556/2700 (epoch 47.333), train_loss = 3.33359208, grad/param norm = 1.0085e-01, time/batch = 0.2691s	
2557/2700 (epoch 47.352), train_loss = 3.33523802, grad/param norm = 1.0695e-01, time/batch = 0.2692s	
2558/2700 (epoch 47.370), train_loss = 3.27807991, grad/param norm = 9.2626e-02, time/batch = 0.2686s	
2559/2700 (epoch 47.389), train_loss = 3.24965858, grad/param norm = 7.3807e-02, time/batch = 0.2693s	
2560/2700 (epoch 47.407), train_loss = 3.27124829, grad/param norm = 9.6016e-02, time/batch = 0.2693s	
2561/2700 (epoch 47.426), train_loss = 3.27154305, grad/param norm = 7.8365e-02, time/batch = 0.2698s	
2562/2700 (epoch 47.444), train_loss = 3.20713810, grad/param norm = 9.3032e-02, time/batch = 0.2685s	
2563/2700 (epoch 47.463), train_loss = 3.24446626, grad/param norm = 1.0485e-01, time/batch = 0.2688s	
2564/2700 (epoch 47.481), train_loss = 3.32435201, grad/param norm = 1.2094e-01, time/batch = 0.2703s	
2565/2700 (epoch 47.500), train_loss = 3.36251580, grad/param norm = 1.3630e-01, time/batch = 0.2694s	
2566/2700 (epoch 47.519), train_loss = 3.31850916, grad/param norm = 9.3486e-02, time/batch = 0.2689s	
2567/2700 (epoch 47.537), train_loss = 3.32286022, grad/param norm = 1.1051e-01, time/batch = 0.2696s	
2568/2700 (epoch 47.556), train_loss = 3.25607965, grad/param norm = 7.6171e-02, time/batch = 0.2688s	
2569/2700 (epoch 47.574), train_loss = 3.22999410, grad/param norm = 1.1361e-01, time/batch = 0.2694s	
2570/2700 (epoch 47.593), train_loss = 3.22287528, grad/param norm = 9.9998e-02, time/batch = 0.2691s	
2571/2700 (epoch 47.611), train_loss = 3.17059755, grad/param norm = 8.4767e-02, time/batch = 0.2700s	
2572/2700 (epoch 47.630), train_loss = 3.20965506, grad/param norm = 7.9984e-02, time/batch = 0.2689s	
2573/2700 (epoch 47.648), train_loss = 3.27220681, grad/param norm = 9.2763e-02, time/batch = 0.2691s	
2574/2700 (epoch 47.667), train_loss = 3.21035038, grad/param norm = 7.3394e-02, time/batch = 0.2696s	
2575/2700 (epoch 47.685), train_loss = 3.20861243, grad/param norm = 1.0970e-01, time/batch = 0.2691s	
2576/2700 (epoch 47.704), train_loss = 3.17955326, grad/param norm = 1.2249e-01, time/batch = 0.2691s	
2577/2700 (epoch 47.722), train_loss = 3.17039475, grad/param norm = 6.6534e-02, time/batch = 0.2691s	
2578/2700 (epoch 47.741), train_loss = 3.30362223, grad/param norm = 1.2644e-01, time/batch = 0.2690s	
2579/2700 (epoch 47.759), train_loss = 3.24931546, grad/param norm = 1.1504e-01, time/batch = 0.2694s	
2580/2700 (epoch 47.778), train_loss = 3.24148420, grad/param norm = 9.6460e-02, time/batch = 0.5731s	
2581/2700 (epoch 47.796), train_loss = 3.23447873, grad/param norm = 9.0671e-02, time/batch = 0.2698s	
2582/2700 (epoch 47.815), train_loss = 3.18911159, grad/param norm = 8.0800e-02, time/batch = 0.2690s	
2583/2700 (epoch 47.833), train_loss = 3.22625133, grad/param norm = 8.7564e-02, time/batch = 0.2690s	
2584/2700 (epoch 47.852), train_loss = 3.21300623, grad/param norm = 8.4420e-02, time/batch = 0.2693s	
2585/2700 (epoch 47.870), train_loss = 3.20838165, grad/param norm = 5.5946e-02, time/batch = 0.2694s	
2586/2700 (epoch 47.889), train_loss = 3.24802720, grad/param norm = 8.4434e-02, time/batch = 0.2690s	
2587/2700 (epoch 47.907), train_loss = 3.29906442, grad/param norm = 1.1873e-01, time/batch = 0.2691s	
2588/2700 (epoch 47.926), train_loss = 3.24655649, grad/param norm = 1.0988e-01, time/batch = 0.2685s	
2589/2700 (epoch 47.944), train_loss = 3.25552236, grad/param norm = 9.3006e-02, time/batch = 0.2693s	
2590/2700 (epoch 47.963), train_loss = 3.33746168, grad/param norm = 9.4454e-02, time/batch = 0.2691s	
2591/2700 (epoch 47.981), train_loss = 3.40154217, grad/param norm = 1.2618e-01, time/batch = 0.2702s	
decayed learning rate by a factor 0.97 to 0.00060971605649306	
2592/2700 (epoch 48.000), train_loss = 3.29199275, grad/param norm = 1.0158e-01, time/batch = 0.2688s	
2593/2700 (epoch 48.019), train_loss = 3.22982173, grad/param norm = 1.2543e-01, time/batch = 0.2692s	
2594/2700 (epoch 48.037), train_loss = 3.25006290, grad/param norm = 8.7234e-02, time/batch = 0.2694s	
2595/2700 (epoch 48.056), train_loss = 3.25886682, grad/param norm = 1.1543e-01, time/batch = 0.2695s	
2596/2700 (epoch 48.074), train_loss = 3.28505799, grad/param norm = 1.3127e-01, time/batch = 0.2691s	
2597/2700 (epoch 48.093), train_loss = 3.28666629, grad/param norm = 1.0715e-01, time/batch = 0.2689s	
2598/2700 (epoch 48.111), train_loss = 3.26133099, grad/param norm = 8.4309e-02, time/batch = 0.2688s	
2599/2700 (epoch 48.130), train_loss = 3.27600500, grad/param norm = 9.7188e-02, time/batch = 0.2691s	
2600/2700 (epoch 48.148), train_loss = 3.24174611, grad/param norm = 9.9548e-02, time/batch = 0.2694s	
2601/2700 (epoch 48.167), train_loss = 3.24689284, grad/param norm = 1.0676e-01, time/batch = 0.2704s	
2602/2700 (epoch 48.185), train_loss = 3.23501794, grad/param norm = 7.5871e-02, time/batch = 0.2688s	
2603/2700 (epoch 48.204), train_loss = 3.17229568, grad/param norm = 9.1627e-02, time/batch = 0.2690s	
2604/2700 (epoch 48.222), train_loss = 3.14539220, grad/param norm = 1.2438e-01, time/batch = 0.2696s	
2605/2700 (epoch 48.241), train_loss = 3.16657775, grad/param norm = 9.3657e-02, time/batch = 0.2694s	
2606/2700 (epoch 48.259), train_loss = 3.19902543, grad/param norm = 9.6707e-02, time/batch = 0.2694s	
2607/2700 (epoch 48.278), train_loss = 3.27172145, grad/param norm = 9.4416e-02, time/batch = 0.2697s	
2608/2700 (epoch 48.296), train_loss = 3.27267784, grad/param norm = 8.4774e-02, time/batch = 0.2690s	
2609/2700 (epoch 48.315), train_loss = 3.24862349, grad/param norm = 8.2539e-02, time/batch = 0.2695s	
2610/2700 (epoch 48.333), train_loss = 3.33355994, grad/param norm = 1.0044e-01, time/batch = 0.2696s	
2611/2700 (epoch 48.352), train_loss = 3.33501919, grad/param norm = 1.0578e-01, time/batch = 0.2705s	
2612/2700 (epoch 48.370), train_loss = 3.27792664, grad/param norm = 9.1695e-02, time/batch = 0.2688s	
2613/2700 (epoch 48.389), train_loss = 3.24963927, grad/param norm = 7.3708e-02, time/batch = 0.2697s	
2614/2700 (epoch 48.407), train_loss = 3.27117727, grad/param norm = 9.5815e-02, time/batch = 0.2696s	
2615/2700 (epoch 48.426), train_loss = 3.27140985, grad/param norm = 7.7501e-02, time/batch = 0.2696s	
2616/2700 (epoch 48.444), train_loss = 3.20712524, grad/param norm = 9.2740e-02, time/batch = 0.2699s	
2617/2700 (epoch 48.463), train_loss = 3.24433429, grad/param norm = 1.0365e-01, time/batch = 0.2693s	
2618/2700 (epoch 48.481), train_loss = 3.32425621, grad/param norm = 1.1953e-01, time/batch = 0.2693s	
2619/2700 (epoch 48.500), train_loss = 3.36230218, grad/param norm = 1.3428e-01, time/batch = 0.2689s	
2620/2700 (epoch 48.519), train_loss = 3.31845815, grad/param norm = 9.1943e-02, time/batch = 0.2694s	
2621/2700 (epoch 48.537), train_loss = 3.32280741, grad/param norm = 1.0957e-01, time/batch = 0.2701s	
2622/2700 (epoch 48.556), train_loss = 3.25599447, grad/param norm = 7.5610e-02, time/batch = 0.2686s	
2623/2700 (epoch 48.574), train_loss = 3.22996188, grad/param norm = 1.1369e-01, time/batch = 0.2693s	
2624/2700 (epoch 48.593), train_loss = 3.22269533, grad/param norm = 9.8671e-02, time/batch = 0.2694s	
2625/2700 (epoch 48.611), train_loss = 3.17060790, grad/param norm = 8.5021e-02, time/batch = 0.2694s	
2626/2700 (epoch 48.630), train_loss = 3.20959507, grad/param norm = 7.9001e-02, time/batch = 0.2691s	
2627/2700 (epoch 48.648), train_loss = 3.27204076, grad/param norm = 9.2340e-02, time/batch = 0.2695s	
2628/2700 (epoch 48.667), train_loss = 3.21033944, grad/param norm = 7.2922e-02, time/batch = 0.2690s	
2629/2700 (epoch 48.685), train_loss = 3.20851671, grad/param norm = 1.0948e-01, time/batch = 0.2696s	
2630/2700 (epoch 48.704), train_loss = 3.17944518, grad/param norm = 1.2131e-01, time/batch = 0.2691s	
2631/2700 (epoch 48.722), train_loss = 3.17033554, grad/param norm = 6.5093e-02, time/batch = 0.2703s	
2632/2700 (epoch 48.741), train_loss = 3.30344514, grad/param norm = 1.2540e-01, time/batch = 0.2688s	
2633/2700 (epoch 48.759), train_loss = 3.24914699, grad/param norm = 1.1382e-01, time/batch = 0.2693s	
2634/2700 (epoch 48.778), train_loss = 3.24134124, grad/param norm = 9.5455e-02, time/batch = 0.2692s	
2635/2700 (epoch 48.796), train_loss = 3.23439236, grad/param norm = 8.9808e-02, time/batch = 0.2693s	
2636/2700 (epoch 48.815), train_loss = 3.18906520, grad/param norm = 8.0554e-02, time/batch = 0.2689s	
2637/2700 (epoch 48.833), train_loss = 3.22616352, grad/param norm = 8.6951e-02, time/batch = 0.2689s	
2638/2700 (epoch 48.852), train_loss = 3.21291544, grad/param norm = 8.3527e-02, time/batch = 0.2690s	
2639/2700 (epoch 48.870), train_loss = 3.20836051, grad/param norm = 5.5590e-02, time/batch = 0.2694s	
2640/2700 (epoch 48.889), train_loss = 3.24794999, grad/param norm = 8.4042e-02, time/batch = 0.2694s	
2641/2700 (epoch 48.907), train_loss = 3.29894482, grad/param norm = 1.1790e-01, time/batch = 0.2704s	
2642/2700 (epoch 48.926), train_loss = 3.24649263, grad/param norm = 1.0923e-01, time/batch = 0.2685s	
2643/2700 (epoch 48.944), train_loss = 3.25538863, grad/param norm = 9.2104e-02, time/batch = 0.2689s	
2644/2700 (epoch 48.963), train_loss = 3.33739571, grad/param norm = 9.4123e-02, time/batch = 0.2695s	
2645/2700 (epoch 48.981), train_loss = 3.40149454, grad/param norm = 1.2579e-01, time/batch = 0.2693s	
decayed learning rate by a factor 0.97 to 0.00059142457479826	
2646/2700 (epoch 49.000), train_loss = 3.29181016, grad/param norm = 1.0060e-01, time/batch = 0.2692s	
2647/2700 (epoch 49.019), train_loss = 3.22950811, grad/param norm = 1.2463e-01, time/batch = 0.2691s	
2648/2700 (epoch 49.037), train_loss = 3.24993651, grad/param norm = 8.6388e-02, time/batch = 0.2689s	
2649/2700 (epoch 49.056), train_loss = 3.25881238, grad/param norm = 1.1491e-01, time/batch = 0.2691s	
2650/2700 (epoch 49.074), train_loss = 3.28489141, grad/param norm = 1.2966e-01, time/batch = 0.2695s	
2651/2700 (epoch 49.093), train_loss = 3.28648071, grad/param norm = 1.0549e-01, time/batch = 0.2706s	
2652/2700 (epoch 49.111), train_loss = 3.26128613, grad/param norm = 8.3764e-02, time/batch = 0.2696s	
2653/2700 (epoch 49.130), train_loss = 3.27588758, grad/param norm = 9.6745e-02, time/batch = 0.2693s	
2654/2700 (epoch 49.148), train_loss = 3.24169276, grad/param norm = 9.9200e-02, time/batch = 0.2699s	
2655/2700 (epoch 49.167), train_loss = 3.24675330, grad/param norm = 1.0554e-01, time/batch = 0.2698s	
2656/2700 (epoch 49.185), train_loss = 3.23496479, grad/param norm = 7.5792e-02, time/batch = 0.2693s	
2657/2700 (epoch 49.204), train_loss = 3.17225524, grad/param norm = 9.1396e-02, time/batch = 0.2691s	
2658/2700 (epoch 49.222), train_loss = 3.14526443, grad/param norm = 1.2371e-01, time/batch = 0.2694s	
2659/2700 (epoch 49.241), train_loss = 3.16654420, grad/param norm = 9.3586e-02, time/batch = 0.2692s	
2660/2700 (epoch 49.259), train_loss = 3.19895326, grad/param norm = 9.6037e-02, time/batch = 0.2693s	
2661/2700 (epoch 49.278), train_loss = 3.27165882, grad/param norm = 9.4293e-02, time/batch = 0.2700s	
2662/2700 (epoch 49.296), train_loss = 3.27257614, grad/param norm = 8.3895e-02, time/batch = 0.2688s	
2663/2700 (epoch 49.315), train_loss = 3.24849159, grad/param norm = 8.1592e-02, time/batch = 0.2695s	
2664/2700 (epoch 49.333), train_loss = 3.33353265, grad/param norm = 1.0008e-01, time/batch = 0.2700s	
2665/2700 (epoch 49.352), train_loss = 3.33480661, grad/param norm = 1.0464e-01, time/batch = 0.2699s	
2666/2700 (epoch 49.370), train_loss = 3.27778109, grad/param norm = 9.0814e-02, time/batch = 0.2697s	
2667/2700 (epoch 49.389), train_loss = 3.24962108, grad/param norm = 7.3593e-02, time/batch = 0.2692s	
2668/2700 (epoch 49.407), train_loss = 3.27110756, grad/param norm = 9.5565e-02, time/batch = 0.2687s	
2669/2700 (epoch 49.426), train_loss = 3.27128166, grad/param norm = 7.6672e-02, time/batch = 0.2698s	
2670/2700 (epoch 49.444), train_loss = 3.20710932, grad/param norm = 9.2419e-02, time/batch = 0.2693s	
2671/2700 (epoch 49.463), train_loss = 3.24420634, grad/param norm = 1.0246e-01, time/batch = 0.2700s	
2672/2700 (epoch 49.481), train_loss = 3.32416368, grad/param norm = 1.1815e-01, time/batch = 0.2689s	
2673/2700 (epoch 49.500), train_loss = 3.36210249, grad/param norm = 1.3238e-01, time/batch = 0.2690s	
2674/2700 (epoch 49.519), train_loss = 3.31841620, grad/param norm = 9.0528e-02, time/batch = 0.2695s	
2675/2700 (epoch 49.537), train_loss = 3.32275953, grad/param norm = 1.0867e-01, time/batch = 0.2694s	
2676/2700 (epoch 49.556), train_loss = 3.25591789, grad/param norm = 7.5087e-02, time/batch = 0.2690s	
2677/2700 (epoch 49.574), train_loss = 3.22992978, grad/param norm = 1.1377e-01, time/batch = 0.2693s	
2678/2700 (epoch 49.593), train_loss = 3.22252121, grad/param norm = 9.7372e-02, time/batch = 0.2690s	
2679/2700 (epoch 49.611), train_loss = 3.17061621, grad/param norm = 8.5268e-02, time/batch = 0.2693s	
2680/2700 (epoch 49.630), train_loss = 3.20953692, grad/param norm = 7.8042e-02, time/batch = 0.2694s	
2681/2700 (epoch 49.648), train_loss = 3.27188372, grad/param norm = 9.1939e-02, time/batch = 0.2699s	
2682/2700 (epoch 49.667), train_loss = 3.21033456, grad/param norm = 7.2483e-02, time/batch = 0.2686s	
2683/2700 (epoch 49.685), train_loss = 3.20841927, grad/param norm = 1.0927e-01, time/batch = 0.2689s	
2684/2700 (epoch 49.704), train_loss = 3.17934208, grad/param norm = 1.2015e-01, time/batch = 0.2694s	
2685/2700 (epoch 49.722), train_loss = 3.17028175, grad/param norm = 6.3727e-02, time/batch = 0.2692s	
2686/2700 (epoch 49.741), train_loss = 3.30327092, grad/param norm = 1.2440e-01, time/batch = 0.2687s	
2687/2700 (epoch 49.759), train_loss = 3.24898560, grad/param norm = 1.1265e-01, time/batch = 0.2689s	
2688/2700 (epoch 49.778), train_loss = 3.24120402, grad/param norm = 9.4500e-02, time/batch = 0.2687s	
2689/2700 (epoch 49.796), train_loss = 3.23431131, grad/param norm = 8.8981e-02, time/batch = 0.2690s	
2690/2700 (epoch 49.815), train_loss = 3.18902196, grad/param norm = 8.0317e-02, time/batch = 0.2694s	
2691/2700 (epoch 49.833), train_loss = 3.22608021, grad/param norm = 8.6352e-02, time/batch = 0.2699s	
2692/2700 (epoch 49.852), train_loss = 3.21282908, grad/param norm = 8.2656e-02, time/batch = 0.2685s	
2693/2700 (epoch 49.870), train_loss = 3.20834360, grad/param norm = 5.5271e-02, time/batch = 0.2687s	
2694/2700 (epoch 49.889), train_loss = 3.24787415, grad/param norm = 8.3679e-02, time/batch = 0.2695s	
2695/2700 (epoch 49.907), train_loss = 3.29882807, grad/param norm = 1.1711e-01, time/batch = 0.2692s	
2696/2700 (epoch 49.926), train_loss = 3.24643254, grad/param norm = 1.0860e-01, time/batch = 0.2691s	
2697/2700 (epoch 49.944), train_loss = 3.25526211, grad/param norm = 9.1246e-02, time/batch = 0.2693s	
2698/2700 (epoch 49.963), train_loss = 3.33733171, grad/param norm = 9.3823e-02, time/batch = 0.2686s	
2699/2700 (epoch 49.981), train_loss = 3.40144318, grad/param norm = 1.2537e-01, time/batch = 0.2692s	
decayed learning rate by a factor 0.97 to 0.00057368183755432	
evaluating loss over split index 2	
1/3...	
2/3...	
3/3...	
saving checkpoint to cv/lm_lstm_epoch50.00_3.2051.t7	
2700/2700 (epoch 50.000), train_loss = 3.29163694, grad/param norm = 9.9673e-02, time/batch = 0.2692s	
