using CUDA on GPU 0...	
loading data files...	
cutting off end of data so that the batches/sequences divide evenly	
reshaping tensor...	
data load done. Number of data batches in train: 54, val: 3, test: 0	
vocab size: 91	
creating an lstm with 2 layers	
setting forget gate biases to 1 in LSTM layer 1	
setting forget gate biases to 1 in LSTM layer 2	
number of parameters in the model: 3386971	
cloning rnn	
cloning criterion	
1/2700 (epoch 0.019), train_loss = 4.55787950, grad/param norm = 9.2292e-01, time/batch = 0.7909s	
2/2700 (epoch 0.037), train_loss = 3.64063436, grad/param norm = 1.2964e+00, time/batch = 0.2176s	
3/2700 (epoch 0.056), train_loss = 4.47687248, grad/param norm = 9.4616e-01, time/batch = 0.3355s	
4/2700 (epoch 0.074), train_loss = 3.68064919, grad/param norm = 1.3503e+00, time/batch = 0.3601s	
5/2700 (epoch 0.093), train_loss = 3.74743302, grad/param norm = 1.1498e+00, time/batch = 0.4029s	
6/2700 (epoch 0.111), train_loss = 3.68466388, grad/param norm = 1.0490e+00, time/batch = 0.3940s	
7/2700 (epoch 0.130), train_loss = 3.36403574, grad/param norm = 5.7608e-01, time/batch = 0.4106s	
8/2700 (epoch 0.148), train_loss = 3.27261929, grad/param norm = 4.6561e-01, time/batch = 0.4175s	
9/2700 (epoch 0.167), train_loss = 3.29474730, grad/param norm = 4.7872e-01, time/batch = 0.4031s	
10/2700 (epoch 0.185), train_loss = 3.25870148, grad/param norm = 3.5131e-01, time/batch = 0.3906s	
11/2700 (epoch 0.204), train_loss = 3.18958973, grad/param norm = 3.4023e-01, time/batch = 0.4081s	
12/2700 (epoch 0.222), train_loss = 3.16002901, grad/param norm = 3.2421e-01, time/batch = 0.4437s	
13/2700 (epoch 0.241), train_loss = 3.18115962, grad/param norm = 2.9649e-01, time/batch = 0.4628s	
14/2700 (epoch 0.259), train_loss = 3.21531937, grad/param norm = 2.8871e-01, time/batch = 0.4787s	
15/2700 (epoch 0.278), train_loss = 3.28778544, grad/param norm = 2.9225e-01, time/batch = 0.4592s	
16/2700 (epoch 0.296), train_loss = 3.29367017, grad/param norm = 3.1947e-01, time/batch = 0.4010s	
17/2700 (epoch 0.315), train_loss = 3.26900075, grad/param norm = 3.0860e-01, time/batch = 0.3716s	
18/2700 (epoch 0.333), train_loss = 3.34627197, grad/param norm = 2.8587e-01, time/batch = 0.4211s	
19/2700 (epoch 0.352), train_loss = 3.35377964, grad/param norm = 3.0249e-01, time/batch = 0.4300s	
20/2700 (epoch 0.370), train_loss = 3.29179969, grad/param norm = 2.2009e-01, time/batch = 0.4041s	
21/2700 (epoch 0.389), train_loss = 3.25622221, grad/param norm = 1.8901e-01, time/batch = 0.3700s	
22/2700 (epoch 0.407), train_loss = 3.28046647, grad/param norm = 1.7608e-01, time/batch = 0.4060s	
23/2700 (epoch 0.426), train_loss = 3.28313706, grad/param norm = 1.8431e-01, time/batch = 0.4551s	
24/2700 (epoch 0.444), train_loss = 3.20829784, grad/param norm = 1.6548e-01, time/batch = 0.4610s	
25/2700 (epoch 0.463), train_loss = 3.25389465, grad/param norm = 2.1279e-01, time/batch = 0.4766s	
26/2700 (epoch 0.481), train_loss = 3.32924955, grad/param norm = 2.1030e-01, time/batch = 0.4494s	
27/2700 (epoch 0.500), train_loss = 3.37388351, grad/param norm = 2.7374e-01, time/batch = 0.4253s	
28/2700 (epoch 0.519), train_loss = 3.32906218, grad/param norm = 2.8853e-01, time/batch = 0.4132s	
29/2700 (epoch 0.537), train_loss = 3.33318912, grad/param norm = 3.1109e-01, time/batch = 0.4042s	
30/2700 (epoch 0.556), train_loss = 3.27615429, grad/param norm = 2.9717e-01, time/batch = 0.4451s	
31/2700 (epoch 0.574), train_loss = 3.23657744, grad/param norm = 3.0169e-01, time/batch = 0.4433s	
32/2700 (epoch 0.593), train_loss = 3.24461682, grad/param norm = 3.7007e-01, time/batch = 0.4036s	
33/2700 (epoch 0.611), train_loss = 3.18284967, grad/param norm = 3.0720e-01, time/batch = 0.3678s	
34/2700 (epoch 0.630), train_loss = 3.22156410, grad/param norm = 3.1793e-01, time/batch = 0.3852s	
35/2700 (epoch 0.648), train_loss = 3.29550156, grad/param norm = 3.1179e-01, time/batch = 0.4262s	
36/2700 (epoch 0.667), train_loss = 3.22347876, grad/param norm = 2.8371e-01, time/batch = 0.4659s	
37/2700 (epoch 0.685), train_loss = 3.21734303, grad/param norm = 2.5165e-01, time/batch = 0.4603s	
38/2700 (epoch 0.704), train_loss = 3.19184368, grad/param norm = 3.1174e-01, time/batch = 0.4335s	
39/2700 (epoch 0.722), train_loss = 3.18248242, grad/param norm = 2.7354e-01, time/batch = 0.4148s	
40/2700 (epoch 0.741), train_loss = 3.31166873, grad/param norm = 2.1870e-01, time/batch = 0.4202s	
41/2700 (epoch 0.759), train_loss = 3.26076634, grad/param norm = 2.4031e-01, time/batch = 0.4214s	
42/2700 (epoch 0.778), train_loss = 3.25402109, grad/param norm = 2.5475e-01, time/batch = 0.4352s	
43/2700 (epoch 0.796), train_loss = 3.24715762, grad/param norm = 2.6397e-01, time/batch = 0.4122s	
44/2700 (epoch 0.815), train_loss = 3.20177021, grad/param norm = 2.8512e-01, time/batch = 0.3767s	
45/2700 (epoch 0.833), train_loss = 3.23796872, grad/param norm = 2.7783e-01, time/batch = 0.3773s	
46/2700 (epoch 0.852), train_loss = 3.22519936, grad/param norm = 2.7567e-01, time/batch = 0.4296s	
47/2700 (epoch 0.870), train_loss = 3.22104367, grad/param norm = 2.5704e-01, time/batch = 0.4641s	
48/2700 (epoch 0.889), train_loss = 3.25724750, grad/param norm = 2.8458e-01, time/batch = 0.4631s	
49/2700 (epoch 0.907), train_loss = 3.31496494, grad/param norm = 3.6731e-01, time/batch = 0.4312s	
50/2700 (epoch 0.926), train_loss = 3.27041143, grad/param norm = 4.0152e-01, time/batch = 0.4089s	
51/2700 (epoch 0.944), train_loss = 3.27532856, grad/param norm = 3.8579e-01, time/batch = 0.4047s	
52/2700 (epoch 0.963), train_loss = 3.35000232, grad/param norm = 3.6067e-01, time/batch = 0.4333s	
53/2700 (epoch 0.981), train_loss = 3.40610160, grad/param norm = 2.9975e-01, time/batch = 0.4341s	
54/2700 (epoch 1.000), train_loss = 3.31780071, grad/param norm = 3.4380e-01, time/batch = 0.4114s	
55/2700 (epoch 1.019), train_loss = 3.25559619, grad/param norm = 3.5514e-01, time/batch = 0.3641s	
56/2700 (epoch 1.037), train_loss = 3.26382738, grad/param norm = 3.1597e-01, time/batch = 0.3919s	
57/2700 (epoch 1.056), train_loss = 3.26292450, grad/param norm = 2.4801e-01, time/batch = 0.4366s	
58/2700 (epoch 1.074), train_loss = 3.29657830, grad/param norm = 2.6767e-01, time/batch = 0.4691s	
59/2700 (epoch 1.093), train_loss = 3.30098737, grad/param norm = 2.7600e-01, time/batch = 0.4722s	
60/2700 (epoch 1.111), train_loss = 3.26905481, grad/param norm = 2.4796e-01, time/batch = 0.4521s	
61/2700 (epoch 1.130), train_loss = 3.28159944, grad/param norm = 2.0747e-01, time/batch = 0.4377s	
62/2700 (epoch 1.148), train_loss = 3.24533087, grad/param norm = 2.8850e-01, time/batch = 0.3880s	
63/2700 (epoch 1.167), train_loss = 3.25927129, grad/param norm = 3.3792e-01, time/batch = 0.4175s	
64/2700 (epoch 1.185), train_loss = 3.24405350, grad/param norm = 3.0924e-01, time/batch = 0.4523s	
65/2700 (epoch 1.204), train_loss = 3.17465369, grad/param norm = 2.7145e-01, time/batch = 0.4448s	
66/2700 (epoch 1.222), train_loss = 3.15679225, grad/param norm = 3.8235e-01, time/batch = 0.4244s	
67/2700 (epoch 1.241), train_loss = 3.18336146, grad/param norm = 3.7309e-01, time/batch = 0.3879s	
68/2700 (epoch 1.259), train_loss = 3.23678500, grad/param norm = 5.5248e-01, time/batch = 0.3514s	
69/2700 (epoch 1.278), train_loss = 3.32028041, grad/param norm = 5.8292e-01, time/batch = 0.4522s	
70/2700 (epoch 1.296), train_loss = 3.28832674, grad/param norm = 4.1768e-01, time/batch = 0.4597s	
71/2700 (epoch 1.315), train_loss = 3.25741559, grad/param norm = 3.1696e-01, time/batch = 0.4516s	
72/2700 (epoch 1.333), train_loss = 3.33488962, grad/param norm = 2.8117e-01, time/batch = 0.4223s	
73/2700 (epoch 1.352), train_loss = 3.34064623, grad/param norm = 3.1280e-01, time/batch = 0.4067s	
74/2700 (epoch 1.370), train_loss = 3.28077355, grad/param norm = 2.6856e-01, time/batch = 0.4182s	
75/2700 (epoch 1.389), train_loss = 3.23919568, grad/param norm = 1.9847e-01, time/batch = 0.4521s	
76/2700 (epoch 1.407), train_loss = 3.26065429, grad/param norm = 1.7787e-01, time/batch = 0.4502s	
77/2700 (epoch 1.426), train_loss = 3.26582686, grad/param norm = 2.2445e-01, time/batch = 0.4261s	
78/2700 (epoch 1.444), train_loss = 3.19141291, grad/param norm = 2.2895e-01, time/batch = 0.3350s	
79/2700 (epoch 1.463), train_loss = 3.25276190, grad/param norm = 3.3652e-01, time/batch = 0.3996s	
80/2700 (epoch 1.481), train_loss = 3.34012390, grad/param norm = 4.2327e-01, time/batch = 0.4555s	
81/2700 (epoch 1.500), train_loss = 3.37433490, grad/param norm = 3.8715e-01, time/batch = 0.4443s	
82/2700 (epoch 1.519), train_loss = 3.31215082, grad/param norm = 4.0989e-01, time/batch = 0.4174s	
83/2700 (epoch 1.537), train_loss = 3.33744755, grad/param norm = 4.7928e-01, time/batch = 0.3990s	
84/2700 (epoch 1.556), train_loss = 3.25325758, grad/param norm = 4.4577e-01, time/batch = 0.4294s	
85/2700 (epoch 1.574), train_loss = 3.22495052, grad/param norm = 4.0289e-01, time/batch = 0.4577s	
86/2700 (epoch 1.593), train_loss = 3.21244757, grad/param norm = 3.4018e-01, time/batch = 0.4659s	
87/2700 (epoch 1.611), train_loss = 3.14429589, grad/param norm = 2.7310e-01, time/batch = 0.4525s	
88/2700 (epoch 1.630), train_loss = 3.17326318, grad/param norm = 2.4277e-01, time/batch = 0.3966s	
89/2700 (epoch 1.648), train_loss = 3.24741167, grad/param norm = 3.1574e-01, time/batch = 0.3613s	
90/2700 (epoch 1.667), train_loss = 3.21993645, grad/param norm = 4.0358e-01, time/batch = 0.3824s	
91/2700 (epoch 1.685), train_loss = 3.21625692, grad/param norm = 4.7973e-01, time/batch = 0.4387s	
92/2700 (epoch 1.704), train_loss = 3.17495278, grad/param norm = 4.3183e-01, time/batch = 0.4356s	
93/2700 (epoch 1.722), train_loss = 3.13298831, grad/param norm = 3.1598e-01, time/batch = 0.4173s	
94/2700 (epoch 1.741), train_loss = 3.27473194, grad/param norm = 4.4520e-01, time/batch = 0.4077s	
95/2700 (epoch 1.759), train_loss = 3.22328747, grad/param norm = 3.9872e-01, time/batch = 0.4385s	
96/2700 (epoch 1.778), train_loss = 3.18364419, grad/param norm = 2.5349e-01, time/batch = 0.4627s	
97/2700 (epoch 1.796), train_loss = 3.17300272, grad/param norm = 2.2119e-01, time/batch = 0.4651s	
98/2700 (epoch 1.815), train_loss = 3.11440747, grad/param norm = 2.8789e-01, time/batch = 0.4402s	
99/2700 (epoch 1.833), train_loss = 3.17027951, grad/param norm = 4.1531e-01, time/batch = 0.3845s	
100/2700 (epoch 1.852), train_loss = 3.18999794, grad/param norm = 5.0776e-01, time/batch = 0.3658s	
101/2700 (epoch 1.870), train_loss = 3.13750637, grad/param norm = 3.7464e-01, time/batch = 0.3687s	
102/2700 (epoch 1.889), train_loss = 3.14949259, grad/param norm = 3.1242e-01, time/batch = 0.4298s	
103/2700 (epoch 1.907), train_loss = 3.19616538, grad/param norm = 2.7665e-01, time/batch = 0.4202s	
104/2700 (epoch 1.926), train_loss = 3.14410082, grad/param norm = 2.7313e-01, time/batch = 0.4071s	
105/2700 (epoch 1.944), train_loss = 3.19583510, grad/param norm = 4.9146e-01, time/batch = 0.4300s	
106/2700 (epoch 1.963), train_loss = 3.35426544, grad/param norm = 5.2758e-01, time/batch = 0.4554s	
107/2700 (epoch 1.981), train_loss = 3.30748547, grad/param norm = 3.3397e-01, time/batch = 0.4573s	
108/2700 (epoch 2.000), train_loss = 3.20517864, grad/param norm = 2.8174e-01, time/batch = 0.4163s	
109/2700 (epoch 2.019), train_loss = 3.08836339, grad/param norm = 2.0418e-01, time/batch = 0.3839s	
110/2700 (epoch 2.037), train_loss = 3.10209630, grad/param norm = 1.6143e-01, time/batch = 0.3764s	
111/2700 (epoch 2.056), train_loss = 3.07859110, grad/param norm = 1.9504e-01, time/batch = 0.3972s	
112/2700 (epoch 2.074), train_loss = 3.14691866, grad/param norm = 4.0106e-01, time/batch = 0.4207s	
113/2700 (epoch 2.093), train_loss = 3.23254704, grad/param norm = 5.2357e-01, time/batch = 0.4509s	
114/2700 (epoch 2.111), train_loss = 3.11583885, grad/param norm = 2.8843e-01, time/batch = 0.4397s	
115/2700 (epoch 2.130), train_loss = 3.09979147, grad/param norm = 2.2369e-01, time/batch = 0.3823s	
116/2700 (epoch 2.148), train_loss = 3.02017963, grad/param norm = 1.5928e-01, time/batch = 0.4616s	
117/2700 (epoch 2.167), train_loss = 3.04420643, grad/param norm = 2.6227e-01, time/batch = 0.4548s	
118/2700 (epoch 2.185), train_loss = 3.06558537, grad/param norm = 3.1501e-01, time/batch = 0.4102s	
119/2700 (epoch 2.204), train_loss = 3.02817114, grad/param norm = 4.5874e-01, time/batch = 0.3767s	
120/2700 (epoch 2.222), train_loss = 3.00993936, grad/param norm = 4.4080e-01, time/batch = 0.3763s	
121/2700 (epoch 2.241), train_loss = 3.05417765, grad/param norm = 5.8579e-01, time/batch = 0.3973s	
122/2700 (epoch 2.259), train_loss = 3.08777440, grad/param norm = 4.9595e-01, time/batch = 0.4273s	
123/2700 (epoch 2.278), train_loss = 3.03616109, grad/param norm = 1.8296e-01, time/batch = 0.4348s	
124/2700 (epoch 2.296), train_loss = 3.00001702, grad/param norm = 1.2895e-01, time/batch = 0.4151s	
125/2700 (epoch 2.315), train_loss = 2.98430694, grad/param norm = 1.4768e-01, time/batch = 0.4182s	
126/2700 (epoch 2.333), train_loss = 3.05362675, grad/param norm = 2.6398e-01, time/batch = 0.4455s	
127/2700 (epoch 2.352), train_loss = 3.22040496, grad/param norm = 4.9637e-01, time/batch = 0.4345s	
128/2700 (epoch 2.370), train_loss = 3.09017437, grad/param norm = 3.7465e-01, time/batch = 0.4652s	
129/2700 (epoch 2.389), train_loss = 3.00425364, grad/param norm = 3.1368e-01, time/batch = 0.4142s	
130/2700 (epoch 2.407), train_loss = 3.00392490, grad/param norm = 3.3836e-01, time/batch = 0.3585s	
131/2700 (epoch 2.426), train_loss = 3.01080133, grad/param norm = 2.6134e-01, time/batch = 0.3598s	
132/2700 (epoch 2.444), train_loss = 2.88444522, grad/param norm = 2.4688e-01, time/batch = 0.4107s	
133/2700 (epoch 2.463), train_loss = 2.95001705, grad/param norm = 2.9953e-01, time/batch = 0.4513s	
134/2700 (epoch 2.481), train_loss = 3.02912210, grad/param norm = 3.2224e-01, time/batch = 0.4397s	
135/2700 (epoch 2.500), train_loss = 3.13769126, grad/param norm = 5.0486e-01, time/batch = 0.4366s	
136/2700 (epoch 2.519), train_loss = 3.08538990, grad/param norm = 3.9688e-01, time/batch = 0.4143s	
137/2700 (epoch 2.537), train_loss = 2.97262881, grad/param norm = 1.7936e-01, time/batch = 0.4358s	
138/2700 (epoch 2.556), train_loss = 2.90254439, grad/param norm = 1.4608e-01, time/batch = 0.4332s	
139/2700 (epoch 2.574), train_loss = 2.83259353, grad/param norm = 1.5787e-01, time/batch = 0.4381s	
140/2700 (epoch 2.593), train_loss = 2.85287113, grad/param norm = 2.8417e-01, time/batch = 0.3934s	
141/2700 (epoch 2.611), train_loss = 2.87085561, grad/param norm = 3.2476e-01, time/batch = 0.3689s	
142/2700 (epoch 2.630), train_loss = 2.89495512, grad/param norm = 3.7712e-01, time/batch = 0.3651s	
143/2700 (epoch 2.648), train_loss = 2.87812250, grad/param norm = 3.0429e-01, time/batch = 0.4320s	
144/2700 (epoch 2.667), train_loss = 2.79328117, grad/param norm = 3.0383e-01, time/batch = 0.4532s	
145/2700 (epoch 2.685), train_loss = 2.82048692, grad/param norm = 3.1072e-01, time/batch = 0.4353s	
146/2700 (epoch 2.704), train_loss = 2.73514669, grad/param norm = 2.7205e-01, time/batch = 0.4296s	
147/2700 (epoch 2.722), train_loss = 2.71755264, grad/param norm = 2.2858e-01, time/batch = 0.4236s	
148/2700 (epoch 2.741), train_loss = 2.89335808, grad/param norm = 2.4365e-01, time/batch = 0.4342s	
149/2700 (epoch 2.759), train_loss = 2.82473456, grad/param norm = 2.3830e-01, time/batch = 0.4380s	
150/2700 (epoch 2.778), train_loss = 2.83886283, grad/param norm = 3.0788e-01, time/batch = 0.4475s	
151/2700 (epoch 2.796), train_loss = 2.97169664, grad/param norm = 4.7996e-01, time/batch = 0.4545s	
152/2700 (epoch 2.815), train_loss = 2.86146161, grad/param norm = 3.1467e-01, time/batch = 0.3950s	
153/2700 (epoch 2.833), train_loss = 2.75242739, grad/param norm = 2.1765e-01, time/batch = 0.3746s	
154/2700 (epoch 2.852), train_loss = 2.75594726, grad/param norm = 1.7427e-01, time/batch = 0.3836s	
155/2700 (epoch 2.870), train_loss = 2.70834968, grad/param norm = 1.5664e-01, time/batch = 0.4271s	
156/2700 (epoch 2.889), train_loss = 2.74501658, grad/param norm = 2.0384e-01, time/batch = 0.4448s	
157/2700 (epoch 2.907), train_loss = 2.84152725, grad/param norm = 2.3210e-01, time/batch = 0.4339s	
158/2700 (epoch 2.926), train_loss = 2.79946134, grad/param norm = 3.3173e-01, time/batch = 0.4228s	
159/2700 (epoch 2.944), train_loss = 2.90506516, grad/param norm = 4.0090e-01, time/batch = 0.4256s	
160/2700 (epoch 2.963), train_loss = 2.95501436, grad/param norm = 3.1420e-01, time/batch = 0.4397s	
161/2700 (epoch 2.981), train_loss = 2.97464168, grad/param norm = 3.1531e-01, time/batch = 0.4564s	
162/2700 (epoch 3.000), train_loss = 2.93567537, grad/param norm = 3.2681e-01, time/batch = 0.4632s	
163/2700 (epoch 3.019), train_loss = 2.81004979, grad/param norm = 2.4469e-01, time/batch = 0.4390s	
164/2700 (epoch 3.037), train_loss = 2.77356258, grad/param norm = 1.8918e-01, time/batch = 0.3538s	
165/2700 (epoch 3.056), train_loss = 2.73411734, grad/param norm = 2.1011e-01, time/batch = 0.4069s	
166/2700 (epoch 3.074), train_loss = 2.76708782, grad/param norm = 1.9592e-01, time/batch = 0.4244s	
167/2700 (epoch 3.093), train_loss = 2.74471644, grad/param norm = 1.7683e-01, time/batch = 0.4070s	
168/2700 (epoch 3.111), train_loss = 2.69634799, grad/param norm = 1.3787e-01, time/batch = 0.4067s	
169/2700 (epoch 3.130), train_loss = 2.71942292, grad/param norm = 1.6707e-01, time/batch = 0.4155s	
170/2700 (epoch 3.148), train_loss = 2.68105824, grad/param norm = 2.2778e-01, time/batch = 0.4505s	
171/2700 (epoch 3.167), train_loss = 2.76140574, grad/param norm = 3.6004e-01, time/batch = 0.4540s	
172/2700 (epoch 3.185), train_loss = 2.69830452, grad/param norm = 2.0840e-01, time/batch = 0.4510s	
173/2700 (epoch 3.204), train_loss = 2.62928976, grad/param norm = 1.6813e-01, time/batch = 0.4517s	
174/2700 (epoch 3.222), train_loss = 2.56232443, grad/param norm = 1.8037e-01, time/batch = 0.4134s	
175/2700 (epoch 3.241), train_loss = 2.57471444, grad/param norm = 1.7743e-01, time/batch = 0.3704s	
176/2700 (epoch 3.259), train_loss = 2.60477297, grad/param norm = 2.3521e-01, time/batch = 0.3746s	
177/2700 (epoch 3.278), train_loss = 2.73023756, grad/param norm = 3.6602e-01, time/batch = 0.4348s	
178/2700 (epoch 3.296), train_loss = 2.80096262, grad/param norm = 4.0233e-01, time/batch = 0.4055s	
179/2700 (epoch 3.315), train_loss = 2.79116027, grad/param norm = 3.4671e-01, time/batch = 0.4118s	
180/2700 (epoch 3.333), train_loss = 2.89144475, grad/param norm = 3.8886e-01, time/batch = 0.4269s	
181/2700 (epoch 3.352), train_loss = 2.79769848, grad/param norm = 2.3461e-01, time/batch = 0.4312s	
182/2700 (epoch 3.370), train_loss = 2.66760990, grad/param norm = 1.5941e-01, time/batch = 0.4538s	
183/2700 (epoch 3.389), train_loss = 2.65203569, grad/param norm = 1.8488e-01, time/batch = 0.4553s	
184/2700 (epoch 3.407), train_loss = 2.63240259, grad/param norm = 1.6217e-01, time/batch = 0.4186s	
185/2700 (epoch 3.426), train_loss = 2.65941131, grad/param norm = 1.4474e-01, time/batch = 0.3823s	
186/2700 (epoch 3.444), train_loss = 2.54653279, grad/param norm = 1.9191e-01, time/batch = 0.3735s	
187/2700 (epoch 3.463), train_loss = 2.72424300, grad/param norm = 2.7120e-01, time/batch = 0.4259s	
188/2700 (epoch 3.481), train_loss = 2.73963136, grad/param norm = 2.6344e-01, time/batch = 0.4459s	
189/2700 (epoch 3.500), train_loss = 2.73535162, grad/param norm = 2.1480e-01, time/batch = 0.3706s	
190/2700 (epoch 3.519), train_loss = 2.66195790, grad/param norm = 2.1008e-01, time/batch = 0.4329s	
191/2700 (epoch 3.537), train_loss = 2.63554855, grad/param norm = 2.2971e-01, time/batch = 0.4418s	
192/2700 (epoch 3.556), train_loss = 3.15022138, grad/param norm = 1.5927e+00, time/batch = 0.4561s	
193/2700 (epoch 3.574), train_loss = 2.84850113, grad/param norm = 4.2685e-01, time/batch = 0.4753s	
194/2700 (epoch 3.593), train_loss = 2.74926215, grad/param norm = 4.5774e-01, time/batch = 0.4487s	
195/2700 (epoch 3.611), train_loss = 2.60766776, grad/param norm = 3.2985e-01, time/batch = 0.4204s	
196/2700 (epoch 3.630), train_loss = 2.56561230, grad/param norm = 1.5762e-01, time/batch = 0.3784s	
197/2700 (epoch 3.648), train_loss = 2.58951905, grad/param norm = 1.0526e-01, time/batch = 0.3927s	
198/2700 (epoch 3.667), train_loss = 2.49464131, grad/param norm = 9.2321e-02, time/batch = 0.4235s	
199/2700 (epoch 3.685), train_loss = 2.51460952, grad/param norm = 1.0361e-01, time/batch = 0.4436s	
200/2700 (epoch 3.704), train_loss = 2.49603567, grad/param norm = 1.1857e-01, time/batch = 0.3962s	
201/2700 (epoch 3.722), train_loss = 2.48737984, grad/param norm = 1.1318e-01, time/batch = 0.3731s	
202/2700 (epoch 3.741), train_loss = 2.67531366, grad/param norm = 1.4765e-01, time/batch = 0.4486s	
203/2700 (epoch 3.759), train_loss = 2.61459804, grad/param norm = 1.9758e-01, time/batch = 0.4714s	
204/2700 (epoch 3.778), train_loss = 2.59294298, grad/param norm = 1.7857e-01, time/batch = 0.4577s	
205/2700 (epoch 3.796), train_loss = 2.52628023, grad/param norm = 1.5923e-01, time/batch = 0.4421s	
206/2700 (epoch 3.815), train_loss = 2.50825854, grad/param norm = 1.6425e-01, time/batch = 0.4060s	
207/2700 (epoch 3.833), train_loss = 2.53505557, grad/param norm = 1.8977e-01, time/batch = 0.3912s	
208/2700 (epoch 3.852), train_loss = 2.53867834, grad/param norm = 1.4647e-01, time/batch = 0.4047s	
209/2700 (epoch 3.870), train_loss = 2.49461198, grad/param norm = 1.3836e-01, time/batch = 0.4344s	
210/2700 (epoch 3.889), train_loss = 2.50801106, grad/param norm = 1.5621e-01, time/batch = 0.4373s	
211/2700 (epoch 3.907), train_loss = 2.64366479, grad/param norm = 2.1877e-01, time/batch = 0.3926s	
212/2700 (epoch 3.926), train_loss = 2.60869705, grad/param norm = 3.0639e-01, time/batch = 0.3990s	
213/2700 (epoch 3.944), train_loss = 2.62226761, grad/param norm = 2.6997e-01, time/batch = 0.4056s	
214/2700 (epoch 3.963), train_loss = 2.60682084, grad/param norm = 1.7130e-01, time/batch = 0.4686s	
215/2700 (epoch 3.981), train_loss = 2.88067002, grad/param norm = 2.1986e+00, time/batch = 0.4828s	
216/2700 (epoch 4.000), train_loss = 2.82890429, grad/param norm = 2.9070e-01, time/batch = 0.4601s	
217/2700 (epoch 4.019), train_loss = 2.60039660, grad/param norm = 2.3705e-01, time/batch = 0.4304s	
218/2700 (epoch 4.037), train_loss = 2.62215083, grad/param norm = 2.3361e-01, time/batch = 0.4022s	
219/2700 (epoch 4.056), train_loss = 2.56977499, grad/param norm = 1.8925e-01, time/batch = 0.3999s	
220/2700 (epoch 4.074), train_loss = 2.56522904, grad/param norm = 1.5451e-01, time/batch = 0.4122s	
221/2700 (epoch 4.093), train_loss = 2.54811509, grad/param norm = 1.2734e-01, time/batch = 0.4215s	
222/2700 (epoch 4.111), train_loss = 2.51375654, grad/param norm = 1.4486e-01, time/batch = 0.3970s	
223/2700 (epoch 4.130), train_loss = 2.54990675, grad/param norm = 2.0397e-01, time/batch = 0.4162s	
224/2700 (epoch 4.148), train_loss = 2.53741308, grad/param norm = 2.8038e-01, time/batch = 0.4038s	
225/2700 (epoch 4.167), train_loss = 2.57016166, grad/param norm = 2.3210e-01, time/batch = 0.4299s	
226/2700 (epoch 4.185), train_loss = 2.46684279, grad/param norm = 1.3708e-01, time/batch = 0.4755s	
227/2700 (epoch 4.204), train_loss = 2.41306519, grad/param norm = 1.2415e-01, time/batch = 0.4769s	
228/2700 (epoch 4.222), train_loss = 2.34663069, grad/param norm = 1.4176e-01, time/batch = 0.4326s	
229/2700 (epoch 4.241), train_loss = 2.34931749, grad/param norm = 1.5647e-01, time/batch = 0.3920s	
230/2700 (epoch 4.259), train_loss = 2.39362838, grad/param norm = 1.9522e-01, time/batch = 0.3808s	
231/2700 (epoch 4.278), train_loss = 2.52838539, grad/param norm = 2.6048e-01, time/batch = 0.3956s	
232/2700 (epoch 4.296), train_loss = 2.71445070, grad/param norm = 8.9673e-01, time/batch = 0.4087s	
233/2700 (epoch 4.315), train_loss = 2.65712017, grad/param norm = 2.5334e-01, time/batch = 0.4262s	
234/2700 (epoch 4.333), train_loss = 2.58432412, grad/param norm = 1.7969e-01, time/batch = 0.4297s	
235/2700 (epoch 4.352), train_loss = 2.59015229, grad/param norm = 1.3772e-01, time/batch = 0.4096s	
236/2700 (epoch 4.370), train_loss = 2.50008186, grad/param norm = 1.2613e-01, time/batch = 0.4340s	
237/2700 (epoch 4.389), train_loss = 2.45179752, grad/param norm = 1.3713e-01, time/batch = 0.4418s	
238/2700 (epoch 4.407), train_loss = 2.46251452, grad/param norm = 1.9512e-01, time/batch = 0.4475s	
239/2700 (epoch 4.426), train_loss = 2.55374669, grad/param norm = 2.4676e-01, time/batch = 0.4099s	
240/2700 (epoch 4.444), train_loss = 2.46525289, grad/param norm = 2.7796e-01, time/batch = 0.3571s	
241/2700 (epoch 4.463), train_loss = 2.52589018, grad/param norm = 2.5820e-01, time/batch = 0.3641s	
242/2700 (epoch 4.481), train_loss = 2.53869975, grad/param norm = 2.0109e-01, time/batch = 0.4189s	
243/2700 (epoch 4.500), train_loss = 2.53782357, grad/param norm = 1.6398e-01, time/batch = 0.4510s	
244/2700 (epoch 4.519), train_loss = 2.46366711, grad/param norm = 1.5774e-01, time/batch = 0.4406s	
245/2700 (epoch 4.537), train_loss = 2.46338959, grad/param norm = 1.8892e-01, time/batch = 0.4399s	
246/2700 (epoch 4.556), train_loss = 2.57296623, grad/param norm = 6.2607e-01, time/batch = 0.4075s	
247/2700 (epoch 4.574), train_loss = 2.54670721, grad/param norm = 2.5162e-01, time/batch = 0.4386s	
248/2700 (epoch 4.593), train_loss = 2.44613057, grad/param norm = 2.0527e-01, time/batch = 0.4297s	
249/2700 (epoch 4.611), train_loss = 2.32652324, grad/param norm = 1.5221e-01, time/batch = 0.4376s	
250/2700 (epoch 4.630), train_loss = 2.36273936, grad/param norm = 1.3057e-01, time/batch = 0.3896s	
251/2700 (epoch 4.648), train_loss = 2.42273062, grad/param norm = 1.3572e-01, time/batch = 0.3876s	
252/2700 (epoch 4.667), train_loss = 2.33386547, grad/param norm = 1.3311e-01, time/batch = 0.3651s	
253/2700 (epoch 4.685), train_loss = 2.36607169, grad/param norm = 1.5218e-01, time/batch = 0.4252s	
254/2700 (epoch 4.704), train_loss = 2.36426733, grad/param norm = 1.7399e-01, time/batch = 0.4503s	
255/2700 (epoch 4.722), train_loss = 2.35332762, grad/param norm = 1.8063e-01, time/batch = 0.4345s	
256/2700 (epoch 4.741), train_loss = 2.52398945, grad/param norm = 2.1751e-01, time/batch = 0.4403s	
257/2700 (epoch 4.759), train_loss = 2.45789580, grad/param norm = 2.2179e-01, time/batch = 0.4275s	
258/2700 (epoch 4.778), train_loss = 2.45617054, grad/param norm = 2.3823e-01, time/batch = 0.4280s	
259/2700 (epoch 4.796), train_loss = 2.43544589, grad/param norm = 2.2830e-01, time/batch = 0.4371s	
260/2700 (epoch 4.815), train_loss = 2.37824845, grad/param norm = 1.6031e-01, time/batch = 0.4357s	
261/2700 (epoch 4.833), train_loss = 2.33839338, grad/param norm = 1.3914e-01, time/batch = 0.4572s	
262/2700 (epoch 4.852), train_loss = 2.36918195, grad/param norm = 1.3274e-01, time/batch = 0.4163s	
263/2700 (epoch 4.870), train_loss = 2.34533590, grad/param norm = 1.7453e-01, time/batch = 0.3744s	
264/2700 (epoch 4.889), train_loss = 2.37269507, grad/param norm = 1.7732e-01, time/batch = 0.3804s	
265/2700 (epoch 4.907), train_loss = 2.46712715, grad/param norm = 1.5835e-01, time/batch = 0.4228s	
266/2700 (epoch 4.926), train_loss = 2.39583639, grad/param norm = 1.7481e-01, time/batch = 0.4406s	
267/2700 (epoch 4.944), train_loss = 2.39873849, grad/param norm = 1.6191e-01, time/batch = 0.4418s	
268/2700 (epoch 4.963), train_loss = 2.43505479, grad/param norm = 1.3714e-01, time/batch = 0.4078s	
269/2700 (epoch 4.981), train_loss = 2.42974678, grad/param norm = 1.7142e-01, time/batch = 0.4270s	
270/2700 (epoch 5.000), train_loss = 2.63938355, grad/param norm = 1.2212e+00, time/batch = 0.4341s	
271/2700 (epoch 5.019), train_loss = 2.54751774, grad/param norm = 2.3070e-01, time/batch = 0.4562s	
272/2700 (epoch 5.037), train_loss = 2.49924041, grad/param norm = 2.1508e-01, time/batch = 0.4516s	
273/2700 (epoch 5.056), train_loss = 2.48703238, grad/param norm = 2.7369e-01, time/batch = 0.4286s	
274/2700 (epoch 5.074), train_loss = 2.51245654, grad/param norm = 3.2975e-01, time/batch = 0.3452s	
275/2700 (epoch 5.093), train_loss = 2.48419213, grad/param norm = 2.2205e-01, time/batch = 0.4102s	
276/2700 (epoch 5.111), train_loss = 2.37176913, grad/param norm = 1.4826e-01, time/batch = 0.4232s	
277/2700 (epoch 5.130), train_loss = 2.38097574, grad/param norm = 1.3014e-01, time/batch = 0.4087s	
278/2700 (epoch 5.148), train_loss = 2.30436643, grad/param norm = 1.1645e-01, time/batch = 0.4160s	
279/2700 (epoch 5.167), train_loss = 2.33953530, grad/param norm = 1.2496e-01, time/batch = 0.4151s	
280/2700 (epoch 5.185), train_loss = 2.29826416, grad/param norm = 1.1908e-01, time/batch = 0.4483s	
281/2700 (epoch 5.204), train_loss = 2.29585554, grad/param norm = 1.2805e-01, time/batch = 0.4477s	
282/2700 (epoch 5.222), train_loss = 2.21829652, grad/param norm = 1.5340e-01, time/batch = 0.4577s	
283/2700 (epoch 5.241), train_loss = 2.20491308, grad/param norm = 1.5507e-01, time/batch = 0.4543s	
284/2700 (epoch 5.259), train_loss = 2.25916371, grad/param norm = 1.7514e-01, time/batch = 0.4203s	
285/2700 (epoch 5.278), train_loss = 2.38509710, grad/param norm = 1.9533e-01, time/batch = 0.3715s	
286/2700 (epoch 5.296), train_loss = 2.34152117, grad/param norm = 1.7482e-01, time/batch = 0.3672s	
287/2700 (epoch 5.315), train_loss = 2.35314502, grad/param norm = 1.7405e-01, time/batch = 0.4310s	
288/2700 (epoch 5.333), train_loss = 2.38040505, grad/param norm = 2.2613e-01, time/batch = 0.4110s	
289/2700 (epoch 5.352), train_loss = 2.44131525, grad/param norm = 2.3213e-01, time/batch = 0.4130s	
290/2700 (epoch 5.370), train_loss = 2.35133021, grad/param norm = 1.8365e-01, time/batch = 0.4208s	
291/2700 (epoch 5.389), train_loss = 2.31710046, grad/param norm = 1.6868e-01, time/batch = 0.4267s	
292/2700 (epoch 5.407), train_loss = 2.30999165, grad/param norm = 1.8690e-01, time/batch = 0.4530s	
293/2700 (epoch 5.426), train_loss = 2.40637204, grad/param norm = 2.7632e-01, time/batch = 0.4475s	
294/2700 (epoch 5.444), train_loss = 2.28704381, grad/param norm = 1.9370e-01, time/batch = 0.4022s	
295/2700 (epoch 5.463), train_loss = 2.30300348, grad/param norm = 1.2293e-01, time/batch = 0.3860s	
296/2700 (epoch 5.481), train_loss = 2.32705882, grad/param norm = 1.1774e-01, time/batch = 0.3781s	
297/2700 (epoch 5.500), train_loss = 2.33701585, grad/param norm = 1.5695e-01, time/batch = 0.4312s	
298/2700 (epoch 5.519), train_loss = 2.32555385, grad/param norm = 1.6729e-01, time/batch = 0.4471s	
299/2700 (epoch 5.537), train_loss = 2.30631480, grad/param norm = 1.4535e-01, time/batch = 0.3928s	
300/2700 (epoch 5.556), train_loss = 2.28569711, grad/param norm = 1.3199e-01, time/batch = 0.4234s	
301/2700 (epoch 5.574), train_loss = 2.26672126, grad/param norm = 1.4846e-01, time/batch = 0.4294s	
302/2700 (epoch 5.593), train_loss = 2.25537623, grad/param norm = 1.6933e-01, time/batch = 0.4498s	
303/2700 (epoch 5.611), train_loss = 2.20464406, grad/param norm = 1.9285e-01, time/batch = 0.4380s	
304/2700 (epoch 5.630), train_loss = 2.27494478, grad/param norm = 2.3147e-01, time/batch = 0.4435s	
305/2700 (epoch 5.648), train_loss = 2.28514185, grad/param norm = 1.6155e-01, time/batch = 0.4105s	
306/2700 (epoch 5.667), train_loss = 2.17352383, grad/param norm = 1.0346e-01, time/batch = 0.3759s	
307/2700 (epoch 5.685), train_loss = 2.19218146, grad/param norm = 1.0647e-01, time/batch = 0.3849s	
308/2700 (epoch 5.704), train_loss = 2.18771038, grad/param norm = 1.2951e-01, time/batch = 0.4410s	
309/2700 (epoch 5.722), train_loss = 2.17225315, grad/param norm = 1.2891e-01, time/batch = 0.4557s	
310/2700 (epoch 5.741), train_loss = 2.30462510, grad/param norm = 1.3577e-01, time/batch = 0.4256s	
311/2700 (epoch 5.759), train_loss = 2.27299351, grad/param norm = 1.5234e-01, time/batch = 0.3710s	
312/2700 (epoch 5.778), train_loss = 2.29451157, grad/param norm = 1.7956e-01, time/batch = 0.4354s	
313/2700 (epoch 5.796), train_loss = 2.27227264, grad/param norm = 1.8490e-01, time/batch = 0.4619s	
314/2700 (epoch 5.815), train_loss = 2.29286143, grad/param norm = 2.4782e-01, time/batch = 0.4759s	
315/2700 (epoch 5.833), train_loss = 2.35423119, grad/param norm = 3.3563e-01, time/batch = 0.4540s	
316/2700 (epoch 5.852), train_loss = 2.35691224, grad/param norm = 2.6190e-01, time/batch = 0.4036s	
317/2700 (epoch 5.870), train_loss = 2.23132604, grad/param norm = 1.4632e-01, time/batch = 0.3897s	
318/2700 (epoch 5.889), train_loss = 2.19640633, grad/param norm = 1.3159e-01, time/batch = 0.4029s	
319/2700 (epoch 5.907), train_loss = 2.29859370, grad/param norm = 1.2012e-01, time/batch = 0.4261s	
320/2700 (epoch 5.926), train_loss = 2.23901640, grad/param norm = 1.2231e-01, time/batch = 0.4292s	
321/2700 (epoch 5.944), train_loss = 2.23850772, grad/param norm = 1.3485e-01, time/batch = 0.3800s	
322/2700 (epoch 5.963), train_loss = 2.27787419, grad/param norm = 1.5249e-01, time/batch = 0.3910s	
323/2700 (epoch 5.981), train_loss = 2.30648612, grad/param norm = 2.0185e-01, time/batch = 0.4198s	
324/2700 (epoch 6.000), train_loss = 2.62746413, grad/param norm = 1.5875e+00, time/batch = 0.4715s	
325/2700 (epoch 6.019), train_loss = 2.55297647, grad/param norm = 4.5268e-01, time/batch = 0.4699s	
326/2700 (epoch 6.037), train_loss = 2.56277977, grad/param norm = 3.2642e-01, time/batch = 0.4488s	
327/2700 (epoch 6.056), train_loss = 2.33548369, grad/param norm = 1.6488e-01, time/batch = 0.4252s	
328/2700 (epoch 6.074), train_loss = 2.24629887, grad/param norm = 1.1490e-01, time/batch = 0.3977s	
329/2700 (epoch 6.093), train_loss = 2.25304786, grad/param norm = 1.1866e-01, time/batch = 0.4055s	
330/2700 (epoch 6.111), train_loss = 2.19426766, grad/param norm = 1.2871e-01, time/batch = 0.4183s	
331/2700 (epoch 6.130), train_loss = 2.22492702, grad/param norm = 1.4789e-01, time/batch = 0.4320s	
332/2700 (epoch 6.148), train_loss = 2.18557063, grad/param norm = 1.7223e-01, time/batch = 0.3900s	
333/2700 (epoch 6.167), train_loss = 2.24488159, grad/param norm = 1.7971e-01, time/batch = 0.4039s	
334/2700 (epoch 6.185), train_loss = 2.18143866, grad/param norm = 1.3775e-01, time/batch = 0.4133s	
335/2700 (epoch 6.204), train_loss = 2.17970794, grad/param norm = 1.2734e-01, time/batch = 0.4409s	
336/2700 (epoch 6.222), train_loss = 2.09688005, grad/param norm = 1.4064e-01, time/batch = 0.4596s	
337/2700 (epoch 6.241), train_loss = 2.06436735, grad/param norm = 1.5423e-01, time/batch = 0.4365s	
338/2700 (epoch 6.259), train_loss = 2.12324008, grad/param norm = 1.5862e-01, time/batch = 0.3962s	
339/2700 (epoch 6.278), train_loss = 2.20037957, grad/param norm = 1.3400e-01, time/batch = 0.3723s	
340/2700 (epoch 6.296), train_loss = 2.16850091, grad/param norm = 1.3430e-01, time/batch = 0.4111s	
341/2700 (epoch 6.315), train_loss = 2.17808377, grad/param norm = 1.4333e-01, time/batch = 0.4144s	
342/2700 (epoch 6.333), train_loss = 2.17851062, grad/param norm = 1.2678e-01, time/batch = 0.4368s	
343/2700 (epoch 6.352), train_loss = 2.19608970, grad/param norm = 1.2196e-01, time/batch = 0.4215s	
344/2700 (epoch 6.370), train_loss = 2.19497662, grad/param norm = 1.2061e-01, time/batch = 0.4198s	
345/2700 (epoch 6.389), train_loss = 2.16022981, grad/param norm = 1.3257e-01, time/batch = 0.4255s	
346/2700 (epoch 6.407), train_loss = 2.16497929, grad/param norm = 1.4451e-01, time/batch = 0.4487s	
347/2700 (epoch 6.426), train_loss = 2.20941747, grad/param norm = 1.6426e-01, time/batch = 0.4441s	
348/2700 (epoch 6.444), train_loss = 2.11364090, grad/param norm = 1.6120e-01, time/batch = 0.4759s	
349/2700 (epoch 6.463), train_loss = 2.20459078, grad/param norm = 2.8230e-01, time/batch = 0.4238s	
350/2700 (epoch 6.481), train_loss = 2.28742916, grad/param norm = 1.7168e-01, time/batch = 0.3856s	
351/2700 (epoch 6.500), train_loss = 2.20512029, grad/param norm = 1.2472e-01, time/batch = 0.3711s	
352/2700 (epoch 6.519), train_loss = 2.14755523, grad/param norm = 1.1341e-01, time/batch = 0.3801s	
353/2700 (epoch 6.537), train_loss = 2.15583009, grad/param norm = 1.2301e-01, time/batch = 0.4242s	
354/2700 (epoch 6.556), train_loss = 2.12968677, grad/param norm = 1.1901e-01, time/batch = 0.4385s	
355/2700 (epoch 6.574), train_loss = 2.09298052, grad/param norm = 9.6643e-02, time/batch = 0.4422s	
356/2700 (epoch 6.593), train_loss = 2.08148068, grad/param norm = 8.9001e-02, time/batch = 0.4135s	
357/2700 (epoch 6.611), train_loss = 1.99360089, grad/param norm = 1.1051e-01, time/batch = 0.4304s	
358/2700 (epoch 6.630), train_loss = 2.08086912, grad/param norm = 1.7793e-01, time/batch = 0.4453s	
359/2700 (epoch 6.648), train_loss = 2.18193396, grad/param norm = 2.1993e-01, time/batch = 0.4352s	
360/2700 (epoch 6.667), train_loss = 2.12470382, grad/param norm = 2.0455e-01, time/batch = 0.4342s	
361/2700 (epoch 6.685), train_loss = 2.11903864, grad/param norm = 1.5554e-01, time/batch = 0.4450s	
362/2700 (epoch 6.704), train_loss = 2.07240249, grad/param norm = 1.2779e-01, time/batch = 0.3690s	
363/2700 (epoch 6.722), train_loss = 2.04687431, grad/param norm = 1.1058e-01, time/batch = 0.3757s	
364/2700 (epoch 6.741), train_loss = 2.12834407, grad/param norm = 1.0955e-01, time/batch = 0.4103s	
365/2700 (epoch 6.759), train_loss = 2.13849508, grad/param norm = 1.3189e-01, time/batch = 0.4268s	
366/2700 (epoch 6.778), train_loss = 2.17051591, grad/param norm = 1.5927e-01, time/batch = 0.4567s	
367/2700 (epoch 6.796), train_loss = 2.11433622, grad/param norm = 1.4817e-01, time/batch = 0.4320s	
368/2700 (epoch 6.815), train_loss = 2.13392965, grad/param norm = 1.7778e-01, time/batch = 0.4188s	
369/2700 (epoch 6.833), train_loss = 2.12663261, grad/param norm = 1.9119e-01, time/batch = 0.4316s	
370/2700 (epoch 6.852), train_loss = 2.12730392, grad/param norm = 1.7347e-01, time/batch = 0.4419s	
371/2700 (epoch 6.870), train_loss = 2.08092261, grad/param norm = 1.5911e-01, time/batch = 0.4496s	
372/2700 (epoch 6.889), train_loss = 2.08423760, grad/param norm = 1.4100e-01, time/batch = 0.4441s	
373/2700 (epoch 6.907), train_loss = 2.18426215, grad/param norm = 1.5067e-01, time/batch = 0.4256s	
374/2700 (epoch 6.926), train_loss = 2.15333481, grad/param norm = 1.8916e-01, time/batch = 0.3812s	
375/2700 (epoch 6.944), train_loss = 2.16100288, grad/param norm = 1.7617e-01, time/batch = 0.3808s	
376/2700 (epoch 6.963), train_loss = 2.14763252, grad/param norm = 1.3598e-01, time/batch = 0.4070s	
377/2700 (epoch 6.981), train_loss = 2.10278578, grad/param norm = 1.1365e-01, time/batch = 0.4592s	
378/2700 (epoch 7.000), train_loss = 2.12131838, grad/param norm = 1.1538e-01, time/batch = 0.4374s	
379/2700 (epoch 7.019), train_loss = 2.14662872, grad/param norm = 1.4918e-01, time/batch = 0.4205s	
380/2700 (epoch 7.037), train_loss = 2.15862205, grad/param norm = 1.5425e-01, time/batch = 0.4220s	
381/2700 (epoch 7.056), train_loss = 2.10970571, grad/param norm = 1.4445e-01, time/batch = 0.4242s	
382/2700 (epoch 7.074), train_loss = 2.08601343, grad/param norm = 1.6395e-01, time/batch = 0.4464s	
383/2700 (epoch 7.093), train_loss = 2.10785908, grad/param norm = 1.6052e-01, time/batch = 0.4468s	
384/2700 (epoch 7.111), train_loss = 2.05322543, grad/param norm = 1.3373e-01, time/batch = 0.4233s	
385/2700 (epoch 7.130), train_loss = 2.06723223, grad/param norm = 1.0957e-01, time/batch = 0.3901s	
386/2700 (epoch 7.148), train_loss = 1.99064059, grad/param norm = 1.0110e-01, time/batch = 0.3745s	
387/2700 (epoch 7.167), train_loss = 2.06721342, grad/param norm = 1.0500e-01, time/batch = 0.4123s	
388/2700 (epoch 7.185), train_loss = 1.98799832, grad/param norm = 1.0444e-01, time/batch = 0.4596s	
389/2700 (epoch 7.204), train_loss = 2.03499710, grad/param norm = 1.2006e-01, time/batch = 0.4479s	
390/2700 (epoch 7.222), train_loss = 1.95842766, grad/param norm = 1.4248e-01, time/batch = 0.4308s	
391/2700 (epoch 7.241), train_loss = 1.92248845, grad/param norm = 1.5461e-01, time/batch = 0.4032s	
392/2700 (epoch 7.259), train_loss = 1.96776530, grad/param norm = 1.3157e-01, time/batch = 0.4031s	
393/2700 (epoch 7.278), train_loss = 2.04634638, grad/param norm = 1.2971e-01, time/batch = 0.4435s	
394/2700 (epoch 7.296), train_loss = 2.04940071, grad/param norm = 1.3643e-01, time/batch = 0.4474s	
395/2700 (epoch 7.315), train_loss = 2.05582726, grad/param norm = 1.5414e-01, time/batch = 0.4533s	
396/2700 (epoch 7.333), train_loss = 2.07570628, grad/param norm = 1.5434e-01, time/batch = 0.3556s	
397/2700 (epoch 7.352), train_loss = 2.07295133, grad/param norm = 1.3315e-01, time/batch = 0.3797s	
398/2700 (epoch 7.370), train_loss = 2.06879689, grad/param norm = 1.1816e-01, time/batch = 0.4302s	
399/2700 (epoch 7.389), train_loss = 2.02010969, grad/param norm = 1.1361e-01, time/batch = 0.4663s	
400/2700 (epoch 7.407), train_loss = 2.03084767, grad/param norm = 1.1516e-01, time/batch = 0.4620s	
401/2700 (epoch 7.426), train_loss = 2.06854730, grad/param norm = 1.2226e-01, time/batch = 0.4387s	
402/2700 (epoch 7.444), train_loss = 1.96944179, grad/param norm = 1.2799e-01, time/batch = 0.3941s	
403/2700 (epoch 7.463), train_loss = 2.07623327, grad/param norm = 1.5336e-01, time/batch = 0.4128s	
404/2700 (epoch 7.481), train_loss = 2.09172815, grad/param norm = 1.5676e-01, time/batch = 0.4458s	
405/2700 (epoch 7.500), train_loss = 2.06675483, grad/param norm = 1.8707e-01, time/batch = 0.4513s	
406/2700 (epoch 7.519), train_loss = 2.08624961, grad/param norm = 2.0134e-01, time/batch = 0.4481s	
407/2700 (epoch 7.537), train_loss = 2.09295378, grad/param norm = 1.6528e-01, time/batch = 0.4012s	
408/2700 (epoch 7.556), train_loss = 2.00702249, grad/param norm = 1.2064e-01, time/batch = 0.3395s	
409/2700 (epoch 7.574), train_loss = 1.99861400, grad/param norm = 1.2773e-01, time/batch = 0.4092s	
410/2700 (epoch 7.593), train_loss = 2.01474711, grad/param norm = 1.5589e-01, time/batch = 0.4607s	
411/2700 (epoch 7.611), train_loss = 1.93306511, grad/param norm = 1.6209e-01, time/batch = 0.4511s	
412/2700 (epoch 7.630), train_loss = 1.96891735, grad/param norm = 1.4723e-01, time/batch = 0.4075s	
413/2700 (epoch 7.648), train_loss = 2.00080997, grad/param norm = 1.4055e-01, time/batch = 0.4025s	
414/2700 (epoch 7.667), train_loss = 1.96100821, grad/param norm = 1.3309e-01, time/batch = 0.4341s	
415/2700 (epoch 7.685), train_loss = 1.97721211, grad/param norm = 1.3716e-01, time/batch = 0.4617s	
416/2700 (epoch 7.704), train_loss = 1.96112809, grad/param norm = 1.2374e-01, time/batch = 0.4700s	
417/2700 (epoch 7.722), train_loss = 1.92634352, grad/param norm = 1.1681e-01, time/batch = 0.4505s	
418/2700 (epoch 7.741), train_loss = 1.99224068, grad/param norm = 1.1858e-01, time/batch = 0.4170s	
419/2700 (epoch 7.759), train_loss = 2.00347881, grad/param norm = 1.1592e-01, time/batch = 0.3673s	
420/2700 (epoch 7.778), train_loss = 2.00310066, grad/param norm = 1.0210e-01, time/batch = 0.3549s	
421/2700 (epoch 7.796), train_loss = 1.94978730, grad/param norm = 1.0127e-01, time/batch = 0.4284s	
422/2700 (epoch 7.815), train_loss = 1.97957653, grad/param norm = 1.1063e-01, time/batch = 0.4403s	
423/2700 (epoch 7.833), train_loss = 1.95657614, grad/param norm = 1.2512e-01, time/batch = 0.4098s	
424/2700 (epoch 7.852), train_loss = 1.97895181, grad/param norm = 1.4333e-01, time/batch = 0.4142s	
425/2700 (epoch 7.870), train_loss = 1.97021230, grad/param norm = 1.2779e-01, time/batch = 0.4387s	
426/2700 (epoch 7.889), train_loss = 1.95945073, grad/param norm = 1.0596e-01, time/batch = 0.4604s	
427/2700 (epoch 7.907), train_loss = 2.04783489, grad/param norm = 1.1160e-01, time/batch = 0.4673s	
428/2700 (epoch 7.926), train_loss = 2.00849026, grad/param norm = 1.2925e-01, time/batch = 0.4676s	
429/2700 (epoch 7.944), train_loss = 1.99428594, grad/param norm = 1.2336e-01, time/batch = 0.4086s	
430/2700 (epoch 7.963), train_loss = 1.99659822, grad/param norm = 1.1021e-01, time/batch = 0.3678s	
431/2700 (epoch 7.981), train_loss = 1.95763365, grad/param norm = 9.6077e-02, time/batch = 0.3351s	
432/2700 (epoch 8.000), train_loss = 1.97943544, grad/param norm = 9.4501e-02, time/batch = 0.4204s	
433/2700 (epoch 8.019), train_loss = 2.00701859, grad/param norm = 1.0458e-01, time/batch = 0.4429s	
434/2700 (epoch 8.037), train_loss = 1.99660358, grad/param norm = 1.0493e-01, time/batch = 0.4244s	
435/2700 (epoch 8.056), train_loss = 1.96338382, grad/param norm = 1.2290e-01, time/batch = 0.4091s	
436/2700 (epoch 8.074), train_loss = 1.90749222, grad/param norm = 1.1297e-01, time/batch = 0.4393s	
437/2700 (epoch 8.093), train_loss = 1.91704696, grad/param norm = 9.9091e-02, time/batch = 0.4550s	
438/2700 (epoch 8.111), train_loss = 1.88356235, grad/param norm = 9.8458e-02, time/batch = 0.4513s	
439/2700 (epoch 8.130), train_loss = 1.92889975, grad/param norm = 1.1003e-01, time/batch = 0.4112s	
440/2700 (epoch 8.148), train_loss = 1.89460407, grad/param norm = 1.5048e-01, time/batch = 0.3860s	
441/2700 (epoch 8.167), train_loss = 2.01622529, grad/param norm = 1.5419e-01, time/batch = 0.3564s	
442/2700 (epoch 8.185), train_loss = 1.90540794, grad/param norm = 1.3593e-01, time/batch = 0.3818s	
443/2700 (epoch 8.204), train_loss = 1.96462437, grad/param norm = 1.3940e-01, time/batch = 0.4573s	
444/2700 (epoch 8.222), train_loss = 1.88563786, grad/param norm = 1.3995e-01, time/batch = 0.4655s	
445/2700 (epoch 8.241), train_loss = 1.81201926, grad/param norm = 1.2384e-01, time/batch = 0.3843s	
446/2700 (epoch 8.259), train_loss = 1.86889421, grad/param norm = 1.4569e-01, time/batch = 0.4333s	
447/2700 (epoch 8.278), train_loss = 1.96088375, grad/param norm = 1.5421e-01, time/batch = 0.4578s	
448/2700 (epoch 8.296), train_loss = 1.96167351, grad/param norm = 1.4796e-01, time/batch = 0.4662s	
449/2700 (epoch 8.315), train_loss = 1.92795197, grad/param norm = 1.0954e-01, time/batch = 0.4674s	
450/2700 (epoch 8.333), train_loss = 1.90343419, grad/param norm = 9.2635e-02, time/batch = 0.4142s	
451/2700 (epoch 8.352), train_loss = 1.91130101, grad/param norm = 1.0533e-01, time/batch = 0.4245s	
452/2700 (epoch 8.370), train_loss = 1.95579998, grad/param norm = 1.3165e-01, time/batch = 0.3687s	
453/2700 (epoch 8.389), train_loss = 1.93038901, grad/param norm = 1.3300e-01, time/batch = 0.3646s	
454/2700 (epoch 8.407), train_loss = 1.92848383, grad/param norm = 1.2083e-01, time/batch = 0.4472s	
455/2700 (epoch 8.426), train_loss = 1.97413245, grad/param norm = 1.3139e-01, time/batch = 0.4573s	
456/2700 (epoch 8.444), train_loss = 1.89630702, grad/param norm = 1.4747e-01, time/batch = 0.4265s	
457/2700 (epoch 8.463), train_loss = 1.98307978, grad/param norm = 1.4979e-01, time/batch = 0.3835s	
458/2700 (epoch 8.481), train_loss = 1.95983659, grad/param norm = 1.1577e-01, time/batch = 0.4618s	
459/2700 (epoch 8.500), train_loss = 1.90682829, grad/param norm = 9.8929e-02, time/batch = 0.4556s	
460/2700 (epoch 8.519), train_loss = 1.88756742, grad/param norm = 9.5062e-02, time/batch = 0.4337s	
461/2700 (epoch 8.537), train_loss = 1.90258095, grad/param norm = 1.0475e-01, time/batch = 0.4196s	
462/2700 (epoch 8.556), train_loss = 1.86901214, grad/param norm = 1.1670e-01, time/batch = 0.3883s	
463/2700 (epoch 8.574), train_loss = 1.87577184, grad/param norm = 1.2771e-01, time/batch = 0.3992s	
464/2700 (epoch 8.593), train_loss = 1.88208075, grad/param norm = 1.1910e-01, time/batch = 0.3921s	
465/2700 (epoch 8.611), train_loss = 1.77462415, grad/param norm = 1.0918e-01, time/batch = 0.4544s	
466/2700 (epoch 8.630), train_loss = 1.82712325, grad/param norm = 1.1447e-01, time/batch = 0.4324s	
467/2700 (epoch 8.648), train_loss = 1.85096525, grad/param norm = 9.9234e-02, time/batch = 0.4054s	
468/2700 (epoch 8.667), train_loss = 1.83110546, grad/param norm = 1.1365e-01, time/batch = 0.4283s	
469/2700 (epoch 8.685), train_loss = 1.87305954, grad/param norm = 1.2936e-01, time/batch = 0.4480s	
470/2700 (epoch 8.704), train_loss = 1.85428572, grad/param norm = 1.1802e-01, time/batch = 0.4756s	
471/2700 (epoch 8.722), train_loss = 1.82312464, grad/param norm = 9.8349e-02, time/batch = 0.4572s	
472/2700 (epoch 8.741), train_loss = 1.83684963, grad/param norm = 8.5119e-02, time/batch = 0.4213s	
473/2700 (epoch 8.759), train_loss = 1.85980206, grad/param norm = 9.1715e-02, time/batch = 0.4115s	
474/2700 (epoch 8.778), train_loss = 1.88288007, grad/param norm = 1.1107e-01, time/batch = 0.4006s	
475/2700 (epoch 8.796), train_loss = 1.86505376, grad/param norm = 1.2507e-01, time/batch = 0.3814s	
476/2700 (epoch 8.815), train_loss = 1.89948906, grad/param norm = 1.4370e-01, time/batch = 0.4519s	
477/2700 (epoch 8.833), train_loss = 1.86880025, grad/param norm = 1.5393e-01, time/batch = 0.4362s	
478/2700 (epoch 8.852), train_loss = 1.85630228, grad/param norm = 1.2411e-01, time/batch = 0.4053s	
479/2700 (epoch 8.870), train_loss = 1.84546492, grad/param norm = 1.0825e-01, time/batch = 0.4198s	
480/2700 (epoch 8.889), train_loss = 1.85820013, grad/param norm = 1.1952e-01, time/batch = 0.4501s	
481/2700 (epoch 8.907), train_loss = 1.96333960, grad/param norm = 1.2559e-01, time/batch = 0.4457s	
482/2700 (epoch 8.926), train_loss = 1.89025205, grad/param norm = 1.2816e-01, time/batch = 0.4624s	
483/2700 (epoch 8.944), train_loss = 1.89704773, grad/param norm = 1.2474e-01, time/batch = 0.4381s	
484/2700 (epoch 8.963), train_loss = 1.88839013, grad/param norm = 1.2674e-01, time/batch = 0.4128s	
485/2700 (epoch 8.981), train_loss = 1.86158766, grad/param norm = 1.0899e-01, time/batch = 0.3965s	
486/2700 (epoch 9.000), train_loss = 1.88320146, grad/param norm = 9.3763e-02, time/batch = 0.3838s	
487/2700 (epoch 9.019), train_loss = 1.90230342, grad/param norm = 1.0523e-01, time/batch = 0.4537s	
488/2700 (epoch 9.037), train_loss = 1.90231876, grad/param norm = 1.0725e-01, time/batch = 0.4352s	
489/2700 (epoch 9.056), train_loss = 1.83797456, grad/param norm = 1.0820e-01, time/batch = 0.4129s	
490/2700 (epoch 9.074), train_loss = 1.79287886, grad/param norm = 1.0125e-01, time/batch = 0.4217s	
491/2700 (epoch 9.093), train_loss = 1.78960788, grad/param norm = 1.0076e-01, time/batch = 0.4255s	
492/2700 (epoch 9.111), train_loss = 1.77200840, grad/param norm = 9.8220e-02, time/batch = 0.4544s	
493/2700 (epoch 9.130), train_loss = 1.81235613, grad/param norm = 9.4320e-02, time/batch = 0.4555s	
494/2700 (epoch 9.148), train_loss = 1.75588102, grad/param norm = 1.0206e-01, time/batch = 0.4350s	
495/2700 (epoch 9.167), train_loss = 1.86487287, grad/param norm = 1.0966e-01, time/batch = 0.3981s	
496/2700 (epoch 9.185), train_loss = 1.76763998, grad/param norm = 1.0579e-01, time/batch = 0.3873s	
497/2700 (epoch 9.204), train_loss = 1.82506735, grad/param norm = 1.1628e-01, time/batch = 0.3976s	
498/2700 (epoch 9.222), train_loss = 1.77019876, grad/param norm = 1.2832e-01, time/batch = 0.4575s	
499/2700 (epoch 9.241), train_loss = 1.69868959, grad/param norm = 1.3063e-01, time/batch = 0.4453s	
500/2700 (epoch 9.259), train_loss = 1.75258339, grad/param norm = 1.0659e-01, time/batch = 0.4185s	
501/2700 (epoch 9.278), train_loss = 1.82223289, grad/param norm = 9.9336e-02, time/batch = 0.4070s	
502/2700 (epoch 9.296), train_loss = 1.79059635, grad/param norm = 9.0479e-02, time/batch = 0.4107s	
503/2700 (epoch 9.315), train_loss = 1.78815947, grad/param norm = 8.1127e-02, time/batch = 0.4526s	
504/2700 (epoch 9.333), train_loss = 1.77759435, grad/param norm = 8.4456e-02, time/batch = 0.4690s	
505/2700 (epoch 9.352), train_loss = 1.80135083, grad/param norm = 1.0577e-01, time/batch = 0.4782s	
506/2700 (epoch 9.370), train_loss = 1.84916003, grad/param norm = 1.1037e-01, time/batch = 0.4225s	
507/2700 (epoch 9.389), train_loss = 1.80411050, grad/param norm = 1.1372e-01, time/batch = 0.4049s	
508/2700 (epoch 9.407), train_loss = 1.83251202, grad/param norm = 1.1467e-01, time/batch = 0.3736s	
509/2700 (epoch 9.426), train_loss = 1.84816565, grad/param norm = 1.0585e-01, time/batch = 0.4404s	
510/2700 (epoch 9.444), train_loss = 1.72548852, grad/param norm = 7.8169e-02, time/batch = 0.4441s	
511/2700 (epoch 9.463), train_loss = 1.81583187, grad/param norm = 8.4564e-02, time/batch = 0.4217s	
512/2700 (epoch 9.481), train_loss = 1.82164659, grad/param norm = 9.3125e-02, time/batch = 0.3825s	
513/2700 (epoch 9.500), train_loss = 1.79241797, grad/param norm = 1.0492e-01, time/batch = 0.4225s	
514/2700 (epoch 9.519), train_loss = 1.85140791, grad/param norm = 1.3614e-01, time/batch = 0.4558s	
515/2700 (epoch 9.537), train_loss = 1.86223245, grad/param norm = 1.5618e-01, time/batch = 0.4726s	
516/2700 (epoch 9.556), train_loss = 1.86952220, grad/param norm = 1.8146e-01, time/batch = 0.4849s	
517/2700 (epoch 9.574), train_loss = 1.84227093, grad/param norm = 1.4711e-01, time/batch = 0.4523s	
518/2700 (epoch 9.593), train_loss = 1.80402044, grad/param norm = 1.2092e-01, time/batch = 0.3760s	
519/2700 (epoch 9.611), train_loss = 1.67435192, grad/param norm = 1.0023e-01, time/batch = 0.3889s	
520/2700 (epoch 9.630), train_loss = 1.70325691, grad/param norm = 8.9659e-02, time/batch = 0.4390s	
521/2700 (epoch 9.648), train_loss = 1.72451134, grad/param norm = 7.4029e-02, time/batch = 0.4188s	
522/2700 (epoch 9.667), train_loss = 1.69556188, grad/param norm = 7.1650e-02, time/batch = 0.3893s	
523/2700 (epoch 9.685), train_loss = 1.72806173, grad/param norm = 9.3428e-02, time/batch = 0.4060s	
524/2700 (epoch 9.704), train_loss = 1.75627762, grad/param norm = 1.1843e-01, time/batch = 0.4423s	
525/2700 (epoch 9.722), train_loss = 1.73162683, grad/param norm = 1.1364e-01, time/batch = 0.4638s	
526/2700 (epoch 9.741), train_loss = 1.74418683, grad/param norm = 1.0419e-01, time/batch = 0.4821s	
527/2700 (epoch 9.759), train_loss = 1.74668212, grad/param norm = 9.7249e-02, time/batch = 0.4528s	
528/2700 (epoch 9.778), train_loss = 1.76615672, grad/param norm = 8.4578e-02, time/batch = 0.3960s	
529/2700 (epoch 9.796), train_loss = 1.71273454, grad/param norm = 9.0706e-02, time/batch = 0.3865s	
530/2700 (epoch 9.815), train_loss = 1.75597480, grad/param norm = 9.0741e-02, time/batch = 0.3799s	
531/2700 (epoch 9.833), train_loss = 1.72409521, grad/param norm = 1.0422e-01, time/batch = 0.4300s	
532/2700 (epoch 9.852), train_loss = 1.72794631, grad/param norm = 1.0207e-01, time/batch = 0.4194s	
533/2700 (epoch 9.870), train_loss = 1.74094912, grad/param norm = 9.9268e-02, time/batch = 0.3962s	
534/2700 (epoch 9.889), train_loss = 1.74932345, grad/param norm = 1.0032e-01, time/batch = 0.4236s	
535/2700 (epoch 9.907), train_loss = 1.83289582, grad/param norm = 1.0184e-01, time/batch = 0.4523s	
536/2700 (epoch 9.926), train_loss = 1.80027787, grad/param norm = 1.2027e-01, time/batch = 0.4741s	
537/2700 (epoch 9.944), train_loss = 1.78532169, grad/param norm = 1.0414e-01, time/batch = 0.4735s	
538/2700 (epoch 9.963), train_loss = 1.76685443, grad/param norm = 8.5067e-02, time/batch = 0.4558s	
539/2700 (epoch 9.981), train_loss = 1.72036162, grad/param norm = 8.2872e-02, time/batch = 0.4029s	
decayed learning rate by a factor 0.97 to 0.00194	
540/2700 (epoch 10.000), train_loss = 1.77136275, grad/param norm = 8.4728e-02, time/batch = 0.3830s	
541/2700 (epoch 10.019), train_loss = 1.79589175, grad/param norm = 9.4920e-02, time/batch = 0.3593s	
542/2700 (epoch 10.037), train_loss = 1.78163278, grad/param norm = 9.1397e-02, time/batch = 0.4180s	
543/2700 (epoch 10.056), train_loss = 1.72054625, grad/param norm = 9.4922e-02, time/batch = 0.4165s	
544/2700 (epoch 10.074), train_loss = 1.67137701, grad/param norm = 8.7429e-02, time/batch = 0.4019s	
545/2700 (epoch 10.093), train_loss = 1.65607436, grad/param norm = 7.7542e-02, time/batch = 0.4215s	
546/2700 (epoch 10.111), train_loss = 1.63384365, grad/param norm = 7.5577e-02, time/batch = 0.4432s	
547/2700 (epoch 10.130), train_loss = 1.69775827, grad/param norm = 7.3612e-02, time/batch = 0.4670s	
548/2700 (epoch 10.148), train_loss = 1.63568155, grad/param norm = 8.4899e-02, time/batch = 0.4830s	
549/2700 (epoch 10.167), train_loss = 1.75221627, grad/param norm = 8.9047e-02, time/batch = 0.4670s	
550/2700 (epoch 10.185), train_loss = 1.64502426, grad/param norm = 8.7907e-02, time/batch = 0.4201s	
551/2700 (epoch 10.204), train_loss = 1.71263840, grad/param norm = 9.9788e-02, time/batch = 0.4073s	
552/2700 (epoch 10.222), train_loss = 1.64594481, grad/param norm = 9.6363e-02, time/batch = 0.3682s	
553/2700 (epoch 10.241), train_loss = 1.57177451, grad/param norm = 9.1739e-02, time/batch = 0.4052s	
554/2700 (epoch 10.259), train_loss = 1.62360924, grad/param norm = 8.5475e-02, time/batch = 0.4288s	
555/2700 (epoch 10.278), train_loss = 1.71464427, grad/param norm = 9.6054e-02, time/batch = 0.3997s	
556/2700 (epoch 10.296), train_loss = 1.70083143, grad/param norm = 1.0042e-01, time/batch = 0.4161s	
557/2700 (epoch 10.315), train_loss = 1.71328671, grad/param norm = 1.1530e-01, time/batch = 0.4426s	
558/2700 (epoch 10.333), train_loss = 1.71733417, grad/param norm = 1.2237e-01, time/batch = 0.4622s	
559/2700 (epoch 10.352), train_loss = 1.71890684, grad/param norm = 1.1006e-01, time/batch = 0.4696s	
560/2700 (epoch 10.370), train_loss = 1.72231736, grad/param norm = 9.8084e-02, time/batch = 0.4603s	
561/2700 (epoch 10.389), train_loss = 1.67046515, grad/param norm = 9.3326e-02, time/batch = 0.4385s	
562/2700 (epoch 10.407), train_loss = 1.69793495, grad/param norm = 8.5121e-02, time/batch = 0.3828s	
563/2700 (epoch 10.426), train_loss = 1.72671590, grad/param norm = 8.5487e-02, time/batch = 0.4036s	
564/2700 (epoch 10.444), train_loss = 1.64949455, grad/param norm = 9.2329e-02, time/batch = 0.4142s	
565/2700 (epoch 10.463), train_loss = 1.74091095, grad/param norm = 9.5538e-02, time/batch = 0.4226s	
566/2700 (epoch 10.481), train_loss = 1.71991138, grad/param norm = 9.1079e-02, time/batch = 0.4249s	
567/2700 (epoch 10.500), train_loss = 1.66432392, grad/param norm = 8.0117e-02, time/batch = 0.3578s	
568/2700 (epoch 10.519), train_loss = 1.69308342, grad/param norm = 8.4431e-02, time/batch = 0.4402s	
569/2700 (epoch 10.537), train_loss = 1.69930828, grad/param norm = 9.0160e-02, time/batch = 0.4624s	
570/2700 (epoch 10.556), train_loss = 1.65609811, grad/param norm = 1.0497e-01, time/batch = 0.4706s	
571/2700 (epoch 10.574), train_loss = 1.69217519, grad/param norm = 1.2399e-01, time/batch = 0.4545s	
572/2700 (epoch 10.593), train_loss = 1.71179891, grad/param norm = 1.2492e-01, time/batch = 0.4198s	
573/2700 (epoch 10.611), train_loss = 1.58757410, grad/param norm = 1.0483e-01, time/batch = 0.4014s	
574/2700 (epoch 10.630), train_loss = 1.62067282, grad/param norm = 1.1136e-01, time/batch = 0.3973s	
575/2700 (epoch 10.648), train_loss = 1.65934259, grad/param norm = 1.1740e-01, time/batch = 0.4065s	
576/2700 (epoch 10.667), train_loss = 1.62951291, grad/param norm = 1.0796e-01, time/batch = 0.4274s	
577/2700 (epoch 10.685), train_loss = 1.65218991, grad/param norm = 1.0430e-01, time/batch = 0.4343s	
578/2700 (epoch 10.704), train_loss = 1.66834220, grad/param norm = 1.0679e-01, time/batch = 0.3935s	
579/2700 (epoch 10.722), train_loss = 1.62765270, grad/param norm = 8.4845e-02, time/batch = 0.3948s	
580/2700 (epoch 10.741), train_loss = 1.61697771, grad/param norm = 7.2460e-02, time/batch = 0.4639s	
581/2700 (epoch 10.759), train_loss = 1.61365267, grad/param norm = 7.3448e-02, time/batch = 0.4537s	
582/2700 (epoch 10.778), train_loss = 1.65783309, grad/param norm = 8.1504e-02, time/batch = 0.4397s	
583/2700 (epoch 10.796), train_loss = 1.61930277, grad/param norm = 9.9264e-02, time/batch = 0.4008s	
584/2700 (epoch 10.815), train_loss = 1.67891773, grad/param norm = 1.1113e-01, time/batch = 0.4141s	
585/2700 (epoch 10.833), train_loss = 1.64986392, grad/param norm = 1.0875e-01, time/batch = 0.3920s	
586/2700 (epoch 10.852), train_loss = 1.64144402, grad/param norm = 1.0049e-01, time/batch = 0.4267s	
587/2700 (epoch 10.870), train_loss = 1.64633856, grad/param norm = 9.1033e-02, time/batch = 0.4362s	
588/2700 (epoch 10.889), train_loss = 1.64318027, grad/param norm = 8.7667e-02, time/batch = 0.3994s	
589/2700 (epoch 10.907), train_loss = 1.73236174, grad/param norm = 9.5038e-02, time/batch = 0.4129s	
590/2700 (epoch 10.926), train_loss = 1.69724636, grad/param norm = 1.0498e-01, time/batch = 0.4471s	
591/2700 (epoch 10.944), train_loss = 1.68871630, grad/param norm = 9.5391e-02, time/batch = 0.4419s	
592/2700 (epoch 10.963), train_loss = 1.67802818, grad/param norm = 9.1624e-02, time/batch = 0.4598s	
593/2700 (epoch 10.981), train_loss = 1.62680780, grad/param norm = 9.0970e-02, time/batch = 0.4436s	
decayed learning rate by a factor 0.97 to 0.0018818	
594/2700 (epoch 11.000), train_loss = 1.68946248, grad/param norm = 1.0080e-01, time/batch = 0.4105s	
595/2700 (epoch 11.019), train_loss = 1.71959439, grad/param norm = 1.0737e-01, time/batch = 0.4214s	
596/2700 (epoch 11.037), train_loss = 1.68970266, grad/param norm = 9.9613e-02, time/batch = 0.3833s	
597/2700 (epoch 11.056), train_loss = 1.62180279, grad/param norm = 9.5556e-02, time/batch = 0.4375s	
598/2700 (epoch 11.074), train_loss = 1.58232766, grad/param norm = 8.5229e-02, time/batch = 0.4294s	
599/2700 (epoch 11.093), train_loss = 1.55971619, grad/param norm = 8.0860e-02, time/batch = 0.4071s	
600/2700 (epoch 11.111), train_loss = 1.55123292, grad/param norm = 9.0562e-02, time/batch = 0.4186s	
601/2700 (epoch 11.130), train_loss = 1.61753034, grad/param norm = 8.8015e-02, time/batch = 0.4300s	
602/2700 (epoch 11.148), train_loss = 1.54039094, grad/param norm = 8.2127e-02, time/batch = 0.4546s	
603/2700 (epoch 11.167), train_loss = 1.63949967, grad/param norm = 7.4301e-02, time/batch = 0.4668s	
604/2700 (epoch 11.185), train_loss = 1.54627942, grad/param norm = 7.6641e-02, time/batch = 0.4382s	
605/2700 (epoch 11.204), train_loss = 1.61787732, grad/param norm = 9.1293e-02, time/batch = 0.4224s	
606/2700 (epoch 11.222), train_loss = 1.56128221, grad/param norm = 9.5487e-02, time/batch = 0.4314s	
607/2700 (epoch 11.241), train_loss = 1.49155646, grad/param norm = 8.9121e-02, time/batch = 0.3872s	
608/2700 (epoch 11.259), train_loss = 1.54192333, grad/param norm = 8.7054e-02, time/batch = 0.4386s	
609/2700 (epoch 11.278), train_loss = 1.61185500, grad/param norm = 9.0294e-02, time/batch = 0.4257s	
610/2700 (epoch 11.296), train_loss = 1.59544733, grad/param norm = 9.2875e-02, time/batch = 0.4022s	
611/2700 (epoch 11.315), train_loss = 1.60934390, grad/param norm = 9.8451e-02, time/batch = 0.3966s	
612/2700 (epoch 11.333), train_loss = 1.60845868, grad/param norm = 1.0430e-01, time/batch = 0.4382s	
613/2700 (epoch 11.352), train_loss = 1.61910650, grad/param norm = 1.0882e-01, time/batch = 0.4458s	
614/2700 (epoch 11.370), train_loss = 1.62536188, grad/param norm = 9.1166e-02, time/batch = 0.4792s	
615/2700 (epoch 11.389), train_loss = 1.56097235, grad/param norm = 8.3599e-02, time/batch = 0.4815s	
616/2700 (epoch 11.407), train_loss = 1.61014042, grad/param norm = 9.3338e-02, time/batch = 0.3852s	
617/2700 (epoch 11.426), train_loss = 1.64344390, grad/param norm = 9.2823e-02, time/batch = 0.4372s	
618/2700 (epoch 11.444), train_loss = 1.55359417, grad/param norm = 8.8779e-02, time/batch = 0.3912s	
619/2700 (epoch 11.463), train_loss = 1.65533680, grad/param norm = 1.0453e-01, time/batch = 0.4475s	
620/2700 (epoch 11.481), train_loss = 1.64939758, grad/param norm = 1.0814e-01, time/batch = 0.4148s	
621/2700 (epoch 11.500), train_loss = 1.57000702, grad/param norm = 9.1541e-02, time/batch = 0.3989s	
622/2700 (epoch 11.519), train_loss = 1.60919541, grad/param norm = 8.2059e-02, time/batch = 0.4041s	
623/2700 (epoch 11.537), train_loss = 1.60408991, grad/param norm = 9.2068e-02, time/batch = 0.4496s	
624/2700 (epoch 11.556), train_loss = 1.54275431, grad/param norm = 8.9910e-02, time/batch = 0.4646s	
625/2700 (epoch 11.574), train_loss = 1.55476280, grad/param norm = 9.6182e-02, time/batch = 0.4775s	
626/2700 (epoch 11.593), train_loss = 1.58798578, grad/param norm = 1.0868e-01, time/batch = 0.4301s	
627/2700 (epoch 11.611), train_loss = 1.49030041, grad/param norm = 9.9250e-02, time/batch = 0.4076s	
628/2700 (epoch 11.630), train_loss = 1.51888233, grad/param norm = 8.9813e-02, time/batch = 0.3729s	
629/2700 (epoch 11.648), train_loss = 1.54326053, grad/param norm = 8.9840e-02, time/batch = 0.3980s	
630/2700 (epoch 11.667), train_loss = 1.52368014, grad/param norm = 8.7443e-02, time/batch = 0.4380s	
631/2700 (epoch 11.685), train_loss = 1.55146924, grad/param norm = 9.5000e-02, time/batch = 0.4341s	
632/2700 (epoch 11.704), train_loss = 1.57808095, grad/param norm = 1.0092e-01, time/batch = 0.4000s	
633/2700 (epoch 11.722), train_loss = 1.54355061, grad/param norm = 8.5892e-02, time/batch = 0.4108s	
634/2700 (epoch 11.741), train_loss = 1.52734005, grad/param norm = 7.9041e-02, time/batch = 0.4497s	
635/2700 (epoch 11.759), train_loss = 1.52154416, grad/param norm = 7.9832e-02, time/batch = 0.4689s	
636/2700 (epoch 11.778), train_loss = 1.56274861, grad/param norm = 8.5260e-02, time/batch = 0.4721s	
637/2700 (epoch 11.796), train_loss = 1.52940639, grad/param norm = 1.0135e-01, time/batch = 0.4475s	
638/2700 (epoch 11.815), train_loss = 1.60283589, grad/param norm = 1.1871e-01, time/batch = 0.4050s	
639/2700 (epoch 11.833), train_loss = 1.57533758, grad/param norm = 1.0807e-01, time/batch = 0.3880s	
640/2700 (epoch 11.852), train_loss = 1.53443564, grad/param norm = 9.0965e-02, time/batch = 0.3771s	
641/2700 (epoch 11.870), train_loss = 1.54439654, grad/param norm = 8.0675e-02, time/batch = 0.4283s	
642/2700 (epoch 11.889), train_loss = 1.55253528, grad/param norm = 8.9402e-02, time/batch = 0.4213s	
643/2700 (epoch 11.907), train_loss = 1.64550054, grad/param norm = 1.0542e-01, time/batch = 0.3990s	
644/2700 (epoch 11.926), train_loss = 1.61379821, grad/param norm = 1.0028e-01, time/batch = 0.4213s	
645/2700 (epoch 11.944), train_loss = 1.57862842, grad/param norm = 9.5185e-02, time/batch = 0.4538s	
646/2700 (epoch 11.963), train_loss = 1.58235635, grad/param norm = 9.8794e-02, time/batch = 0.4552s	
647/2700 (epoch 11.981), train_loss = 1.51615617, grad/param norm = 8.5280e-02, time/batch = 0.4644s	
decayed learning rate by a factor 0.97 to 0.001825346	
648/2700 (epoch 12.000), train_loss = 1.58380141, grad/param norm = 9.1059e-02, time/batch = 0.4261s	
649/2700 (epoch 12.019), train_loss = 1.62488423, grad/param norm = 9.8403e-02, time/batch = 0.3887s	
650/2700 (epoch 12.037), train_loss = 1.57658699, grad/param norm = 9.4823e-02, time/batch = 0.3773s	
651/2700 (epoch 12.056), train_loss = 1.51564903, grad/param norm = 8.6318e-02, time/batch = 0.3757s	
652/2700 (epoch 12.074), train_loss = 1.48678886, grad/param norm = 8.4277e-02, time/batch = 0.4367s	
653/2700 (epoch 12.093), train_loss = 1.46149352, grad/param norm = 7.8114e-02, time/batch = 0.4187s	
654/2700 (epoch 12.111), train_loss = 1.45219931, grad/param norm = 7.9417e-02, time/batch = 0.4100s	
655/2700 (epoch 12.130), train_loss = 1.52427014, grad/param norm = 8.4398e-02, time/batch = 0.4322s	
656/2700 (epoch 12.148), train_loss = 1.46243497, grad/param norm = 8.8204e-02, time/batch = 0.4539s	
657/2700 (epoch 12.167), train_loss = 1.56308725, grad/param norm = 8.5315e-02, time/batch = 0.4734s	
658/2700 (epoch 12.185), train_loss = 1.47975631, grad/param norm = 9.5973e-02, time/batch = 0.4646s	
659/2700 (epoch 12.204), train_loss = 1.53336417, grad/param norm = 1.0125e-01, time/batch = 0.4239s	
660/2700 (epoch 12.222), train_loss = 1.47409848, grad/param norm = 9.5513e-02, time/batch = 0.3814s	
661/2700 (epoch 12.241), train_loss = 1.39937328, grad/param norm = 9.0114e-02, time/batch = 0.3717s	
662/2700 (epoch 12.259), train_loss = 1.45773636, grad/param norm = 9.5565e-02, time/batch = 0.3751s	
663/2700 (epoch 12.278), train_loss = 1.52823397, grad/param norm = 9.6649e-02, time/batch = 0.4555s	
664/2700 (epoch 12.296), train_loss = 1.52121138, grad/param norm = 1.0666e-01, time/batch = 0.4572s	
665/2700 (epoch 12.315), train_loss = 1.50063690, grad/param norm = 9.4849e-02, time/batch = 0.3720s	
666/2700 (epoch 12.333), train_loss = 1.47249925, grad/param norm = 7.9879e-02, time/batch = 0.4524s	
667/2700 (epoch 12.352), train_loss = 1.48229330, grad/param norm = 8.4289e-02, time/batch = 0.4712s	
668/2700 (epoch 12.370), train_loss = 1.53275820, grad/param norm = 1.1157e-01, time/batch = 0.4604s	
669/2700 (epoch 12.389), train_loss = 1.50398237, grad/param norm = 1.0326e-01, time/batch = 0.4315s	
670/2700 (epoch 12.407), train_loss = 1.52327224, grad/param norm = 9.1548e-02, time/batch = 0.3876s	
671/2700 (epoch 12.426), train_loss = 1.55168313, grad/param norm = 9.0912e-02, time/batch = 0.3789s	
672/2700 (epoch 12.444), train_loss = 1.46913799, grad/param norm = 8.3648e-02, time/batch = 0.3775s	
673/2700 (epoch 12.463), train_loss = 1.55051850, grad/param norm = 8.8327e-02, time/batch = 0.4063s	
674/2700 (epoch 12.481), train_loss = 1.53010080, grad/param norm = 8.7199e-02, time/batch = 0.4598s	
675/2700 (epoch 12.500), train_loss = 1.45580492, grad/param norm = 8.1840e-02, time/batch = 0.4371s	
676/2700 (epoch 12.519), train_loss = 1.53283690, grad/param norm = 1.0162e-01, time/batch = 0.4213s	
677/2700 (epoch 12.537), train_loss = 1.52220602, grad/param norm = 9.8484e-02, time/batch = 0.4023s	
678/2700 (epoch 12.556), train_loss = 1.45423834, grad/param norm = 9.9150e-02, time/batch = 0.4564s	
679/2700 (epoch 12.574), train_loss = 1.47257840, grad/param norm = 1.0691e-01, time/batch = 0.4683s	
680/2700 (epoch 12.593), train_loss = 1.50310622, grad/param norm = 1.0056e-01, time/batch = 0.4447s	
681/2700 (epoch 12.611), train_loss = 1.39096171, grad/param norm = 7.7901e-02, time/batch = 0.4291s	
682/2700 (epoch 12.630), train_loss = 1.41754301, grad/param norm = 8.3302e-02, time/batch = 0.4008s	
683/2700 (epoch 12.648), train_loss = 1.44717320, grad/param norm = 8.9007e-02, time/batch = 0.3938s	
684/2700 (epoch 12.667), train_loss = 1.43427969, grad/param norm = 8.6778e-02, time/batch = 0.3806s	
685/2700 (epoch 12.685), train_loss = 1.45640445, grad/param norm = 8.4735e-02, time/batch = 0.4513s	
686/2700 (epoch 12.704), train_loss = 1.47334483, grad/param norm = 8.9077e-02, time/batch = 0.4332s	
687/2700 (epoch 12.722), train_loss = 1.45141506, grad/param norm = 8.3080e-02, time/batch = 0.4095s	
688/2700 (epoch 12.741), train_loss = 1.44110773, grad/param norm = 8.6796e-02, time/batch = 0.4254s	
689/2700 (epoch 12.759), train_loss = 1.44552221, grad/param norm = 9.2207e-02, time/batch = 0.4436s	
690/2700 (epoch 12.778), train_loss = 1.47871499, grad/param norm = 1.0217e-01, time/batch = 0.4749s	
691/2700 (epoch 12.796), train_loss = 1.43638503, grad/param norm = 9.3632e-02, time/batch = 0.4623s	
692/2700 (epoch 12.815), train_loss = 1.47940114, grad/param norm = 9.5049e-02, time/batch = 0.4299s	
693/2700 (epoch 12.833), train_loss = 1.48272675, grad/param norm = 1.2150e-01, time/batch = 0.4183s	
694/2700 (epoch 12.852), train_loss = 1.46216446, grad/param norm = 1.0288e-01, time/batch = 0.3936s	
695/2700 (epoch 12.870), train_loss = 1.46516379, grad/param norm = 9.3663e-02, time/batch = 0.3773s	
696/2700 (epoch 12.889), train_loss = 1.46123373, grad/param norm = 8.5693e-02, time/batch = 0.4473s	
697/2700 (epoch 12.907), train_loss = 1.53008994, grad/param norm = 9.1120e-02, time/batch = 0.4386s	
698/2700 (epoch 12.926), train_loss = 1.49337279, grad/param norm = 8.9118e-02, time/batch = 0.4121s	
699/2700 (epoch 12.944), train_loss = 1.46330170, grad/param norm = 8.3689e-02, time/batch = 0.4149s	
700/2700 (epoch 12.963), train_loss = 1.46985673, grad/param norm = 8.3377e-02, time/batch = 0.4475s	
701/2700 (epoch 12.981), train_loss = 1.41088590, grad/param norm = 8.3287e-02, time/batch = 0.4406s	
decayed learning rate by a factor 0.97 to 0.00177058562	
702/2700 (epoch 13.000), train_loss = 1.49437052, grad/param norm = 9.2105e-02, time/batch = 0.4445s	
703/2700 (epoch 13.019), train_loss = 1.53231791, grad/param norm = 9.5740e-02, time/batch = 0.4391s	
704/2700 (epoch 13.037), train_loss = 1.48375758, grad/param norm = 9.3192e-02, time/batch = 0.3993s	
705/2700 (epoch 13.056), train_loss = 1.43900722, grad/param norm = 1.1035e-01, time/batch = 0.3867s	
706/2700 (epoch 13.074), train_loss = 1.41935393, grad/param norm = 1.0059e-01, time/batch = 0.3944s	
707/2700 (epoch 13.093), train_loss = 1.37496829, grad/param norm = 8.0044e-02, time/batch = 0.4576s	
708/2700 (epoch 13.111), train_loss = 1.36332781, grad/param norm = 8.2201e-02, time/batch = 0.4334s	
709/2700 (epoch 13.130), train_loss = 1.42475314, grad/param norm = 8.5665e-02, time/batch = 0.4170s	
710/2700 (epoch 13.148), train_loss = 1.36454753, grad/param norm = 8.5195e-02, time/batch = 0.4215s	
711/2700 (epoch 13.167), train_loss = 1.47664661, grad/param norm = 9.1255e-02, time/batch = 0.4269s	
712/2700 (epoch 13.185), train_loss = 1.38734890, grad/param norm = 9.1327e-02, time/batch = 0.4541s	
713/2700 (epoch 13.204), train_loss = 1.44071313, grad/param norm = 9.6295e-02, time/batch = 0.4501s	
714/2700 (epoch 13.222), train_loss = 1.39785658, grad/param norm = 1.0107e-01, time/batch = 0.3935s	
715/2700 (epoch 13.241), train_loss = 1.32426526, grad/param norm = 8.7158e-02, time/batch = 0.3689s	
716/2700 (epoch 13.259), train_loss = 1.36151058, grad/param norm = 8.9420e-02, time/batch = 0.4082s	
717/2700 (epoch 13.278), train_loss = 1.44220472, grad/param norm = 1.0441e-01, time/batch = 0.4292s	
718/2700 (epoch 13.296), train_loss = 1.44611360, grad/param norm = 1.1471e-01, time/batch = 0.4660s	
719/2700 (epoch 13.315), train_loss = 1.42144468, grad/param norm = 1.1219e-01, time/batch = 0.4378s	
720/2700 (epoch 13.333), train_loss = 1.40289469, grad/param norm = 9.7679e-02, time/batch = 0.4216s	
721/2700 (epoch 13.352), train_loss = 1.40775218, grad/param norm = 9.8356e-02, time/batch = 0.3974s	
722/2700 (epoch 13.370), train_loss = 1.41321404, grad/param norm = 9.7063e-02, time/batch = 0.4253s	
723/2700 (epoch 13.389), train_loss = 1.40641367, grad/param norm = 1.0303e-01, time/batch = 0.4421s	
724/2700 (epoch 13.407), train_loss = 1.43847066, grad/param norm = 9.3696e-02, time/batch = 0.4506s	
725/2700 (epoch 13.426), train_loss = 1.45488143, grad/param norm = 9.1068e-02, time/batch = 0.4345s	
726/2700 (epoch 13.444), train_loss = 1.38509317, grad/param norm = 8.9459e-02, time/batch = 0.3416s	
727/2700 (epoch 13.463), train_loss = 1.47796636, grad/param norm = 1.0533e-01, time/batch = 0.4058s	
728/2700 (epoch 13.481), train_loss = 1.44713614, grad/param norm = 9.8073e-02, time/batch = 0.4262s	
729/2700 (epoch 13.500), train_loss = 1.36029986, grad/param norm = 8.2713e-02, time/batch = 0.4528s	
730/2700 (epoch 13.519), train_loss = 1.41228692, grad/param norm = 8.2931e-02, time/batch = 0.4197s	
731/2700 (epoch 13.537), train_loss = 1.40900707, grad/param norm = 9.2909e-02, time/batch = 0.4141s	
732/2700 (epoch 13.556), train_loss = 1.35220758, grad/param norm = 9.3059e-02, time/batch = 0.4035s	
733/2700 (epoch 13.574), train_loss = 1.35530075, grad/param norm = 9.2326e-02, time/batch = 0.4479s	
734/2700 (epoch 13.593), train_loss = 1.40736381, grad/param norm = 9.5240e-02, time/batch = 0.4594s	
735/2700 (epoch 13.611), train_loss = 1.33431943, grad/param norm = 9.3328e-02, time/batch = 0.4712s	
736/2700 (epoch 13.630), train_loss = 1.33946044, grad/param norm = 9.4326e-02, time/batch = 0.4145s	
737/2700 (epoch 13.648), train_loss = 1.35755429, grad/param norm = 9.2054e-02, time/batch = 0.3845s	
738/2700 (epoch 13.667), train_loss = 1.34360984, grad/param norm = 8.7855e-02, time/batch = 0.3528s	
739/2700 (epoch 13.685), train_loss = 1.38776585, grad/param norm = 9.5461e-02, time/batch = 0.4204s	
740/2700 (epoch 13.704), train_loss = 1.39311874, grad/param norm = 1.0523e-01, time/batch = 0.4527s	
741/2700 (epoch 13.722), train_loss = 1.38506214, grad/param norm = 9.5126e-02, time/batch = 0.4427s	
742/2700 (epoch 13.741), train_loss = 1.35416681, grad/param norm = 8.8267e-02, time/batch = 0.3945s	
743/2700 (epoch 13.759), train_loss = 1.34495526, grad/param norm = 9.1280e-02, time/batch = 0.4099s	
744/2700 (epoch 13.778), train_loss = 1.37855765, grad/param norm = 1.0085e-01, time/batch = 0.4522s	
745/2700 (epoch 13.796), train_loss = 1.37000779, grad/param norm = 1.1478e-01, time/batch = 0.4588s	
746/2700 (epoch 13.815), train_loss = 1.40992015, grad/param norm = 9.8648e-02, time/batch = 0.4432s	
747/2700 (epoch 13.833), train_loss = 1.37779853, grad/param norm = 1.0452e-01, time/batch = 0.3832s	
748/2700 (epoch 13.852), train_loss = 1.35287385, grad/param norm = 9.3111e-02, time/batch = 0.3584s	
749/2700 (epoch 13.870), train_loss = 1.35823701, grad/param norm = 8.2894e-02, time/batch = 0.4150s	
750/2700 (epoch 13.889), train_loss = 1.35925620, grad/param norm = 9.1832e-02, time/batch = 0.4350s	
751/2700 (epoch 13.907), train_loss = 1.43015347, grad/param norm = 9.7234e-02, time/batch = 0.4461s	
752/2700 (epoch 13.926), train_loss = 1.42718664, grad/param norm = 1.1470e-01, time/batch = 0.4081s	
753/2700 (epoch 13.944), train_loss = 1.39976063, grad/param norm = 9.5479e-02, time/batch = 0.4031s	
754/2700 (epoch 13.963), train_loss = 1.37146282, grad/param norm = 8.9453e-02, time/batch = 0.4402s	
755/2700 (epoch 13.981), train_loss = 1.33188323, grad/param norm = 9.8508e-02, time/batch = 0.4647s	
decayed learning rate by a factor 0.97 to 0.0017174680514	
756/2700 (epoch 14.000), train_loss = 1.38991491, grad/param norm = 9.5249e-02, time/batch = 0.4729s	
757/2700 (epoch 14.019), train_loss = 1.42616164, grad/param norm = 8.9772e-02, time/batch = 0.4465s	
758/2700 (epoch 14.037), train_loss = 1.37926642, grad/param norm = 9.0729e-02, time/batch = 0.3997s	
759/2700 (epoch 14.056), train_loss = 1.32488232, grad/param norm = 8.6086e-02, time/batch = 0.3589s	
760/2700 (epoch 14.074), train_loss = 1.30536544, grad/param norm = 8.6599e-02, time/batch = 0.4035s	
761/2700 (epoch 14.093), train_loss = 1.28419801, grad/param norm = 8.6595e-02, time/batch = 0.4010s	
762/2700 (epoch 14.111), train_loss = 1.28168824, grad/param norm = 9.0576e-02, time/batch = 0.4502s	
763/2700 (epoch 14.130), train_loss = 1.33277266, grad/param norm = 8.7945e-02, time/batch = 0.3778s	
764/2700 (epoch 14.148), train_loss = 1.26411458, grad/param norm = 8.2089e-02, time/batch = 0.4312s	
765/2700 (epoch 14.167), train_loss = 1.37781952, grad/param norm = 9.2131e-02, time/batch = 0.4606s	
766/2700 (epoch 14.185), train_loss = 1.29583155, grad/param norm = 9.1362e-02, time/batch = 0.4755s	
767/2700 (epoch 14.204), train_loss = 1.35242492, grad/param norm = 9.3278e-02, time/batch = 0.4758s	
768/2700 (epoch 14.222), train_loss = 1.30733736, grad/param norm = 9.7628e-02, time/batch = 0.4379s	
769/2700 (epoch 14.241), train_loss = 1.25421980, grad/param norm = 1.0013e-01, time/batch = 0.4065s	
770/2700 (epoch 14.259), train_loss = 1.27778016, grad/param norm = 1.0190e-01, time/batch = 0.3775s	
771/2700 (epoch 14.278), train_loss = 1.32409267, grad/param norm = 9.0820e-02, time/batch = 0.3913s	
772/2700 (epoch 14.296), train_loss = 1.32450621, grad/param norm = 9.6665e-02, time/batch = 0.3970s	
773/2700 (epoch 14.315), train_loss = 1.32355430, grad/param norm = 1.1396e-01, time/batch = 0.4485s	
774/2700 (epoch 14.333), train_loss = 1.31253378, grad/param norm = 1.0766e-01, time/batch = 0.4303s	
775/2700 (epoch 14.352), train_loss = 1.31892636, grad/param norm = 1.0745e-01, time/batch = 0.3862s	
776/2700 (epoch 14.370), train_loss = 1.30320679, grad/param norm = 9.4884e-02, time/batch = 0.4609s	
777/2700 (epoch 14.389), train_loss = 1.29138946, grad/param norm = 9.5648e-02, time/batch = 0.4753s	
778/2700 (epoch 14.407), train_loss = 1.34934334, grad/param norm = 9.8746e-02, time/batch = 0.4731s	
779/2700 (epoch 14.426), train_loss = 1.37676849, grad/param norm = 1.0826e-01, time/batch = 0.4462s	
780/2700 (epoch 14.444), train_loss = 1.30247526, grad/param norm = 9.7146e-02, time/batch = 0.3966s	
781/2700 (epoch 14.463), train_loss = 1.35201908, grad/param norm = 9.0342e-02, time/batch = 0.4011s	
782/2700 (epoch 14.481), train_loss = 1.33148816, grad/param norm = 9.0003e-02, time/batch = 0.4074s	
783/2700 (epoch 14.500), train_loss = 1.28082364, grad/param norm = 9.6533e-02, time/batch = 0.3864s	
784/2700 (epoch 14.519), train_loss = 1.34494260, grad/param norm = 1.0011e-01, time/batch = 0.4507s	
785/2700 (epoch 14.537), train_loss = 1.32236050, grad/param norm = 9.4374e-02, time/batch = 0.4247s	
786/2700 (epoch 14.556), train_loss = 1.26051835, grad/param norm = 9.6384e-02, time/batch = 0.4027s	
787/2700 (epoch 14.574), train_loss = 1.26718672, grad/param norm = 9.9841e-02, time/batch = 0.4168s	
788/2700 (epoch 14.593), train_loss = 1.33007876, grad/param norm = 1.0884e-01, time/batch = 0.4746s	
789/2700 (epoch 14.611), train_loss = 1.24632485, grad/param norm = 8.9586e-02, time/batch = 0.4824s	
790/2700 (epoch 14.630), train_loss = 1.25240961, grad/param norm = 9.5556e-02, time/batch = 0.4597s	
791/2700 (epoch 14.648), train_loss = 1.26612349, grad/param norm = 9.5250e-02, time/batch = 0.4262s	
792/2700 (epoch 14.667), train_loss = 1.25393648, grad/param norm = 9.2046e-02, time/batch = 0.3830s	
793/2700 (epoch 14.685), train_loss = 1.28821504, grad/param norm = 9.4198e-02, time/batch = 0.4076s	
794/2700 (epoch 14.704), train_loss = 1.28673367, grad/param norm = 9.6120e-02, time/batch = 0.3828s	
795/2700 (epoch 14.722), train_loss = 1.28280168, grad/param norm = 9.6187e-02, time/batch = 0.4468s	
796/2700 (epoch 14.741), train_loss = 1.24671804, grad/param norm = 8.5232e-02, time/batch = 0.4298s	
797/2700 (epoch 14.759), train_loss = 1.24384677, grad/param norm = 9.0852e-02, time/batch = 0.4059s	
798/2700 (epoch 14.778), train_loss = 1.30626190, grad/param norm = 1.1495e-01, time/batch = 0.4246s	
799/2700 (epoch 14.796), train_loss = 1.26364758, grad/param norm = 1.1201e-01, time/batch = 0.4480s	
800/2700 (epoch 14.815), train_loss = 1.32073359, grad/param norm = 1.0396e-01, time/batch = 0.4745s	
801/2700 (epoch 14.833), train_loss = 1.27377175, grad/param norm = 1.1281e-01, time/batch = 0.4610s	
802/2700 (epoch 14.852), train_loss = 1.28420281, grad/param norm = 1.1438e-01, time/batch = 0.4159s	
803/2700 (epoch 14.870), train_loss = 1.27747105, grad/param norm = 9.4232e-02, time/batch = 0.4139s	
804/2700 (epoch 14.889), train_loss = 1.26056575, grad/param norm = 9.6152e-02, time/batch = 0.4068s	
805/2700 (epoch 14.907), train_loss = 1.32492851, grad/param norm = 1.0261e-01, time/batch = 0.3837s	
806/2700 (epoch 14.926), train_loss = 1.32362936, grad/param norm = 1.2304e-01, time/batch = 0.4487s	
807/2700 (epoch 14.944), train_loss = 1.28913682, grad/param norm = 9.9490e-02, time/batch = 0.4285s	
808/2700 (epoch 14.963), train_loss = 1.26985905, grad/param norm = 8.8776e-02, time/batch = 0.4044s	
809/2700 (epoch 14.981), train_loss = 1.22313682, grad/param norm = 9.4738e-02, time/batch = 0.4247s	
decayed learning rate by a factor 0.97 to 0.001665944009858	
810/2700 (epoch 15.000), train_loss = 1.28883810, grad/param norm = 9.9161e-02, time/batch = 0.4551s	
811/2700 (epoch 15.019), train_loss = 1.34764579, grad/param norm = 1.0245e-01, time/batch = 0.4467s	
812/2700 (epoch 15.037), train_loss = 1.28478622, grad/param norm = 1.0192e-01, time/batch = 0.4406s	
813/2700 (epoch 15.056), train_loss = 1.23519974, grad/param norm = 9.3436e-02, time/batch = 0.4063s	
814/2700 (epoch 15.074), train_loss = 1.22063452, grad/param norm = 9.9492e-02, time/batch = 0.4094s	
815/2700 (epoch 15.093), train_loss = 1.18270398, grad/param norm = 8.3740e-02, time/batch = 0.4177s	
816/2700 (epoch 15.111), train_loss = 1.18469800, grad/param norm = 8.6346e-02, time/batch = 0.3995s	
817/2700 (epoch 15.130), train_loss = 1.22648380, grad/param norm = 8.8668e-02, time/batch = 0.4401s	
818/2700 (epoch 15.148), train_loss = 1.16944641, grad/param norm = 8.7686e-02, time/batch = 0.4135s	
819/2700 (epoch 15.167), train_loss = 1.27644211, grad/param norm = 9.8551e-02, time/batch = 0.4144s	
820/2700 (epoch 15.185), train_loss = 1.20726195, grad/param norm = 1.0175e-01, time/batch = 0.4412s	
821/2700 (epoch 15.204), train_loss = 1.27505844, grad/param norm = 1.1745e-01, time/batch = 0.4471s	
822/2700 (epoch 15.222), train_loss = 1.23008366, grad/param norm = 1.0622e-01, time/batch = 0.4622s	
823/2700 (epoch 15.241), train_loss = 1.16628662, grad/param norm = 9.9892e-02, time/batch = 0.4699s	
824/2700 (epoch 15.259), train_loss = 1.18974316, grad/param norm = 1.0488e-01, time/batch = 0.3789s	
825/2700 (epoch 15.278), train_loss = 1.24563064, grad/param norm = 1.1690e-01, time/batch = 0.4178s	
826/2700 (epoch 15.296), train_loss = 1.26726854, grad/param norm = 1.1669e-01, time/batch = 0.4234s	
827/2700 (epoch 15.315), train_loss = 1.20092412, grad/param norm = 9.5074e-02, time/batch = 0.4040s	
828/2700 (epoch 15.333), train_loss = 1.20408115, grad/param norm = 1.0450e-01, time/batch = 0.4239s	
829/2700 (epoch 15.352), train_loss = 1.22408693, grad/param norm = 1.1541e-01, time/batch = 0.4015s	
830/2700 (epoch 15.370), train_loss = 1.23013164, grad/param norm = 1.2346e-01, time/batch = 0.4295s	
831/2700 (epoch 15.389), train_loss = 1.21294059, grad/param norm = 1.1114e-01, time/batch = 0.4361s	
832/2700 (epoch 15.407), train_loss = 1.24383176, grad/param norm = 8.9117e-02, time/batch = 0.4591s	
833/2700 (epoch 15.426), train_loss = 1.24368313, grad/param norm = 9.0472e-02, time/batch = 0.4723s	
834/2700 (epoch 15.444), train_loss = 1.19862317, grad/param norm = 9.2825e-02, time/batch = 0.4547s	
835/2700 (epoch 15.463), train_loss = 1.27476506, grad/param norm = 1.1323e-01, time/batch = 0.3842s	
836/2700 (epoch 15.481), train_loss = 1.24581397, grad/param norm = 1.1229e-01, time/batch = 0.3517s	
837/2700 (epoch 15.500), train_loss = 1.18452352, grad/param norm = 9.9132e-02, time/batch = 0.4184s	
838/2700 (epoch 15.519), train_loss = 1.21857270, grad/param norm = 9.0613e-02, time/batch = 0.4170s	
839/2700 (epoch 15.537), train_loss = 1.23029299, grad/param norm = 1.0600e-01, time/batch = 0.4128s	
840/2700 (epoch 15.556), train_loss = 1.18235627, grad/param norm = 1.0863e-01, time/batch = 0.4137s	
841/2700 (epoch 15.574), train_loss = 1.17865298, grad/param norm = 1.0624e-01, time/batch = 0.4146s	
842/2700 (epoch 15.593), train_loss = 1.21138553, grad/param norm = 9.8947e-02, time/batch = 0.4483s	
843/2700 (epoch 15.611), train_loss = 1.15382362, grad/param norm = 9.8635e-02, time/batch = 0.4683s	
844/2700 (epoch 15.630), train_loss = 1.16900454, grad/param norm = 9.6273e-02, time/batch = 0.4682s	
845/2700 (epoch 15.648), train_loss = 1.15691110, grad/param norm = 9.3333e-02, time/batch = 0.4311s	
846/2700 (epoch 15.667), train_loss = 1.16455869, grad/param norm = 9.3306e-02, time/batch = 0.3974s	
847/2700 (epoch 15.685), train_loss = 1.18495673, grad/param norm = 9.2388e-02, time/batch = 0.3659s	
848/2700 (epoch 15.704), train_loss = 1.19404561, grad/param norm = 1.0277e-01, time/batch = 0.3980s	
849/2700 (epoch 15.722), train_loss = 1.18246229, grad/param norm = 1.0210e-01, time/batch = 0.4382s	
850/2700 (epoch 15.741), train_loss = 1.16314673, grad/param norm = 9.8784e-02, time/batch = 0.4340s	
851/2700 (epoch 15.759), train_loss = 1.15521051, grad/param norm = 1.0905e-01, time/batch = 0.4350s	
852/2700 (epoch 15.778), train_loss = 1.17396664, grad/param norm = 1.0372e-01, time/batch = 0.3903s	
853/2700 (epoch 15.796), train_loss = 1.14531101, grad/param norm = 1.1323e-01, time/batch = 0.4240s	
854/2700 (epoch 15.815), train_loss = 1.23506008, grad/param norm = 1.2617e-01, time/batch = 0.4559s	
855/2700 (epoch 15.833), train_loss = 1.21075945, grad/param norm = 1.1396e-01, time/batch = 0.4696s	
856/2700 (epoch 15.852), train_loss = 1.20014118, grad/param norm = 1.2205e-01, time/batch = 0.4359s	
857/2700 (epoch 15.870), train_loss = 1.18788968, grad/param norm = 1.0380e-01, time/batch = 0.3954s	
858/2700 (epoch 15.889), train_loss = 1.17816559, grad/param norm = 1.1540e-01, time/batch = 0.3667s	
859/2700 (epoch 15.907), train_loss = 1.23371776, grad/param norm = 1.1619e-01, time/batch = 0.4024s	
860/2700 (epoch 15.926), train_loss = 1.21110049, grad/param norm = 1.1063e-01, time/batch = 0.4209s	
861/2700 (epoch 15.944), train_loss = 1.19646183, grad/param norm = 1.0954e-01, time/batch = 0.4359s	
862/2700 (epoch 15.963), train_loss = 1.18265465, grad/param norm = 1.0043e-01, time/batch = 0.3967s	
863/2700 (epoch 15.981), train_loss = 1.12114376, grad/param norm = 9.9866e-02, time/batch = 0.4091s	
decayed learning rate by a factor 0.97 to 0.0016159656895623	
864/2700 (epoch 16.000), train_loss = 1.18293652, grad/param norm = 1.1020e-01, time/batch = 0.4489s	
865/2700 (epoch 16.019), train_loss = 1.26828006, grad/param norm = 1.2836e-01, time/batch = 0.4660s	
866/2700 (epoch 16.037), train_loss = 1.20736905, grad/param norm = 1.1376e-01, time/batch = 0.4716s	
867/2700 (epoch 16.056), train_loss = 1.15572411, grad/param norm = 1.0446e-01, time/batch = 0.4503s	
868/2700 (epoch 16.074), train_loss = 1.12032438, grad/param norm = 9.2372e-02, time/batch = 0.4119s	
869/2700 (epoch 16.093), train_loss = 1.10990419, grad/param norm = 1.0327e-01, time/batch = 0.3755s	
870/2700 (epoch 16.111), train_loss = 1.11955944, grad/param norm = 1.0957e-01, time/batch = 0.3929s	
871/2700 (epoch 16.130), train_loss = 1.15093944, grad/param norm = 1.0164e-01, time/batch = 0.3892s	
872/2700 (epoch 16.148), train_loss = 1.07786498, grad/param norm = 9.1920e-02, time/batch = 0.4449s	
873/2700 (epoch 16.167), train_loss = 1.16305596, grad/param norm = 9.7713e-02, time/batch = 0.3861s	
874/2700 (epoch 16.185), train_loss = 1.11831268, grad/param norm = 1.0323e-01, time/batch = 0.4247s	
875/2700 (epoch 16.204), train_loss = 1.15185386, grad/param norm = 9.6632e-02, time/batch = 0.4583s	
876/2700 (epoch 16.222), train_loss = 1.12229511, grad/param norm = 1.0582e-01, time/batch = 0.4727s	
877/2700 (epoch 16.241), train_loss = 1.08832272, grad/param norm = 1.0917e-01, time/batch = 0.4844s	
878/2700 (epoch 16.259), train_loss = 1.09725068, grad/param norm = 1.1376e-01, time/batch = 0.4351s	
879/2700 (epoch 16.278), train_loss = 1.13189358, grad/param norm = 1.0460e-01, time/batch = 0.4085s	
880/2700 (epoch 16.296), train_loss = 1.12997693, grad/param norm = 9.8596e-02, time/batch = 0.3928s	
881/2700 (epoch 16.315), train_loss = 1.10843706, grad/param norm = 1.1262e-01, time/batch = 0.4023s	
882/2700 (epoch 16.333), train_loss = 1.11035793, grad/param norm = 1.1686e-01, time/batch = 0.3851s	
883/2700 (epoch 16.352), train_loss = 1.10323986, grad/param norm = 1.0788e-01, time/batch = 0.4527s	
884/2700 (epoch 16.370), train_loss = 1.13956863, grad/param norm = 1.4399e-01, time/batch = 0.4282s	
885/2700 (epoch 16.389), train_loss = 1.13757431, grad/param norm = 1.2162e-01, time/batch = 0.3804s	
886/2700 (epoch 16.407), train_loss = 1.19832228, grad/param norm = 1.3142e-01, time/batch = 0.4584s	
887/2700 (epoch 16.426), train_loss = 1.17569432, grad/param norm = 1.0346e-01, time/batch = 0.4746s	
888/2700 (epoch 16.444), train_loss = 1.10112742, grad/param norm = 9.2870e-02, time/batch = 0.4646s	
889/2700 (epoch 16.463), train_loss = 1.14762524, grad/param norm = 9.9230e-02, time/batch = 0.4228s	
890/2700 (epoch 16.481), train_loss = 1.13310642, grad/param norm = 1.1925e-01, time/batch = 0.3979s	
891/2700 (epoch 16.500), train_loss = 1.10213544, grad/param norm = 1.1605e-01, time/batch = 0.3976s	
892/2700 (epoch 16.519), train_loss = 1.13462267, grad/param norm = 1.0450e-01, time/batch = 0.4070s	
893/2700 (epoch 16.537), train_loss = 1.13529396, grad/param norm = 1.1774e-01, time/batch = 0.4026s	
894/2700 (epoch 16.556), train_loss = 1.07580137, grad/param norm = 1.0124e-01, time/batch = 0.4406s	
895/2700 (epoch 16.574), train_loss = 1.06436031, grad/param norm = 1.0004e-01, time/batch = 0.4225s	
896/2700 (epoch 16.593), train_loss = 1.10874295, grad/param norm = 1.0483e-01, time/batch = 0.4085s	
897/2700 (epoch 16.611), train_loss = 1.05644053, grad/param norm = 1.1214e-01, time/batch = 0.4338s	
898/2700 (epoch 16.630), train_loss = 1.07672871, grad/param norm = 1.0013e-01, time/batch = 0.4634s	
899/2700 (epoch 16.648), train_loss = 1.06327195, grad/param norm = 1.0855e-01, time/batch = 0.4593s	
900/2700 (epoch 16.667), train_loss = 1.09130692, grad/param norm = 1.1218e-01, time/batch = 0.4211s	
901/2700 (epoch 16.685), train_loss = 1.10875407, grad/param norm = 1.1359e-01, time/batch = 0.4280s	
902/2700 (epoch 16.704), train_loss = 1.11199165, grad/param norm = 1.1860e-01, time/batch = 0.3894s	
903/2700 (epoch 16.722), train_loss = 1.07713299, grad/param norm = 9.9754e-02, time/batch = 0.3906s	
904/2700 (epoch 16.741), train_loss = 1.06988427, grad/param norm = 1.1110e-01, time/batch = 0.3930s	
905/2700 (epoch 16.759), train_loss = 1.05078681, grad/param norm = 1.1133e-01, time/batch = 0.4549s	
906/2700 (epoch 16.778), train_loss = 1.10000895, grad/param norm = 1.3389e-01, time/batch = 0.4371s	
907/2700 (epoch 16.796), train_loss = 1.09302074, grad/param norm = 1.2975e-01, time/batch = 0.4141s	
908/2700 (epoch 16.815), train_loss = 1.11568507, grad/param norm = 1.1527e-01, time/batch = 0.4226s	
909/2700 (epoch 16.833), train_loss = 1.07595171, grad/param norm = 1.0256e-01, time/batch = 0.4409s	
910/2700 (epoch 16.852), train_loss = 1.07174797, grad/param norm = 1.0832e-01, time/batch = 0.4715s	
911/2700 (epoch 16.870), train_loss = 1.08810947, grad/param norm = 1.1145e-01, time/batch = 0.4549s	
912/2700 (epoch 16.889), train_loss = 1.06249611, grad/param norm = 1.0183e-01, time/batch = 0.4184s	
913/2700 (epoch 16.907), train_loss = 1.10730459, grad/param norm = 1.1693e-01, time/batch = 0.3876s	
914/2700 (epoch 16.926), train_loss = 1.10693317, grad/param norm = 1.1895e-01, time/batch = 0.3887s	
915/2700 (epoch 16.944), train_loss = 1.08772042, grad/param norm = 1.2036e-01, time/batch = 0.3960s	
916/2700 (epoch 16.963), train_loss = 1.09164081, grad/param norm = 1.1031e-01, time/batch = 0.4575s	
917/2700 (epoch 16.981), train_loss = 1.03247897, grad/param norm = 1.1324e-01, time/batch = 0.4357s	
decayed learning rate by a factor 0.97 to 0.0015674867188754	
918/2700 (epoch 17.000), train_loss = 1.06890525, grad/param norm = 1.0683e-01, time/batch = 0.4109s	
919/2700 (epoch 17.019), train_loss = 1.15809099, grad/param norm = 1.1965e-01, time/batch = 0.4251s	
920/2700 (epoch 17.037), train_loss = 1.10109486, grad/param norm = 1.1712e-01, time/batch = 0.4522s	
921/2700 (epoch 17.056), train_loss = 1.06684380, grad/param norm = 1.1271e-01, time/batch = 0.4451s	
922/2700 (epoch 17.074), train_loss = 1.04236788, grad/param norm = 1.1255e-01, time/batch = 0.4485s	
923/2700 (epoch 17.093), train_loss = 1.02049283, grad/param norm = 1.0495e-01, time/batch = 0.4363s	
924/2700 (epoch 17.111), train_loss = 1.00495137, grad/param norm = 9.8372e-02, time/batch = 0.4015s	
925/2700 (epoch 17.130), train_loss = 1.05010660, grad/param norm = 1.1148e-01, time/batch = 0.3920s	
926/2700 (epoch 17.148), train_loss = 0.98766296, grad/param norm = 9.7957e-02, time/batch = 0.3908s	
927/2700 (epoch 17.167), train_loss = 1.05640895, grad/param norm = 1.0568e-01, time/batch = 0.4556s	
928/2700 (epoch 17.185), train_loss = 1.01656645, grad/param norm = 1.0462e-01, time/batch = 0.4359s	
929/2700 (epoch 17.204), train_loss = 1.05263665, grad/param norm = 1.0549e-01, time/batch = 0.4127s	
930/2700 (epoch 17.222), train_loss = 1.03417597, grad/param norm = 1.2561e-01, time/batch = 0.4232s	
931/2700 (epoch 17.241), train_loss = 0.98647858, grad/param norm = 1.1377e-01, time/batch = 0.4255s	
932/2700 (epoch 17.259), train_loss = 1.00077604, grad/param norm = 1.1674e-01, time/batch = 0.4535s	
933/2700 (epoch 17.278), train_loss = 1.03364602, grad/param norm = 1.1047e-01, time/batch = 0.4667s	
934/2700 (epoch 17.296), train_loss = 1.04288947, grad/param norm = 1.1138e-01, time/batch = 0.4495s	
935/2700 (epoch 17.315), train_loss = 0.99592012, grad/param norm = 1.1634e-01, time/batch = 0.4268s	
936/2700 (epoch 17.333), train_loss = 1.00558916, grad/param norm = 1.1406e-01, time/batch = 0.4009s	
937/2700 (epoch 17.352), train_loss = 1.01733228, grad/param norm = 1.3208e-01, time/batch = 0.3763s	
938/2700 (epoch 17.370), train_loss = 1.02112138, grad/param norm = 1.2504e-01, time/batch = 0.4478s	
939/2700 (epoch 17.389), train_loss = 1.03661286, grad/param norm = 1.2996e-01, time/batch = 0.4458s	
940/2700 (epoch 17.407), train_loss = 1.08438741, grad/param norm = 1.1569e-01, time/batch = 0.4171s	
941/2700 (epoch 17.426), train_loss = 1.06625101, grad/param norm = 1.1423e-01, time/batch = 0.4041s	
942/2700 (epoch 17.444), train_loss = 1.04495101, grad/param norm = 1.1787e-01, time/batch = 0.4057s	
943/2700 (epoch 17.463), train_loss = 1.05280805, grad/param norm = 1.1422e-01, time/batch = 0.4475s	
944/2700 (epoch 17.481), train_loss = 1.02320005, grad/param norm = 1.1653e-01, time/batch = 0.4679s	
945/2700 (epoch 17.500), train_loss = 1.02376760, grad/param norm = 1.3915e-01, time/batch = 0.4755s	
946/2700 (epoch 17.519), train_loss = 1.05435796, grad/param norm = 1.1492e-01, time/batch = 0.4014s	
947/2700 (epoch 17.537), train_loss = 1.01700743, grad/param norm = 1.1162e-01, time/batch = 0.4064s	
948/2700 (epoch 17.556), train_loss = 0.96976174, grad/param norm = 1.0060e-01, time/batch = 0.3803s	
949/2700 (epoch 17.574), train_loss = 0.95680238, grad/param norm = 1.0737e-01, time/batch = 0.4524s	
950/2700 (epoch 17.593), train_loss = 1.00341358, grad/param norm = 1.0633e-01, time/batch = 0.4390s	
951/2700 (epoch 17.611), train_loss = 0.95063731, grad/param norm = 1.0769e-01, time/batch = 0.4166s	
952/2700 (epoch 17.630), train_loss = 0.96580284, grad/param norm = 9.9023e-02, time/batch = 0.3922s	
953/2700 (epoch 17.648), train_loss = 0.97578828, grad/param norm = 1.1872e-01, time/batch = 0.4236s	
954/2700 (epoch 17.667), train_loss = 0.99379068, grad/param norm = 1.1826e-01, time/batch = 0.4566s	
955/2700 (epoch 17.685), train_loss = 1.03087906, grad/param norm = 1.2280e-01, time/batch = 0.4732s	
956/2700 (epoch 17.704), train_loss = 1.02489946, grad/param norm = 1.2532e-01, time/batch = 0.4853s	
957/2700 (epoch 17.722), train_loss = 1.00266319, grad/param norm = 1.2184e-01, time/batch = 0.4409s	
958/2700 (epoch 17.741), train_loss = 0.98389000, grad/param norm = 1.2596e-01, time/batch = 0.3796s	
959/2700 (epoch 17.759), train_loss = 0.93862353, grad/param norm = 1.0817e-01, time/batch = 0.3908s	
960/2700 (epoch 17.778), train_loss = 0.98402333, grad/param norm = 1.3017e-01, time/batch = 0.4424s	
961/2700 (epoch 17.796), train_loss = 0.96985198, grad/param norm = 1.3416e-01, time/batch = 0.4155s	
962/2700 (epoch 17.815), train_loss = 0.99785104, grad/param norm = 1.1272e-01, time/batch = 0.3921s	
963/2700 (epoch 17.833), train_loss = 1.01048978, grad/param norm = 1.4143e-01, time/batch = 0.4094s	
964/2700 (epoch 17.852), train_loss = 0.98940426, grad/param norm = 1.1681e-01, time/batch = 0.4495s	
965/2700 (epoch 17.870), train_loss = 0.96808105, grad/param norm = 1.0006e-01, time/batch = 0.4697s	
966/2700 (epoch 17.889), train_loss = 0.96332445, grad/param norm = 1.1862e-01, time/batch = 0.4777s	
967/2700 (epoch 17.907), train_loss = 1.00673365, grad/param norm = 1.2032e-01, time/batch = 0.4123s	
968/2700 (epoch 17.926), train_loss = 0.98862055, grad/param norm = 1.2405e-01, time/batch = 0.4000s	
969/2700 (epoch 17.944), train_loss = 1.00219963, grad/param norm = 1.3349e-01, time/batch = 0.3798s	
970/2700 (epoch 17.963), train_loss = 0.99012415, grad/param norm = 1.2281e-01, time/batch = 0.3937s	
971/2700 (epoch 17.981), train_loss = 0.93580034, grad/param norm = 1.1307e-01, time/batch = 0.4344s	
decayed learning rate by a factor 0.97 to 0.0015204621173091	
972/2700 (epoch 18.000), train_loss = 0.95452921, grad/param norm = 1.0770e-01, time/batch = 0.4169s	
973/2700 (epoch 18.019), train_loss = 1.04449218, grad/param norm = 1.2242e-01, time/batch = 0.3985s	
974/2700 (epoch 18.037), train_loss = 0.98882119, grad/param norm = 1.2129e-01, time/batch = 0.4263s	
975/2700 (epoch 18.056), train_loss = 0.99196292, grad/param norm = 1.3286e-01, time/batch = 0.4498s	
976/2700 (epoch 18.074), train_loss = 0.94718251, grad/param norm = 1.2052e-01, time/batch = 0.4729s	
977/2700 (epoch 18.093), train_loss = 0.93344942, grad/param norm = 1.1032e-01, time/batch = 0.4844s	
978/2700 (epoch 18.111), train_loss = 0.90919162, grad/param norm = 1.0256e-01, time/batch = 0.4780s	
979/2700 (epoch 18.130), train_loss = 0.95517617, grad/param norm = 1.2603e-01, time/batch = 0.4512s	
980/2700 (epoch 18.148), train_loss = 0.92699524, grad/param norm = 1.1819e-01, time/batch = 0.3964s	
981/2700 (epoch 18.167), train_loss = 0.94575197, grad/param norm = 1.0653e-01, time/batch = 0.3611s	
982/2700 (epoch 18.185), train_loss = 0.92287582, grad/param norm = 1.1398e-01, time/batch = 0.3574s	
983/2700 (epoch 18.204), train_loss = 0.95695406, grad/param norm = 1.1070e-01, time/batch = 0.4385s	
984/2700 (epoch 18.222), train_loss = 0.92003808, grad/param norm = 1.2521e-01, time/batch = 0.4146s	
985/2700 (epoch 18.241), train_loss = 0.91866839, grad/param norm = 1.3486e-01, time/batch = 0.3944s	
986/2700 (epoch 18.259), train_loss = 0.93234596, grad/param norm = 1.4016e-01, time/batch = 0.4319s	
987/2700 (epoch 18.278), train_loss = 0.98166824, grad/param norm = 1.4372e-01, time/batch = 0.4612s	
988/2700 (epoch 18.296), train_loss = 0.94174154, grad/param norm = 1.1234e-01, time/batch = 0.4780s	
989/2700 (epoch 18.315), train_loss = 0.88448160, grad/param norm = 1.1616e-01, time/batch = 0.4534s	
990/2700 (epoch 18.333), train_loss = 0.90979904, grad/param norm = 1.1912e-01, time/batch = 0.4452s	
991/2700 (epoch 18.352), train_loss = 0.90100200, grad/param norm = 1.2095e-01, time/batch = 0.4146s	
992/2700 (epoch 18.370), train_loss = 0.89098522, grad/param norm = 1.2440e-01, time/batch = 0.3901s	
993/2700 (epoch 18.389), train_loss = 0.95873700, grad/param norm = 1.3426e-01, time/batch = 0.3612s	
994/2700 (epoch 18.407), train_loss = 0.99702402, grad/param norm = 1.3078e-01, time/batch = 0.3337s	
995/2700 (epoch 18.426), train_loss = 0.96178946, grad/param norm = 1.1633e-01, time/batch = 0.4453s	
996/2700 (epoch 18.444), train_loss = 0.93466019, grad/param norm = 1.1685e-01, time/batch = 0.3848s	
997/2700 (epoch 18.463), train_loss = 0.93641734, grad/param norm = 1.0836e-01, time/batch = 0.4014s	
998/2700 (epoch 18.481), train_loss = 0.92372327, grad/param norm = 1.2921e-01, time/batch = 0.4398s	
999/2700 (epoch 18.500), train_loss = 0.89831198, grad/param norm = 1.1599e-01, time/batch = 0.4552s	
evaluating loss over split index 2	
1/3...	
2/3...	
3/3...	
saving checkpoint to cv/lm_lstm_epoch18.52_1.7673.t7	
1000/2700 (epoch 18.519), train_loss = 0.97900199, grad/param norm = 1.4197e-01, time/batch = 0.4652s	
1001/2700 (epoch 18.537), train_loss = 1.26536972, grad/param norm = 1.4075e-01, time/batch = 0.3900s	
1002/2700 (epoch 18.556), train_loss = 0.90284571, grad/param norm = 1.2259e-01, time/batch = 0.4181s	
1003/2700 (epoch 18.574), train_loss = 0.86948469, grad/param norm = 1.1058e-01, time/batch = 0.4181s	
1004/2700 (epoch 18.593), train_loss = 0.91193369, grad/param norm = 1.1365e-01, time/batch = 0.3974s	
1005/2700 (epoch 18.611), train_loss = 0.86881941, grad/param norm = 1.2048e-01, time/batch = 0.3521s	
1006/2700 (epoch 18.630), train_loss = 0.93119727, grad/param norm = 1.6269e-01, time/batch = 0.4208s	
1007/2700 (epoch 18.648), train_loss = 0.92727134, grad/param norm = 1.3082e-01, time/batch = 0.4422s	
1008/2700 (epoch 18.667), train_loss = 0.88769108, grad/param norm = 1.0963e-01, time/batch = 0.4072s	
1009/2700 (epoch 18.685), train_loss = 0.92785324, grad/param norm = 1.2018e-01, time/batch = 0.4275s	
1010/2700 (epoch 18.704), train_loss = 0.89932013, grad/param norm = 1.1160e-01, time/batch = 0.4245s	
1011/2700 (epoch 18.722), train_loss = 0.89361892, grad/param norm = 1.1770e-01, time/batch = 0.4293s	
1012/2700 (epoch 18.741), train_loss = 0.87163864, grad/param norm = 1.2359e-01, time/batch = 0.4047s	
1013/2700 (epoch 18.759), train_loss = 0.86351141, grad/param norm = 1.2490e-01, time/batch = 0.4100s	
1014/2700 (epoch 18.778), train_loss = 0.87015158, grad/param norm = 1.1642e-01, time/batch = 0.4192s	
1015/2700 (epoch 18.796), train_loss = 0.85279095, grad/param norm = 1.2543e-01, time/batch = 0.4355s	
1016/2700 (epoch 18.815), train_loss = 0.92189646, grad/param norm = 1.3428e-01, time/batch = 0.4156s	
1017/2700 (epoch 18.833), train_loss = 0.90275094, grad/param norm = 1.2915e-01, time/batch = 0.3931s	
1018/2700 (epoch 18.852), train_loss = 0.90923478, grad/param norm = 1.3648e-01, time/batch = 0.4534s	
1019/2700 (epoch 18.870), train_loss = 0.92674658, grad/param norm = 1.2421e-01, time/batch = 0.4659s	
1020/2700 (epoch 18.889), train_loss = 0.87047428, grad/param norm = 1.2087e-01, time/batch = 0.4401s	
1021/2700 (epoch 18.907), train_loss = 0.91362572, grad/param norm = 1.3514e-01, time/batch = 0.4308s	
1022/2700 (epoch 18.926), train_loss = 0.87307955, grad/param norm = 1.2131e-01, time/batch = 0.3839s	
1023/2700 (epoch 18.944), train_loss = 0.88751283, grad/param norm = 1.3880e-01, time/batch = 0.4012s	
1024/2700 (epoch 18.963), train_loss = 0.90129975, grad/param norm = 1.2818e-01, time/batch = 0.4086s	
1025/2700 (epoch 18.981), train_loss = 0.85704095, grad/param norm = 1.2541e-01, time/batch = 0.4312s	
decayed learning rate by a factor 0.97 to 0.0014748482537899	
1026/2700 (epoch 19.000), train_loss = 0.88286239, grad/param norm = 1.3261e-01, time/batch = 0.4281s	
1027/2700 (epoch 19.019), train_loss = 0.94677030, grad/param norm = 1.2774e-01, time/batch = 0.3965s	
1028/2700 (epoch 19.037), train_loss = 0.87989101, grad/param norm = 1.2031e-01, time/batch = 0.4185s	
1029/2700 (epoch 19.056), train_loss = 0.87464018, grad/param norm = 1.2119e-01, time/batch = 0.4415s	
1030/2700 (epoch 19.074), train_loss = 0.84096127, grad/param norm = 1.2404e-01, time/batch = 0.4749s	
1031/2700 (epoch 19.093), train_loss = 0.84362405, grad/param norm = 1.2202e-01, time/batch = 0.4602s	
1032/2700 (epoch 19.111), train_loss = 0.82108478, grad/param norm = 1.0773e-01, time/batch = 0.4328s	
1033/2700 (epoch 19.130), train_loss = 0.85029315, grad/param norm = 1.2324e-01, time/batch = 0.4120s	
1034/2700 (epoch 19.148), train_loss = 0.82902762, grad/param norm = 1.1719e-01, time/batch = 0.4038s	
1035/2700 (epoch 19.167), train_loss = 0.85967407, grad/param norm = 1.2899e-01, time/batch = 0.4121s	
1036/2700 (epoch 19.185), train_loss = 0.86415695, grad/param norm = 1.4216e-01, time/batch = 0.4231s	
1037/2700 (epoch 19.204), train_loss = 0.86269194, grad/param norm = 1.1958e-01, time/batch = 0.4354s	
1038/2700 (epoch 19.222), train_loss = 0.82580202, grad/param norm = 1.2470e-01, time/batch = 0.4129s	
1039/2700 (epoch 19.241), train_loss = 0.82676216, grad/param norm = 1.4078e-01, time/batch = 0.3902s	
1040/2700 (epoch 19.259), train_loss = 0.85361493, grad/param norm = 1.4558e-01, time/batch = 0.4250s	
1041/2700 (epoch 19.278), train_loss = 0.88241933, grad/param norm = 1.3962e-01, time/batch = 0.4188s	
1042/2700 (epoch 19.296), train_loss = 0.83652316, grad/param norm = 1.2203e-01, time/batch = 0.4573s	
1043/2700 (epoch 19.315), train_loss = 0.79587466, grad/param norm = 1.1772e-01, time/batch = 0.4330s	
1044/2700 (epoch 19.333), train_loss = 0.79993897, grad/param norm = 1.1732e-01, time/batch = 0.4238s	
1045/2700 (epoch 19.352), train_loss = 0.80678003, grad/param norm = 1.2985e-01, time/batch = 0.4120s	
1046/2700 (epoch 19.370), train_loss = 0.77791414, grad/param norm = 1.2259e-01, time/batch = 0.4067s	
1047/2700 (epoch 19.389), train_loss = 0.84563989, grad/param norm = 1.4162e-01, time/batch = 0.4217s	
1048/2700 (epoch 19.407), train_loss = 0.88600618, grad/param norm = 1.2927e-01, time/batch = 0.4378s	
1049/2700 (epoch 19.426), train_loss = 0.87610139, grad/param norm = 1.4614e-01, time/batch = 0.4230s	
1050/2700 (epoch 19.444), train_loss = 0.86444445, grad/param norm = 1.2326e-01, time/batch = 0.3977s	
1051/2700 (epoch 19.463), train_loss = 0.83838867, grad/param norm = 1.1723e-01, time/batch = 0.3863s	
1052/2700 (epoch 19.481), train_loss = 0.81655293, grad/param norm = 1.2591e-01, time/batch = 0.4320s	
1053/2700 (epoch 19.500), train_loss = 0.80223558, grad/param norm = 1.3175e-01, time/batch = 0.4463s	
1054/2700 (epoch 19.519), train_loss = 0.85334041, grad/param norm = 1.1520e-01, time/batch = 0.4566s	
1055/2700 (epoch 19.537), train_loss = 0.85248057, grad/param norm = 1.2126e-01, time/batch = 0.4241s	
1056/2700 (epoch 19.556), train_loss = 0.77649444, grad/param norm = 1.0646e-01, time/batch = 0.4359s	
1057/2700 (epoch 19.574), train_loss = 0.77322763, grad/param norm = 1.1893e-01, time/batch = 0.4174s	
1058/2700 (epoch 19.593), train_loss = 0.83760969, grad/param norm = 1.2679e-01, time/batch = 0.4129s	
1059/2700 (epoch 19.611), train_loss = 0.77332384, grad/param norm = 1.3130e-01, time/batch = 0.4222s	
1060/2700 (epoch 19.630), train_loss = 0.80791862, grad/param norm = 1.2474e-01, time/batch = 0.4407s	
1061/2700 (epoch 19.648), train_loss = 0.80222887, grad/param norm = 1.3520e-01, time/batch = 0.4025s	
1062/2700 (epoch 19.667), train_loss = 0.82361583, grad/param norm = 1.2612e-01, time/batch = 0.3680s	
1063/2700 (epoch 19.685), train_loss = 0.82909674, grad/param norm = 1.2811e-01, time/batch = 0.4124s	
1064/2700 (epoch 19.704), train_loss = 0.81350150, grad/param norm = 1.2092e-01, time/batch = 0.4488s	
1065/2700 (epoch 19.722), train_loss = 0.79168086, grad/param norm = 1.1647e-01, time/batch = 0.4518s	
1066/2700 (epoch 19.741), train_loss = 0.74999473, grad/param norm = 1.1066e-01, time/batch = 0.4487s	
1067/2700 (epoch 19.759), train_loss = 0.76534678, grad/param norm = 1.2574e-01, time/batch = 0.4207s	
1068/2700 (epoch 19.778), train_loss = 0.78922178, grad/param norm = 1.3275e-01, time/batch = 0.4036s	
1069/2700 (epoch 19.796), train_loss = 0.76838559, grad/param norm = 1.4273e-01, time/batch = 0.4115s	
1070/2700 (epoch 19.815), train_loss = 0.83205443, grad/param norm = 1.4023e-01, time/batch = 0.4198s	
1071/2700 (epoch 19.833), train_loss = 0.78620127, grad/param norm = 1.2310e-01, time/batch = 0.4325s	
1072/2700 (epoch 19.852), train_loss = 0.78927771, grad/param norm = 1.2756e-01, time/batch = 0.4197s	
1073/2700 (epoch 19.870), train_loss = 0.80250866, grad/param norm = 1.1830e-01, time/batch = 0.3988s	
1074/2700 (epoch 19.889), train_loss = 0.75402770, grad/param norm = 1.0872e-01, time/batch = 0.3897s	
1075/2700 (epoch 19.907), train_loss = 0.81990684, grad/param norm = 1.5394e-01, time/batch = 0.4232s	
1076/2700 (epoch 19.926), train_loss = 0.80830647, grad/param norm = 1.3558e-01, time/batch = 0.4553s	
1077/2700 (epoch 19.944), train_loss = 0.79213671, grad/param norm = 1.4249e-01, time/batch = 0.4557s	
1078/2700 (epoch 19.963), train_loss = 0.80161731, grad/param norm = 1.1868e-01, time/batch = 0.4307s	
1079/2700 (epoch 19.981), train_loss = 0.75173874, grad/param norm = 1.2109e-01, time/batch = 0.4181s	
decayed learning rate by a factor 0.97 to 0.0014306028061762	
1080/2700 (epoch 20.000), train_loss = 0.76850039, grad/param norm = 1.2727e-01, time/batch = 0.4309s	
1081/2700 (epoch 20.019), train_loss = 0.84830382, grad/param norm = 1.3411e-01, time/batch = 0.4401s	
1082/2700 (epoch 20.037), train_loss = 0.80924702, grad/param norm = 1.4226e-01, time/batch = 0.4515s	
1083/2700 (epoch 20.056), train_loss = 0.80303787, grad/param norm = 1.3419e-01, time/batch = 0.4366s	
1084/2700 (epoch 20.074), train_loss = 0.74203747, grad/param norm = 1.2241e-01, time/batch = 0.4039s	
1085/2700 (epoch 20.093), train_loss = 0.74718098, grad/param norm = 1.1547e-01, time/batch = 0.3695s	
1086/2700 (epoch 20.111), train_loss = 0.72454424, grad/param norm = 1.0937e-01, time/batch = 0.3914s	
1087/2700 (epoch 20.130), train_loss = 0.73450953, grad/param norm = 1.1233e-01, time/batch = 0.4476s	
1088/2700 (epoch 20.148), train_loss = 0.73861504, grad/param norm = 1.2499e-01, time/batch = 0.4508s	
1089/2700 (epoch 20.167), train_loss = 0.76050350, grad/param norm = 1.2760e-01, time/batch = 0.4602s	
1090/2700 (epoch 20.185), train_loss = 0.75505329, grad/param norm = 1.3374e-01, time/batch = 0.3746s	
1091/2700 (epoch 20.204), train_loss = 0.79846019, grad/param norm = 1.4603e-01, time/batch = 0.4336s	
1092/2700 (epoch 20.222), train_loss = 0.73736501, grad/param norm = 1.3265e-01, time/batch = 0.4572s	
1093/2700 (epoch 20.241), train_loss = 0.73206080, grad/param norm = 1.3039e-01, time/batch = 0.4691s	
1094/2700 (epoch 20.259), train_loss = 0.73597801, grad/param norm = 1.2913e-01, time/batch = 0.4402s	
1095/2700 (epoch 20.278), train_loss = 0.78415271, grad/param norm = 1.3352e-01, time/batch = 0.4124s	
1096/2700 (epoch 20.296), train_loss = 0.76936383, grad/param norm = 1.4058e-01, time/batch = 0.3822s	
1097/2700 (epoch 20.315), train_loss = 0.70046992, grad/param norm = 1.2286e-01, time/batch = 0.4002s	
1098/2700 (epoch 20.333), train_loss = 0.70114532, grad/param norm = 1.1437e-01, time/batch = 0.4262s	
1099/2700 (epoch 20.352), train_loss = 0.68415211, grad/param norm = 1.1370e-01, time/batch = 0.4322s	
1100/2700 (epoch 20.370), train_loss = 0.68232202, grad/param norm = 1.3072e-01, time/batch = 0.3683s	
1101/2700 (epoch 20.389), train_loss = 0.74683506, grad/param norm = 1.5334e-01, time/batch = 0.3355s	
1102/2700 (epoch 20.407), train_loss = 0.81625167, grad/param norm = 1.4324e-01, time/batch = 0.3773s	
1103/2700 (epoch 20.426), train_loss = 0.76687758, grad/param norm = 1.2944e-01, time/batch = 0.4357s	
1104/2700 (epoch 20.444), train_loss = 0.77395357, grad/param norm = 1.3363e-01, time/batch = 0.4276s	
1105/2700 (epoch 20.463), train_loss = 0.77349292, grad/param norm = 1.3964e-01, time/batch = 0.3938s	
1106/2700 (epoch 20.481), train_loss = 0.71098026, grad/param norm = 1.1738e-01, time/batch = 0.3681s	
1107/2700 (epoch 20.500), train_loss = 0.69129001, grad/param norm = 1.2259e-01, time/batch = 0.4200s	
1108/2700 (epoch 20.519), train_loss = 0.76850692, grad/param norm = 1.4144e-01, time/batch = 0.4575s	
1109/2700 (epoch 20.537), train_loss = 0.75168739, grad/param norm = 1.1963e-01, time/batch = 0.4661s	
1110/2700 (epoch 20.556), train_loss = 0.69037584, grad/param norm = 1.2459e-01, time/batch = 0.4554s	
1111/2700 (epoch 20.574), train_loss = 0.69281351, grad/param norm = 1.1745e-01, time/batch = 0.4441s	
1112/2700 (epoch 20.593), train_loss = 0.72423428, grad/param norm = 1.2149e-01, time/batch = 0.3804s	
1113/2700 (epoch 20.611), train_loss = 0.70404300, grad/param norm = 1.3751e-01, time/batch = 0.4120s	
1114/2700 (epoch 20.630), train_loss = 0.75354346, grad/param norm = 1.6731e-01, time/batch = 0.4121s	
1115/2700 (epoch 20.648), train_loss = 0.75456941, grad/param norm = 1.4330e-01, time/batch = 0.4366s	
1116/2700 (epoch 20.667), train_loss = 0.73637328, grad/param norm = 1.3458e-01, time/batch = 0.3953s	
1117/2700 (epoch 20.685), train_loss = 0.73353768, grad/param norm = 1.1883e-01, time/batch = 0.3743s	
1118/2700 (epoch 20.704), train_loss = 0.74021901, grad/param norm = 1.5021e-01, time/batch = 0.4160s	
1119/2700 (epoch 20.722), train_loss = 0.72073619, grad/param norm = 1.2366e-01, time/batch = 0.4558s	
1120/2700 (epoch 20.741), train_loss = 0.66941831, grad/param norm = 1.2195e-01, time/batch = 0.4646s	
1121/2700 (epoch 20.759), train_loss = 0.66053517, grad/param norm = 1.1751e-01, time/batch = 0.4520s	
1122/2700 (epoch 20.778), train_loss = 0.66603079, grad/param norm = 1.1353e-01, time/batch = 0.4311s	
1123/2700 (epoch 20.796), train_loss = 0.65683737, grad/param norm = 1.2306e-01, time/batch = 0.4059s	
1124/2700 (epoch 20.815), train_loss = 0.76552096, grad/param norm = 2.0173e-01, time/batch = 0.4034s	
1125/2700 (epoch 20.833), train_loss = 0.74634646, grad/param norm = 1.4259e-01, time/batch = 0.4191s	
1126/2700 (epoch 20.852), train_loss = 0.69585056, grad/param norm = 1.2851e-01, time/batch = 0.4179s	
1127/2700 (epoch 20.870), train_loss = 0.73501623, grad/param norm = 1.3714e-01, time/batch = 0.4010s	
1128/2700 (epoch 20.889), train_loss = 0.69276876, grad/param norm = 1.3122e-01, time/batch = 0.3797s	
1129/2700 (epoch 20.907), train_loss = 0.72426305, grad/param norm = 1.4448e-01, time/batch = 0.4219s	
1130/2700 (epoch 20.926), train_loss = 0.70970343, grad/param norm = 1.4630e-01, time/batch = 0.4552s	
1131/2700 (epoch 20.944), train_loss = 0.68545958, grad/param norm = 1.2048e-01, time/batch = 0.4443s	
1132/2700 (epoch 20.963), train_loss = 0.73840995, grad/param norm = 1.5418e-01, time/batch = 0.4520s	
1133/2700 (epoch 20.981), train_loss = 0.68806493, grad/param norm = 1.3049e-01, time/batch = 0.4391s	
decayed learning rate by a factor 0.97 to 0.0013876847219909	
1134/2700 (epoch 21.000), train_loss = 0.67000564, grad/param norm = 1.1991e-01, time/batch = 0.4104s	
1135/2700 (epoch 21.019), train_loss = 0.75515017, grad/param norm = 1.3435e-01, time/batch = 0.4228s	
1136/2700 (epoch 21.037), train_loss = 0.68936518, grad/param norm = 1.3233e-01, time/batch = 0.4194s	
1137/2700 (epoch 21.056), train_loss = 0.69714724, grad/param norm = 1.3199e-01, time/batch = 0.4217s	
1138/2700 (epoch 21.074), train_loss = 0.67524755, grad/param norm = 1.4576e-01, time/batch = 0.4359s	
1139/2700 (epoch 21.093), train_loss = 0.67548720, grad/param norm = 1.1988e-01, time/batch = 0.3695s	
1140/2700 (epoch 21.111), train_loss = 0.65337218, grad/param norm = 1.1940e-01, time/batch = 0.3983s	
1141/2700 (epoch 21.130), train_loss = 0.65642901, grad/param norm = 1.2451e-01, time/batch = 0.4072s	
1142/2700 (epoch 21.148), train_loss = 0.63828226, grad/param norm = 1.1755e-01, time/batch = 0.4461s	
1143/2700 (epoch 21.167), train_loss = 0.65275925, grad/param norm = 1.1815e-01, time/batch = 0.4610s	
1144/2700 (epoch 21.185), train_loss = 0.65675370, grad/param norm = 1.3302e-01, time/batch = 0.4254s	
1145/2700 (epoch 21.204), train_loss = 0.70639765, grad/param norm = 1.5138e-01, time/batch = 0.4083s	
1146/2700 (epoch 21.222), train_loss = 0.64193797, grad/param norm = 1.2717e-01, time/batch = 0.4122s	
1147/2700 (epoch 21.241), train_loss = 0.65609667, grad/param norm = 1.4559e-01, time/batch = 0.4175s	
1148/2700 (epoch 21.259), train_loss = 0.64252724, grad/param norm = 1.2218e-01, time/batch = 0.4313s	
1149/2700 (epoch 21.278), train_loss = 0.68448617, grad/param norm = 1.2749e-01, time/batch = 0.4275s	
1150/2700 (epoch 21.296), train_loss = 0.65897557, grad/param norm = 1.2443e-01, time/batch = 0.3867s	
1151/2700 (epoch 21.315), train_loss = 0.62572102, grad/param norm = 1.4485e-01, time/batch = 0.3479s	
1152/2700 (epoch 21.333), train_loss = 0.64450621, grad/param norm = 1.3624e-01, time/batch = 0.4350s	
1153/2700 (epoch 21.352), train_loss = 0.60903991, grad/param norm = 1.2262e-01, time/batch = 0.4501s	
1154/2700 (epoch 21.370), train_loss = 0.57992225, grad/param norm = 1.2227e-01, time/batch = 0.4264s	
1155/2700 (epoch 21.389), train_loss = 0.61796690, grad/param norm = 1.1521e-01, time/batch = 0.4085s	
1156/2700 (epoch 21.407), train_loss = 0.70496728, grad/param norm = 1.5163e-01, time/batch = 0.4218s	
1157/2700 (epoch 21.426), train_loss = 0.68693674, grad/param norm = 1.3801e-01, time/batch = 0.4171s	
1158/2700 (epoch 21.444), train_loss = 0.66978096, grad/param norm = 1.3284e-01, time/batch = 0.4370s	
1159/2700 (epoch 21.463), train_loss = 0.67518374, grad/param norm = 1.3528e-01, time/batch = 0.4426s	
1160/2700 (epoch 21.481), train_loss = 0.63300556, grad/param norm = 1.3371e-01, time/batch = 0.4079s	
1161/2700 (epoch 21.500), train_loss = 0.60004271, grad/param norm = 1.2344e-01, time/batch = 0.3908s	
1162/2700 (epoch 21.519), train_loss = 0.65640345, grad/param norm = 1.2430e-01, time/batch = 0.3650s	
1163/2700 (epoch 21.537), train_loss = 0.66362311, grad/param norm = 1.3594e-01, time/batch = 0.3955s	
1164/2700 (epoch 21.556), train_loss = 0.61960482, grad/param norm = 1.3767e-01, time/batch = 0.4454s	
1165/2700 (epoch 21.574), train_loss = 0.60523932, grad/param norm = 1.2470e-01, time/batch = 0.4271s	
1166/2700 (epoch 21.593), train_loss = 0.63232406, grad/param norm = 1.1940e-01, time/batch = 0.3982s	
1167/2700 (epoch 21.611), train_loss = 0.58662002, grad/param norm = 1.1677e-01, time/batch = 0.4091s	
1168/2700 (epoch 21.630), train_loss = 0.63297332, grad/param norm = 1.2412e-01, time/batch = 0.4294s	
1169/2700 (epoch 21.648), train_loss = 0.67244583, grad/param norm = 1.6993e-01, time/batch = 0.4464s	
1170/2700 (epoch 21.667), train_loss = 0.67994110, grad/param norm = 1.4495e-01, time/batch = 0.4667s	
1171/2700 (epoch 21.685), train_loss = 0.63643835, grad/param norm = 1.1757e-01, time/batch = 0.4600s	
1172/2700 (epoch 21.704), train_loss = 0.63635962, grad/param norm = 1.2516e-01, time/batch = 0.4175s	
1173/2700 (epoch 21.722), train_loss = 0.63415958, grad/param norm = 1.3304e-01, time/batch = 0.4111s	
1174/2700 (epoch 21.741), train_loss = 0.59763058, grad/param norm = 1.2299e-01, time/batch = 0.3975s	
1175/2700 (epoch 21.759), train_loss = 0.57932387, grad/param norm = 1.3516e-01, time/batch = 0.3934s	
1176/2700 (epoch 21.778), train_loss = 0.58680365, grad/param norm = 1.1972e-01, time/batch = 0.4192s	
1177/2700 (epoch 21.796), train_loss = 0.56203103, grad/param norm = 1.1954e-01, time/batch = 0.3848s	
1178/2700 (epoch 21.815), train_loss = 0.63884158, grad/param norm = 1.4698e-01, time/batch = 0.3987s	
1179/2700 (epoch 21.833), train_loss = 0.66169001, grad/param norm = 1.6466e-01, time/batch = 0.4515s	
1180/2700 (epoch 21.852), train_loss = 0.63359427, grad/param norm = 1.3631e-01, time/batch = 0.4662s	
1181/2700 (epoch 21.870), train_loss = 0.65498815, grad/param norm = 1.4346e-01, time/batch = 0.4668s	
1182/2700 (epoch 21.889), train_loss = 0.60004068, grad/param norm = 1.2388e-01, time/batch = 0.4562s	
1183/2700 (epoch 21.907), train_loss = 0.62177608, grad/param norm = 1.3696e-01, time/batch = 0.4293s	
1184/2700 (epoch 21.926), train_loss = 0.64655697, grad/param norm = 1.6625e-01, time/batch = 0.4123s	
1185/2700 (epoch 21.944), train_loss = 0.60669133, grad/param norm = 1.3097e-01, time/batch = 0.3930s	
1186/2700 (epoch 21.963), train_loss = 0.62357484, grad/param norm = 1.2592e-01, time/batch = 0.4101s	
1187/2700 (epoch 21.981), train_loss = 0.59268815, grad/param norm = 1.1751e-01, time/batch = 0.4226s	
decayed learning rate by a factor 0.97 to 0.0013460541803311	
1188/2700 (epoch 22.000), train_loss = 0.58992199, grad/param norm = 1.2488e-01, time/batch = 0.3771s	
1189/2700 (epoch 22.019), train_loss = 0.64357623, grad/param norm = 1.1799e-01, time/batch = 0.4079s	
1190/2700 (epoch 22.037), train_loss = 0.62022835, grad/param norm = 1.4308e-01, time/batch = 0.4537s	
1191/2700 (epoch 22.056), train_loss = 0.62580988, grad/param norm = 1.4895e-01, time/batch = 0.4540s	
1192/2700 (epoch 22.074), train_loss = 0.57093393, grad/param norm = 1.3305e-01, time/batch = 0.4568s	
1193/2700 (epoch 22.093), train_loss = 0.58206973, grad/param norm = 1.2124e-01, time/batch = 0.4367s	
1194/2700 (epoch 22.111), train_loss = 0.57786602, grad/param norm = 1.2835e-01, time/batch = 0.4156s	
1195/2700 (epoch 22.130), train_loss = 0.55804473, grad/param norm = 1.1302e-01, time/batch = 0.3930s	
1196/2700 (epoch 22.148), train_loss = 0.55910328, grad/param norm = 1.1703e-01, time/batch = 0.4014s	
1197/2700 (epoch 22.167), train_loss = 0.59682115, grad/param norm = 1.4383e-01, time/batch = 0.4209s	
1198/2700 (epoch 22.185), train_loss = 0.58888899, grad/param norm = 1.3447e-01, time/batch = 0.4324s	
1199/2700 (epoch 22.204), train_loss = 0.61253966, grad/param norm = 1.4171e-01, time/batch = 0.3965s	
1200/2700 (epoch 22.222), train_loss = 0.56541799, grad/param norm = 1.3771e-01, time/batch = 0.3920s	
1201/2700 (epoch 22.241), train_loss = 0.58418487, grad/param norm = 1.4651e-01, time/batch = 0.4527s	
1202/2700 (epoch 22.259), train_loss = 0.57475264, grad/param norm = 1.4032e-01, time/batch = 0.4593s	
1203/2700 (epoch 22.278), train_loss = 0.60026339, grad/param norm = 1.4236e-01, time/batch = 0.4492s	
1204/2700 (epoch 22.296), train_loss = 0.58261862, grad/param norm = 1.2481e-01, time/batch = 0.4162s	
1205/2700 (epoch 22.315), train_loss = 0.52007170, grad/param norm = 1.1395e-01, time/batch = 0.4026s	
1206/2700 (epoch 22.333), train_loss = 0.56568677, grad/param norm = 1.5131e-01, time/batch = 0.4108s	
1207/2700 (epoch 22.352), train_loss = 0.58050571, grad/param norm = 1.5310e-01, time/batch = 0.4237s	
1208/2700 (epoch 22.370), train_loss = 0.51303150, grad/param norm = 1.2587e-01, time/batch = 0.4351s	
1209/2700 (epoch 22.389), train_loss = 0.56278786, grad/param norm = 1.4048e-01, time/batch = 0.4035s	
1210/2700 (epoch 22.407), train_loss = 0.63131359, grad/param norm = 1.5571e-01, time/batch = 0.3912s	
1211/2700 (epoch 22.426), train_loss = 0.59206279, grad/param norm = 1.3147e-01, time/batch = 0.3841s	
1212/2700 (epoch 22.444), train_loss = 0.58220785, grad/param norm = 1.2892e-01, time/batch = 0.4273s	
1213/2700 (epoch 22.463), train_loss = 0.59224783, grad/param norm = 1.3225e-01, time/batch = 0.4713s	
1214/2700 (epoch 22.481), train_loss = 0.56605498, grad/param norm = 1.3671e-01, time/batch = 0.4610s	
1215/2700 (epoch 22.500), train_loss = 0.55340200, grad/param norm = 1.5233e-01, time/batch = 0.4300s	
1216/2700 (epoch 22.519), train_loss = 0.57482504, grad/param norm = 1.3791e-01, time/batch = 0.4077s	
1217/2700 (epoch 22.537), train_loss = 0.56819366, grad/param norm = 1.2770e-01, time/batch = 0.4066s	
1218/2700 (epoch 22.556), train_loss = 0.52245756, grad/param norm = 1.3017e-01, time/batch = 0.4203s	
1219/2700 (epoch 22.574), train_loss = 0.53013319, grad/param norm = 1.2772e-01, time/batch = 0.4276s	
1220/2700 (epoch 22.593), train_loss = 0.55650271, grad/param norm = 1.2493e-01, time/batch = 0.4085s	
1221/2700 (epoch 22.611), train_loss = 0.51810145, grad/param norm = 1.1705e-01, time/batch = 0.3754s	
1222/2700 (epoch 22.630), train_loss = 0.53003342, grad/param norm = 1.1482e-01, time/batch = 0.3842s	
1223/2700 (epoch 22.648), train_loss = 0.55113615, grad/param norm = 1.2792e-01, time/batch = 0.4456s	
1224/2700 (epoch 22.667), train_loss = 0.56005883, grad/param norm = 1.3618e-01, time/batch = 0.4606s	
1225/2700 (epoch 22.685), train_loss = 0.58033028, grad/param norm = 1.3757e-01, time/batch = 0.4080s	
1226/2700 (epoch 22.704), train_loss = 0.56896107, grad/param norm = 1.4897e-01, time/batch = 0.3946s	
1227/2700 (epoch 22.722), train_loss = 0.57529837, grad/param norm = 1.3301e-01, time/batch = 0.3721s	
1228/2700 (epoch 22.741), train_loss = 0.51815851, grad/param norm = 1.3120e-01, time/batch = 0.4565s	
1229/2700 (epoch 22.759), train_loss = 0.51967944, grad/param norm = 1.2620e-01, time/batch = 0.4509s	
1230/2700 (epoch 22.778), train_loss = 0.52114989, grad/param norm = 1.2689e-01, time/batch = 0.4184s	
1231/2700 (epoch 22.796), train_loss = 0.51359682, grad/param norm = 1.3688e-01, time/batch = 0.4436s	
1232/2700 (epoch 22.815), train_loss = 0.53623941, grad/param norm = 1.3839e-01, time/batch = 0.3826s	
1233/2700 (epoch 22.833), train_loss = 0.58197829, grad/param norm = 1.5636e-01, time/batch = 0.3418s	
1234/2700 (epoch 22.852), train_loss = 0.55356642, grad/param norm = 1.3789e-01, time/batch = 0.4335s	
1235/2700 (epoch 22.870), train_loss = 0.55773438, grad/param norm = 1.2171e-01, time/batch = 0.4537s	
1236/2700 (epoch 22.889), train_loss = 0.53876095, grad/param norm = 1.3743e-01, time/batch = 0.4418s	
1237/2700 (epoch 22.907), train_loss = 0.54639158, grad/param norm = 1.3512e-01, time/batch = 0.4102s	
1238/2700 (epoch 22.926), train_loss = 0.51347338, grad/param norm = 1.2095e-01, time/batch = 0.4189s	
1239/2700 (epoch 22.944), train_loss = 0.52545705, grad/param norm = 1.3832e-01, time/batch = 0.4303s	
1240/2700 (epoch 22.963), train_loss = 0.57816200, grad/param norm = 1.5815e-01, time/batch = 0.4461s	
1241/2700 (epoch 22.981), train_loss = 0.53751076, grad/param norm = 1.3431e-01, time/batch = 0.4580s	
decayed learning rate by a factor 0.97 to 0.0013056725549212	
1242/2700 (epoch 23.000), train_loss = 0.51892786, grad/param norm = 1.2455e-01, time/batch = 0.4197s	
1243/2700 (epoch 23.019), train_loss = 0.57334052, grad/param norm = 1.2472e-01, time/batch = 0.3882s	
1244/2700 (epoch 23.037), train_loss = 0.51091835, grad/param norm = 1.2059e-01, time/batch = 0.3564s	
1245/2700 (epoch 23.056), train_loss = 0.53560819, grad/param norm = 1.2615e-01, time/batch = 0.4405s	
1246/2700 (epoch 23.074), train_loss = 0.51282468, grad/param norm = 1.3836e-01, time/batch = 0.4552s	
1247/2700 (epoch 23.093), train_loss = 0.50533648, grad/param norm = 1.1735e-01, time/batch = 0.4251s	
1248/2700 (epoch 23.111), train_loss = 0.48525028, grad/param norm = 1.1684e-01, time/batch = 0.4097s	
1249/2700 (epoch 23.130), train_loss = 0.50105129, grad/param norm = 1.2932e-01, time/batch = 0.4334s	
1250/2700 (epoch 23.148), train_loss = 0.48811232, grad/param norm = 1.2681e-01, time/batch = 0.4433s	
1251/2700 (epoch 23.167), train_loss = 0.50362956, grad/param norm = 1.3064e-01, time/batch = 0.4438s	
1252/2700 (epoch 23.185), train_loss = 0.51153024, grad/param norm = 1.4347e-01, time/batch = 0.4446s	
1253/2700 (epoch 23.204), train_loss = 0.55611522, grad/param norm = 1.5110e-01, time/batch = 0.4247s	
1254/2700 (epoch 23.222), train_loss = 0.52131992, grad/param norm = 1.5853e-01, time/batch = 0.3784s	
1255/2700 (epoch 23.241), train_loss = 0.53709989, grad/param norm = 1.4115e-01, time/batch = 0.3664s	
1256/2700 (epoch 23.259), train_loss = 0.50774920, grad/param norm = 1.3072e-01, time/batch = 0.4495s	
1257/2700 (epoch 23.278), train_loss = 0.53757665, grad/param norm = 1.3326e-01, time/batch = 0.4539s	
1258/2700 (epoch 23.296), train_loss = 0.49860012, grad/param norm = 1.2976e-01, time/batch = 0.4172s	
1259/2700 (epoch 23.315), train_loss = 0.46912908, grad/param norm = 1.1907e-01, time/batch = 0.4132s	
1260/2700 (epoch 23.333), train_loss = 0.47033465, grad/param norm = 1.2504e-01, time/batch = 0.4372s	
1261/2700 (epoch 23.352), train_loss = 0.46689892, grad/param norm = 1.2394e-01, time/batch = 0.4398s	
1262/2700 (epoch 23.370), train_loss = 0.47605578, grad/param norm = 1.3542e-01, time/batch = 0.4568s	
1263/2700 (epoch 23.389), train_loss = 0.48046411, grad/param norm = 1.3500e-01, time/batch = 0.4702s	
1264/2700 (epoch 23.407), train_loss = 0.52263994, grad/param norm = 1.2916e-01, time/batch = 0.3749s	
1265/2700 (epoch 23.426), train_loss = 0.53107258, grad/param norm = 1.5076e-01, time/batch = 0.3922s	
1266/2700 (epoch 23.444), train_loss = 0.52572140, grad/param norm = 1.3226e-01, time/batch = 0.3957s	
1267/2700 (epoch 23.463), train_loss = 0.50230467, grad/param norm = 1.1901e-01, time/batch = 0.4532s	
1268/2700 (epoch 23.481), train_loss = 0.47988454, grad/param norm = 1.2434e-01, time/batch = 0.4260s	
1269/2700 (epoch 23.500), train_loss = 0.44962758, grad/param norm = 1.1579e-01, time/batch = 0.4005s	
1270/2700 (epoch 23.519), train_loss = 0.49219668, grad/param norm = 1.3136e-01, time/batch = 0.4352s	
1271/2700 (epoch 23.537), train_loss = 0.50493740, grad/param norm = 1.3797e-01, time/batch = 0.4361s	
1272/2700 (epoch 23.556), train_loss = 0.44855162, grad/param norm = 1.1811e-01, time/batch = 0.4570s	
1273/2700 (epoch 23.574), train_loss = 0.46165802, grad/param norm = 1.3187e-01, time/batch = 0.4741s	
1274/2700 (epoch 23.593), train_loss = 0.48544035, grad/param norm = 1.2923e-01, time/batch = 0.4592s	
1275/2700 (epoch 23.611), train_loss = 0.46328929, grad/param norm = 1.3277e-01, time/batch = 0.3885s	
1276/2700 (epoch 23.630), train_loss = 0.48427572, grad/param norm = 1.3382e-01, time/batch = 0.3555s	
1277/2700 (epoch 23.648), train_loss = 0.48152896, grad/param norm = 1.2723e-01, time/batch = 0.4025s	
1278/2700 (epoch 23.667), train_loss = 0.48760527, grad/param norm = 1.4125e-01, time/batch = 0.4448s	
1279/2700 (epoch 23.685), train_loss = 0.48731174, grad/param norm = 1.2328e-01, time/batch = 0.4226s	
1280/2700 (epoch 23.704), train_loss = 0.48957331, grad/param norm = 1.3678e-01, time/batch = 0.4038s	
1281/2700 (epoch 23.722), train_loss = 0.50068608, grad/param norm = 1.4269e-01, time/batch = 0.4116s	
1282/2700 (epoch 23.741), train_loss = 0.47466487, grad/param norm = 1.2861e-01, time/batch = 0.4437s	
1283/2700 (epoch 23.759), train_loss = 0.44554631, grad/param norm = 1.2546e-01, time/batch = 0.4653s	
1284/2700 (epoch 23.778), train_loss = 0.45710895, grad/param norm = 1.2505e-01, time/batch = 0.4749s	
1285/2700 (epoch 23.796), train_loss = 0.42739724, grad/param norm = 1.2078e-01, time/batch = 0.4263s	
1286/2700 (epoch 23.815), train_loss = 0.46702007, grad/param norm = 1.2901e-01, time/batch = 0.3859s	
1287/2700 (epoch 23.833), train_loss = 0.48343350, grad/param norm = 1.3667e-01, time/batch = 0.3751s	
1288/2700 (epoch 23.852), train_loss = 0.44615474, grad/param norm = 1.2137e-01, time/batch = 0.3902s	
1289/2700 (epoch 23.870), train_loss = 0.48740014, grad/param norm = 1.3661e-01, time/batch = 0.4272s	
1290/2700 (epoch 23.889), train_loss = 0.46603559, grad/param norm = 1.3125e-01, time/batch = 0.4014s	
1291/2700 (epoch 23.907), train_loss = 0.45585297, grad/param norm = 1.1820e-01, time/batch = 0.3961s	
1292/2700 (epoch 23.926), train_loss = 0.45377397, grad/param norm = 1.2415e-01, time/batch = 0.4295s	
1293/2700 (epoch 23.944), train_loss = 0.42431570, grad/param norm = 1.1471e-01, time/batch = 0.4616s	
1294/2700 (epoch 23.963), train_loss = 0.45530825, grad/param norm = 1.0636e-01, time/batch = 0.4739s	
1295/2700 (epoch 23.981), train_loss = 0.45772299, grad/param norm = 1.3437e-01, time/batch = 0.4803s	
decayed learning rate by a factor 0.97 to 0.0012665023782736	
1296/2700 (epoch 24.000), train_loss = 0.46837327, grad/param norm = 1.4296e-01, time/batch = 0.4499s	
1297/2700 (epoch 24.019), train_loss = 0.51825229, grad/param norm = 1.2987e-01, time/batch = 0.3902s	
1298/2700 (epoch 24.037), train_loss = 0.45202849, grad/param norm = 1.2597e-01, time/batch = 0.3843s	
1299/2700 (epoch 24.056), train_loss = 0.45717157, grad/param norm = 1.2430e-01, time/batch = 0.3949s	
1300/2700 (epoch 24.074), train_loss = 0.43189177, grad/param norm = 1.2136e-01, time/batch = 0.4512s	
1301/2700 (epoch 24.093), train_loss = 0.43567357, grad/param norm = 1.2011e-01, time/batch = 0.3985s	
1302/2700 (epoch 24.111), train_loss = 0.42585527, grad/param norm = 1.1764e-01, time/batch = 0.3858s	
1303/2700 (epoch 24.130), train_loss = 0.41500007, grad/param norm = 1.0986e-01, time/batch = 0.4356s	
1304/2700 (epoch 24.148), train_loss = 0.42212900, grad/param norm = 1.1573e-01, time/batch = 0.4609s	
1305/2700 (epoch 24.167), train_loss = 0.43513275, grad/param norm = 1.3841e-01, time/batch = 0.4709s	
1306/2700 (epoch 24.185), train_loss = 0.43700382, grad/param norm = 1.2905e-01, time/batch = 0.4504s	
1307/2700 (epoch 24.204), train_loss = 0.45747490, grad/param norm = 1.3143e-01, time/batch = 0.4358s	
1308/2700 (epoch 24.222), train_loss = 0.45346364, grad/param norm = 1.5343e-01, time/batch = 0.4099s	
1309/2700 (epoch 24.241), train_loss = 0.47207993, grad/param norm = 1.5566e-01, time/batch = 0.4054s	
1310/2700 (epoch 24.259), train_loss = 0.45302697, grad/param norm = 1.4254e-01, time/batch = 0.3858s	
1311/2700 (epoch 24.278), train_loss = 0.45516216, grad/param norm = 1.1972e-01, time/batch = 0.4261s	
1312/2700 (epoch 24.296), train_loss = 0.44465617, grad/param norm = 1.3177e-01, time/batch = 0.4341s	
1313/2700 (epoch 24.315), train_loss = 0.39368042, grad/param norm = 1.1143e-01, time/batch = 0.3670s	
1314/2700 (epoch 24.333), train_loss = 0.38572009, grad/param norm = 1.0337e-01, time/batch = 0.4520s	
1315/2700 (epoch 24.352), train_loss = 0.41086318, grad/param norm = 1.3634e-01, time/batch = 0.4697s	
1316/2700 (epoch 24.370), train_loss = 0.40560925, grad/param norm = 1.2994e-01, time/batch = 0.4752s	
1317/2700 (epoch 24.389), train_loss = 0.40006274, grad/param norm = 1.0855e-01, time/batch = 0.4453s	
1318/2700 (epoch 24.407), train_loss = 0.44926624, grad/param norm = 1.2688e-01, time/batch = 0.4332s	
1319/2700 (epoch 24.426), train_loss = 0.45759211, grad/param norm = 1.4755e-01, time/batch = 0.4124s	
1320/2700 (epoch 24.444), train_loss = 0.45189569, grad/param norm = 1.4329e-01, time/batch = 0.4122s	
1321/2700 (epoch 24.463), train_loss = 0.43909344, grad/param norm = 1.2006e-01, time/batch = 0.3985s	
1322/2700 (epoch 24.481), train_loss = 0.40215251, grad/param norm = 1.1487e-01, time/batch = 0.4113s	
1323/2700 (epoch 24.500), train_loss = 0.38055022, grad/param norm = 1.0695e-01, time/batch = 0.4389s	
1324/2700 (epoch 24.519), train_loss = 0.42344198, grad/param norm = 1.3226e-01, time/batch = 0.4132s	
1325/2700 (epoch 24.537), train_loss = 0.42453274, grad/param norm = 1.3144e-01, time/batch = 0.3882s	
1326/2700 (epoch 24.556), train_loss = 0.38549874, grad/param norm = 1.1797e-01, time/batch = 0.4614s	
1327/2700 (epoch 24.574), train_loss = 0.40821500, grad/param norm = 1.3805e-01, time/batch = 0.4743s	
1328/2700 (epoch 24.593), train_loss = 0.41755931, grad/param norm = 1.2131e-01, time/batch = 0.4618s	
1329/2700 (epoch 24.611), train_loss = 0.40655195, grad/param norm = 1.3259e-01, time/batch = 0.4295s	
1330/2700 (epoch 24.630), train_loss = 0.41870533, grad/param norm = 1.3094e-01, time/batch = 0.4007s	
1331/2700 (epoch 24.648), train_loss = 0.43531960, grad/param norm = 1.4284e-01, time/batch = 0.4032s	
1332/2700 (epoch 24.667), train_loss = 0.43363046, grad/param norm = 1.3522e-01, time/batch = 0.3800s	
1333/2700 (epoch 24.685), train_loss = 0.43115799, grad/param norm = 1.3433e-01, time/batch = 0.4187s	
1334/2700 (epoch 24.704), train_loss = 0.41325584, grad/param norm = 1.1198e-01, time/batch = 0.4469s	
1335/2700 (epoch 24.722), train_loss = 0.43733650, grad/param norm = 1.3155e-01, time/batch = 0.4119s	
1336/2700 (epoch 24.741), train_loss = 0.39660340, grad/param norm = 1.1894e-01, time/batch = 0.4035s	
1337/2700 (epoch 24.759), train_loss = 0.39272995, grad/param norm = 1.3004e-01, time/batch = 0.4282s	
1338/2700 (epoch 24.778), train_loss = 0.38348448, grad/param norm = 1.1261e-01, time/batch = 0.4683s	
1339/2700 (epoch 24.796), train_loss = 0.35941695, grad/param norm = 1.0797e-01, time/batch = 0.4511s	
1340/2700 (epoch 24.815), train_loss = 0.38590811, grad/param norm = 1.1810e-01, time/batch = 0.4059s	
1341/2700 (epoch 24.833), train_loss = 0.41064604, grad/param norm = 1.3484e-01, time/batch = 0.4082s	
1342/2700 (epoch 24.852), train_loss = 0.41229931, grad/param norm = 1.3557e-01, time/batch = 0.3882s	
1343/2700 (epoch 24.870), train_loss = 0.42740649, grad/param norm = 1.1979e-01, time/batch = 0.3799s	
1344/2700 (epoch 24.889), train_loss = 0.41890781, grad/param norm = 1.4642e-01, time/batch = 0.4506s	
1345/2700 (epoch 24.907), train_loss = 0.41786861, grad/param norm = 1.3197e-01, time/batch = 0.4365s	
1346/2700 (epoch 24.926), train_loss = 0.42251338, grad/param norm = 1.4658e-01, time/batch = 0.4038s	
1347/2700 (epoch 24.944), train_loss = 0.36725636, grad/param norm = 1.0860e-01, time/batch = 0.4188s	
1348/2700 (epoch 24.963), train_loss = 0.39531925, grad/param norm = 1.1182e-01, time/batch = 0.4520s	
1349/2700 (epoch 24.981), train_loss = 0.37361018, grad/param norm = 1.0711e-01, time/batch = 0.4539s	
decayed learning rate by a factor 0.97 to 0.0012285073069254	
1350/2700 (epoch 25.000), train_loss = 0.37150401, grad/param norm = 1.1049e-01, time/batch = 0.4341s	
1351/2700 (epoch 25.019), train_loss = 0.44047239, grad/param norm = 1.2012e-01, time/batch = 0.4272s	
1352/2700 (epoch 25.037), train_loss = 0.39102232, grad/param norm = 1.2408e-01, time/batch = 0.3832s	
1353/2700 (epoch 25.056), train_loss = 0.39056895, grad/param norm = 1.2450e-01, time/batch = 0.3984s	
1354/2700 (epoch 25.074), train_loss = 0.36684475, grad/param norm = 1.1685e-01, time/batch = 0.3907s	
1355/2700 (epoch 25.093), train_loss = 0.38014901, grad/param norm = 1.1143e-01, time/batch = 0.4529s	
1356/2700 (epoch 25.111), train_loss = 0.36504142, grad/param norm = 1.1802e-01, time/batch = 0.4265s	
1357/2700 (epoch 25.130), train_loss = 0.35186348, grad/param norm = 1.0444e-01, time/batch = 0.4029s	
1358/2700 (epoch 25.148), train_loss = 0.33650687, grad/param norm = 9.4327e-02, time/batch = 0.4303s	
1359/2700 (epoch 25.167), train_loss = 0.36073455, grad/param norm = 1.0901e-01, time/batch = 0.4507s	
1360/2700 (epoch 25.185), train_loss = 0.37235688, grad/param norm = 1.3058e-01, time/batch = 0.4605s	
1361/2700 (epoch 25.204), train_loss = 0.40105613, grad/param norm = 1.3243e-01, time/batch = 0.4552s	
1362/2700 (epoch 25.222), train_loss = 0.35286314, grad/param norm = 1.1383e-01, time/batch = 0.3982s	
1363/2700 (epoch 25.241), train_loss = 0.36073769, grad/param norm = 1.0833e-01, time/batch = 0.4024s	
1364/2700 (epoch 25.259), train_loss = 0.37529006, grad/param norm = 1.3626e-01, time/batch = 0.4142s	
1365/2700 (epoch 25.278), train_loss = 0.42195966, grad/param norm = 1.3375e-01, time/batch = 0.4012s	
1366/2700 (epoch 25.296), train_loss = 0.38067750, grad/param norm = 1.3121e-01, time/batch = 0.4394s	
1367/2700 (epoch 25.315), train_loss = 0.36051558, grad/param norm = 1.2390e-01, time/batch = 0.3971s	
1368/2700 (epoch 25.333), train_loss = 0.34682758, grad/param norm = 1.1634e-01, time/batch = 0.4249s	
1369/2700 (epoch 25.352), train_loss = 0.33946187, grad/param norm = 1.1431e-01, time/batch = 0.4544s	
1370/2700 (epoch 25.370), train_loss = 0.33464892, grad/param norm = 1.2043e-01, time/batch = 0.4570s	
1371/2700 (epoch 25.389), train_loss = 0.36545707, grad/param norm = 1.2149e-01, time/batch = 0.4607s	
1372/2700 (epoch 25.407), train_loss = 0.38065057, grad/param norm = 1.1976e-01, time/batch = 0.4438s	
1373/2700 (epoch 25.426), train_loss = 0.37735429, grad/param norm = 1.1652e-01, time/batch = 0.4069s	
1374/2700 (epoch 25.444), train_loss = 0.38378612, grad/param norm = 1.2439e-01, time/batch = 0.3573s	
1375/2700 (epoch 25.463), train_loss = 0.40896195, grad/param norm = 1.5192e-01, time/batch = 0.4165s	
1376/2700 (epoch 25.481), train_loss = 0.36359474, grad/param norm = 1.2531e-01, time/batch = 0.4047s	
1377/2700 (epoch 25.500), train_loss = 0.32026039, grad/param norm = 1.0051e-01, time/batch = 0.4231s	
1378/2700 (epoch 25.519), train_loss = 0.34702129, grad/param norm = 1.1043e-01, time/batch = 0.4027s	
1379/2700 (epoch 25.537), train_loss = 0.36402133, grad/param norm = 1.2308e-01, time/batch = 0.4346s	
1380/2700 (epoch 25.556), train_loss = 0.35048669, grad/param norm = 1.3268e-01, time/batch = 0.4599s	
1381/2700 (epoch 25.574), train_loss = 0.35178920, grad/param norm = 1.1831e-01, time/batch = 0.4578s	
1382/2700 (epoch 25.593), train_loss = 0.36252886, grad/param norm = 1.1357e-01, time/batch = 0.4527s	
1383/2700 (epoch 25.611), train_loss = 0.34413180, grad/param norm = 1.2097e-01, time/batch = 0.4327s	
1384/2700 (epoch 25.630), train_loss = 0.35788083, grad/param norm = 1.2027e-01, time/batch = 0.3870s	
1385/2700 (epoch 25.648), train_loss = 0.35589518, grad/param norm = 1.2129e-01, time/batch = 0.3801s	
1386/2700 (epoch 25.667), train_loss = 0.37087968, grad/param norm = 1.2791e-01, time/batch = 0.3944s	
1387/2700 (epoch 25.685), train_loss = 0.37309580, grad/param norm = 1.2488e-01, time/batch = 0.3963s	
1388/2700 (epoch 25.704), train_loss = 0.36502757, grad/param norm = 1.2133e-01, time/batch = 0.4108s	
1389/2700 (epoch 25.722), train_loss = 0.37077708, grad/param norm = 1.1916e-01, time/batch = 0.4234s	
1390/2700 (epoch 25.741), train_loss = 0.33334191, grad/param norm = 1.1446e-01, time/batch = 0.4570s	
1391/2700 (epoch 25.759), train_loss = 0.31073430, grad/param norm = 9.9606e-02, time/batch = 0.4580s	
1392/2700 (epoch 25.778), train_loss = 0.32379680, grad/param norm = 1.0547e-01, time/batch = 0.4541s	
1393/2700 (epoch 25.796), train_loss = 0.31319067, grad/param norm = 1.1137e-01, time/batch = 0.4442s	
1394/2700 (epoch 25.815), train_loss = 0.33914552, grad/param norm = 1.1929e-01, time/batch = 0.4218s	
1395/2700 (epoch 25.833), train_loss = 0.34546505, grad/param norm = 1.1924e-01, time/batch = 0.3727s	
1396/2700 (epoch 25.852), train_loss = 0.35183233, grad/param norm = 1.1813e-01, time/batch = 0.3944s	
1397/2700 (epoch 25.870), train_loss = 0.35433237, grad/param norm = 1.1261e-01, time/batch = 0.4233s	
1398/2700 (epoch 25.889), train_loss = 0.34423624, grad/param norm = 1.1227e-01, time/batch = 0.4303s	
1399/2700 (epoch 25.907), train_loss = 0.34699488, grad/param norm = 1.2708e-01, time/batch = 0.3730s	
1400/2700 (epoch 25.926), train_loss = 0.35097598, grad/param norm = 1.2221e-01, time/batch = 0.4555s	
1401/2700 (epoch 25.944), train_loss = 0.34020932, grad/param norm = 1.3722e-01, time/batch = 0.4529s	
1402/2700 (epoch 25.963), train_loss = 0.35309593, grad/param norm = 1.1615e-01, time/batch = 0.4520s	
1403/2700 (epoch 25.981), train_loss = 0.32463748, grad/param norm = 1.0701e-01, time/batch = 0.4458s	
decayed learning rate by a factor 0.97 to 0.0011916520877176	
1404/2700 (epoch 26.000), train_loss = 0.31931262, grad/param norm = 1.0585e-01, time/batch = 0.3974s	
1405/2700 (epoch 26.019), train_loss = 0.38530283, grad/param norm = 1.1743e-01, time/batch = 0.3928s	
1406/2700 (epoch 26.037), train_loss = 0.32851183, grad/param norm = 1.0836e-01, time/batch = 0.4023s	
1407/2700 (epoch 26.056), train_loss = 0.33233206, grad/param norm = 1.2333e-01, time/batch = 0.4264s	
1408/2700 (epoch 26.074), train_loss = 0.31511007, grad/param norm = 1.1430e-01, time/batch = 0.4260s	
1409/2700 (epoch 26.093), train_loss = 0.31988205, grad/param norm = 1.1616e-01, time/batch = 0.3841s	
1410/2700 (epoch 26.111), train_loss = 0.30222732, grad/param norm = 1.0166e-01, time/batch = 0.4216s	
1411/2700 (epoch 26.130), train_loss = 0.31386574, grad/param norm = 1.1548e-01, time/batch = 0.4180s	
1412/2700 (epoch 26.148), train_loss = 0.30473114, grad/param norm = 9.9251e-02, time/batch = 0.4597s	
1413/2700 (epoch 26.167), train_loss = 0.29260959, grad/param norm = 9.3302e-02, time/batch = 0.4578s	
1414/2700 (epoch 26.185), train_loss = 0.29708911, grad/param norm = 1.0014e-01, time/batch = 0.3992s	
1415/2700 (epoch 26.204), train_loss = 0.31187649, grad/param norm = 1.0816e-01, time/batch = 0.3716s	
1416/2700 (epoch 26.222), train_loss = 0.31783262, grad/param norm = 1.2931e-01, time/batch = 0.3953s	
1417/2700 (epoch 26.241), train_loss = 0.33373485, grad/param norm = 1.3158e-01, time/batch = 0.4308s	
1418/2700 (epoch 26.259), train_loss = 0.32266572, grad/param norm = 1.1250e-01, time/batch = 0.4478s	
1419/2700 (epoch 26.278), train_loss = 0.35362222, grad/param norm = 1.2922e-01, time/batch = 0.4130s	
1420/2700 (epoch 26.296), train_loss = 0.31510885, grad/param norm = 1.1634e-01, time/batch = 0.3975s	
1421/2700 (epoch 26.315), train_loss = 0.31698691, grad/param norm = 1.2519e-01, time/batch = 0.4160s	
1422/2700 (epoch 26.333), train_loss = 0.31028032, grad/param norm = 1.1471e-01, time/batch = 0.4428s	
1423/2700 (epoch 26.352), train_loss = 0.29104028, grad/param norm = 1.1267e-01, time/batch = 0.4371s	
1424/2700 (epoch 26.370), train_loss = 0.27944825, grad/param norm = 1.0775e-01, time/batch = 0.3997s	
1425/2700 (epoch 26.389), train_loss = 0.29556234, grad/param norm = 1.0963e-01, time/batch = 0.3584s	
1426/2700 (epoch 26.407), train_loss = 0.32964043, grad/param norm = 1.1194e-01, time/batch = 0.4115s	
1427/2700 (epoch 26.426), train_loss = 0.32212212, grad/param norm = 1.2387e-01, time/batch = 0.4494s	
1428/2700 (epoch 26.444), train_loss = 0.33186524, grad/param norm = 1.2417e-01, time/batch = 0.4477s	
1429/2700 (epoch 26.463), train_loss = 0.32477956, grad/param norm = 1.1545e-01, time/batch = 0.4285s	
1430/2700 (epoch 26.481), train_loss = 0.30270565, grad/param norm = 1.1002e-01, time/batch = 0.4047s	
1431/2700 (epoch 26.500), train_loss = 0.28591460, grad/param norm = 1.0495e-01, time/batch = 0.3979s	
1432/2700 (epoch 26.519), train_loss = 0.28948428, grad/param norm = 9.7026e-02, time/batch = 0.4257s	
1433/2700 (epoch 26.537), train_loss = 0.29133772, grad/param norm = 1.0176e-01, time/batch = 0.4350s	
1434/2700 (epoch 26.556), train_loss = 0.27632560, grad/param norm = 1.0671e-01, time/batch = 0.4392s	
1435/2700 (epoch 26.574), train_loss = 0.28897964, grad/param norm = 9.9547e-02, time/batch = 0.4257s	
1436/2700 (epoch 26.593), train_loss = 0.31482637, grad/param norm = 1.2451e-01, time/batch = 0.3346s	
1437/2700 (epoch 26.611), train_loss = 0.31378642, grad/param norm = 1.1819e-01, time/batch = 0.4233s	
1438/2700 (epoch 26.630), train_loss = 0.29581921, grad/param norm = 1.0533e-01, time/batch = 0.4477s	
1439/2700 (epoch 26.648), train_loss = 0.30235075, grad/param norm = 1.1519e-01, time/batch = 0.4336s	
1440/2700 (epoch 26.667), train_loss = 0.30255675, grad/param norm = 1.1606e-01, time/batch = 0.4010s	
1441/2700 (epoch 26.685), train_loss = 0.31778212, grad/param norm = 1.1671e-01, time/batch = 0.3933s	
1442/2700 (epoch 26.704), train_loss = 0.31792757, grad/param norm = 1.2744e-01, time/batch = 0.4234s	
1443/2700 (epoch 26.722), train_loss = 0.33450019, grad/param norm = 1.1826e-01, time/batch = 0.4442s	
1444/2700 (epoch 26.741), train_loss = 0.31064593, grad/param norm = 1.1882e-01, time/batch = 0.4637s	
1445/2700 (epoch 26.759), train_loss = 0.28032933, grad/param norm = 1.0679e-01, time/batch = 0.4430s	
1446/2700 (epoch 26.778), train_loss = 0.30069222, grad/param norm = 1.1834e-01, time/batch = 0.4383s	
1447/2700 (epoch 26.796), train_loss = 0.27228397, grad/param norm = 1.0889e-01, time/batch = 0.3684s	
1448/2700 (epoch 26.815), train_loss = 0.28272159, grad/param norm = 1.0538e-01, time/batch = 0.3711s	
1449/2700 (epoch 26.833), train_loss = 0.29651735, grad/param norm = 1.1433e-01, time/batch = 0.4336s	
1450/2700 (epoch 26.852), train_loss = 0.29431189, grad/param norm = 1.2243e-01, time/batch = 0.4282s	
1451/2700 (epoch 26.870), train_loss = 0.29805766, grad/param norm = 9.4884e-02, time/batch = 0.3964s	
1452/2700 (epoch 26.889), train_loss = 0.28012881, grad/param norm = 1.0338e-01, time/batch = 0.3889s	
1453/2700 (epoch 26.907), train_loss = 0.28756792, grad/param norm = 1.0760e-01, time/batch = 0.4285s	
1454/2700 (epoch 26.926), train_loss = 0.27780596, grad/param norm = 9.6165e-02, time/batch = 0.4580s	
1455/2700 (epoch 26.944), train_loss = 0.28222370, grad/param norm = 1.2445e-01, time/batch = 0.4744s	
1456/2700 (epoch 26.963), train_loss = 0.29378525, grad/param norm = 1.0713e-01, time/batch = 0.4438s	
1457/2700 (epoch 26.981), train_loss = 0.28659895, grad/param norm = 1.1185e-01, time/batch = 0.4112s	
decayed learning rate by a factor 0.97 to 0.0011559025250861	
1458/2700 (epoch 27.000), train_loss = 0.28192017, grad/param norm = 1.0569e-01, time/batch = 0.3670s	
1459/2700 (epoch 27.019), train_loss = 0.33717023, grad/param norm = 1.0670e-01, time/batch = 0.4043s	
1460/2700 (epoch 27.037), train_loss = 0.27130133, grad/param norm = 9.3639e-02, time/batch = 0.4285s	
1461/2700 (epoch 27.056), train_loss = 0.28498196, grad/param norm = 1.1807e-01, time/batch = 0.4150s	
1462/2700 (epoch 27.074), train_loss = 0.27842662, grad/param norm = 1.2458e-01, time/batch = 0.3771s	
1463/2700 (epoch 27.093), train_loss = 0.28048642, grad/param norm = 1.0670e-01, time/batch = 0.4223s	
1464/2700 (epoch 27.111), train_loss = 0.26874727, grad/param norm = 1.0784e-01, time/batch = 0.4393s	
1465/2700 (epoch 27.130), train_loss = 0.27772073, grad/param norm = 1.1211e-01, time/batch = 0.4660s	
1466/2700 (epoch 27.148), train_loss = 0.25714335, grad/param norm = 9.7278e-02, time/batch = 0.4758s	
1467/2700 (epoch 27.167), train_loss = 0.27192019, grad/param norm = 1.0838e-01, time/batch = 0.4054s	
1468/2700 (epoch 27.185), train_loss = 0.24663464, grad/param norm = 9.7806e-02, time/batch = 0.3790s	
1469/2700 (epoch 27.204), train_loss = 0.25832155, grad/param norm = 8.8846e-02, time/batch = 0.3848s	
1470/2700 (epoch 27.222), train_loss = 0.23731371, grad/param norm = 9.0548e-02, time/batch = 0.4259s	
1471/2700 (epoch 27.241), train_loss = 0.25761952, grad/param norm = 1.0202e-01, time/batch = 0.4235s	
1472/2700 (epoch 27.259), train_loss = 0.26748154, grad/param norm = 1.1309e-01, time/batch = 0.4338s	
1473/2700 (epoch 27.278), train_loss = 0.29824391, grad/param norm = 1.0834e-01, time/batch = 0.3671s	
1474/2700 (epoch 27.296), train_loss = 0.27745286, grad/param norm = 1.1739e-01, time/batch = 0.4536s	
1475/2700 (epoch 27.315), train_loss = 0.26130092, grad/param norm = 1.1055e-01, time/batch = 0.4598s	
1476/2700 (epoch 27.333), train_loss = 0.25453174, grad/param norm = 1.0080e-01, time/batch = 0.4782s	
1477/2700 (epoch 27.352), train_loss = 0.24149929, grad/param norm = 9.6506e-02, time/batch = 0.4497s	
1478/2700 (epoch 27.370), train_loss = 0.23181404, grad/param norm = 9.4968e-02, time/batch = 0.4227s	
1479/2700 (epoch 27.389), train_loss = 0.25597211, grad/param norm = 1.0509e-01, time/batch = 0.3929s	
1480/2700 (epoch 27.407), train_loss = 0.27820269, grad/param norm = 1.0247e-01, time/batch = 0.3961s	
1481/2700 (epoch 27.426), train_loss = 0.28078792, grad/param norm = 1.2390e-01, time/batch = 0.4104s	
1482/2700 (epoch 27.444), train_loss = 0.28823911, grad/param norm = 1.3401e-01, time/batch = 0.4168s	
1483/2700 (epoch 27.463), train_loss = 0.29713648, grad/param norm = 1.1638e-01, time/batch = 0.4179s	
1484/2700 (epoch 27.481), train_loss = 0.26575541, grad/param norm = 1.1842e-01, time/batch = 0.4001s	
1485/2700 (epoch 27.500), train_loss = 0.25134709, grad/param norm = 1.0513e-01, time/batch = 0.3964s	
1486/2700 (epoch 27.519), train_loss = 0.25988498, grad/param norm = 9.9712e-02, time/batch = 0.4572s	
1487/2700 (epoch 27.537), train_loss = 0.25828005, grad/param norm = 1.0832e-01, time/batch = 0.4739s	
1488/2700 (epoch 27.556), train_loss = 0.23441458, grad/param norm = 9.5085e-02, time/batch = 0.4465s	
1489/2700 (epoch 27.574), train_loss = 0.24967058, grad/param norm = 9.1554e-02, time/batch = 0.3983s	
1490/2700 (epoch 27.593), train_loss = 0.25944166, grad/param norm = 1.0097e-01, time/batch = 0.4008s	
1491/2700 (epoch 27.611), train_loss = 0.24466377, grad/param norm = 9.5035e-02, time/batch = 0.4254s	
1492/2700 (epoch 27.630), train_loss = 0.25659621, grad/param norm = 1.0185e-01, time/batch = 0.4273s	
1493/2700 (epoch 27.648), train_loss = 0.26444025, grad/param norm = 1.0346e-01, time/batch = 0.4338s	
1494/2700 (epoch 27.667), train_loss = 0.25675490, grad/param norm = 1.0258e-01, time/batch = 0.3939s	
1495/2700 (epoch 27.685), train_loss = 0.26907321, grad/param norm = 1.0821e-01, time/batch = 0.3718s	
1496/2700 (epoch 27.704), train_loss = 0.26096125, grad/param norm = 1.0048e-01, time/batch = 0.4185s	
1497/2700 (epoch 27.722), train_loss = 0.27720833, grad/param norm = 1.1403e-01, time/batch = 0.4358s	
1498/2700 (epoch 27.741), train_loss = 0.25851689, grad/param norm = 1.0529e-01, time/batch = 0.4691s	
1499/2700 (epoch 27.759), train_loss = 0.23844129, grad/param norm = 9.6488e-02, time/batch = 0.4370s	
1500/2700 (epoch 27.778), train_loss = 0.23904801, grad/param norm = 9.0657e-02, time/batch = 0.3973s	
1501/2700 (epoch 27.796), train_loss = 0.22111624, grad/param norm = 9.5220e-02, time/batch = 0.4001s	
1502/2700 (epoch 27.815), train_loss = 0.24902730, grad/param norm = 1.0608e-01, time/batch = 0.4231s	
1503/2700 (epoch 27.833), train_loss = 0.25543136, grad/param norm = 1.0745e-01, time/batch = 0.4364s	
1504/2700 (epoch 27.852), train_loss = 0.24819698, grad/param norm = 1.0069e-01, time/batch = 0.4273s	
1505/2700 (epoch 27.870), train_loss = 0.26330324, grad/param norm = 1.0787e-01, time/batch = 0.3971s	
1506/2700 (epoch 27.889), train_loss = 0.25809132, grad/param norm = 1.1279e-01, time/batch = 0.3711s	
1507/2700 (epoch 27.907), train_loss = 0.24215954, grad/param norm = 1.0554e-01, time/batch = 0.4275s	
1508/2700 (epoch 27.926), train_loss = 0.24877636, grad/param norm = 1.0371e-01, time/batch = 0.4526s	
1509/2700 (epoch 27.944), train_loss = 0.23313494, grad/param norm = 1.0495e-01, time/batch = 0.4643s	
1510/2700 (epoch 27.963), train_loss = 0.25943352, grad/param norm = 1.0212e-01, time/batch = 0.4149s	
1511/2700 (epoch 27.981), train_loss = 0.22922768, grad/param norm = 9.3382e-02, time/batch = 0.4002s	
decayed learning rate by a factor 0.97 to 0.0011212254493335	
1512/2700 (epoch 28.000), train_loss = 0.23891808, grad/param norm = 9.7160e-02, time/batch = 0.4156s	
1513/2700 (epoch 28.019), train_loss = 0.31191220, grad/param norm = 1.2368e-01, time/batch = 0.4474s	
1514/2700 (epoch 28.037), train_loss = 0.25994959, grad/param norm = 1.0707e-01, time/batch = 0.4316s	
1515/2700 (epoch 28.056), train_loss = 0.23237756, grad/param norm = 9.5162e-02, time/batch = 0.4036s	
1516/2700 (epoch 28.074), train_loss = 0.23434257, grad/param norm = 1.1123e-01, time/batch = 0.3792s	
1517/2700 (epoch 28.093), train_loss = 0.24328088, grad/param norm = 1.0411e-01, time/batch = 0.3937s	
1518/2700 (epoch 28.111), train_loss = 0.22978230, grad/param norm = 9.6401e-02, time/batch = 0.4393s	
1519/2700 (epoch 28.130), train_loss = 0.23889495, grad/param norm = 1.1240e-01, time/batch = 0.4515s	
1520/2700 (epoch 28.148), train_loss = 0.22908055, grad/param norm = 9.4355e-02, time/batch = 0.4705s	
1521/2700 (epoch 28.167), train_loss = 0.22187302, grad/param norm = 8.5442e-02, time/batch = 0.4551s	
1522/2700 (epoch 28.185), train_loss = 0.22791928, grad/param norm = 1.0156e-01, time/batch = 0.3665s	
1523/2700 (epoch 28.204), train_loss = 0.23438235, grad/param norm = 1.0187e-01, time/batch = 0.4367s	
1524/2700 (epoch 28.222), train_loss = 0.20978473, grad/param norm = 9.5608e-02, time/batch = 0.4603s	
1525/2700 (epoch 28.241), train_loss = 0.21076378, grad/param norm = 8.9456e-02, time/batch = 0.4479s	
1526/2700 (epoch 28.259), train_loss = 0.21361886, grad/param norm = 8.1897e-02, time/batch = 0.4135s	
1527/2700 (epoch 28.278), train_loss = 0.25211636, grad/param norm = 1.0877e-01, time/batch = 0.3599s	
1528/2700 (epoch 28.296), train_loss = 0.23138321, grad/param norm = 9.8097e-02, time/batch = 0.3879s	
1529/2700 (epoch 28.315), train_loss = 0.22852095, grad/param norm = 1.0929e-01, time/batch = 0.4307s	
1530/2700 (epoch 28.333), train_loss = 0.24070693, grad/param norm = 1.1410e-01, time/batch = 0.4384s	
1531/2700 (epoch 28.352), train_loss = 0.22248820, grad/param norm = 9.8572e-02, time/batch = 0.4393s	
1532/2700 (epoch 28.370), train_loss = 0.20304303, grad/param norm = 9.3869e-02, time/batch = 0.3988s	
1533/2700 (epoch 28.389), train_loss = 0.22660763, grad/param norm = 1.0387e-01, time/batch = 0.4120s	
1534/2700 (epoch 28.407), train_loss = 0.25845271, grad/param norm = 1.1146e-01, time/batch = 0.4378s	
1535/2700 (epoch 28.426), train_loss = 0.23855340, grad/param norm = 1.0837e-01, time/batch = 0.4755s	
1536/2700 (epoch 28.444), train_loss = 0.23151140, grad/param norm = 1.0247e-01, time/batch = 0.4664s	
1537/2700 (epoch 28.463), train_loss = 0.26809862, grad/param norm = 1.2447e-01, time/batch = 0.4343s	
1538/2700 (epoch 28.481), train_loss = 0.22389190, grad/param norm = 1.0056e-01, time/batch = 0.3870s	
1539/2700 (epoch 28.500), train_loss = 0.21451296, grad/param norm = 1.0220e-01, time/batch = 0.3906s	
1540/2700 (epoch 28.519), train_loss = 0.22364367, grad/param norm = 9.4929e-02, time/batch = 0.4112s	
1541/2700 (epoch 28.537), train_loss = 0.22113957, grad/param norm = 1.0311e-01, time/batch = 0.4025s	
1542/2700 (epoch 28.556), train_loss = 0.20558954, grad/param norm = 9.7509e-02, time/batch = 0.4375s	
1543/2700 (epoch 28.574), train_loss = 0.22960261, grad/param norm = 1.0636e-01, time/batch = 0.4083s	
1544/2700 (epoch 28.593), train_loss = 0.24096611, grad/param norm = 1.0413e-01, time/batch = 0.4132s	
1545/2700 (epoch 28.611), train_loss = 0.21939528, grad/param norm = 9.8942e-02, time/batch = 0.4495s	
1546/2700 (epoch 28.630), train_loss = 0.21818353, grad/param norm = 9.0598e-02, time/batch = 0.4464s	
1547/2700 (epoch 28.648), train_loss = 0.22276226, grad/param norm = 8.9879e-02, time/batch = 0.3939s	
1548/2700 (epoch 28.667), train_loss = 0.23340448, grad/param norm = 1.1396e-01, time/batch = 0.3725s	
1549/2700 (epoch 28.685), train_loss = 0.23607479, grad/param norm = 1.0427e-01, time/batch = 0.3969s	
1550/2700 (epoch 28.704), train_loss = 0.22259989, grad/param norm = 9.3621e-02, time/batch = 0.4389s	
1551/2700 (epoch 28.722), train_loss = 0.21754953, grad/param norm = 8.4909e-02, time/batch = 0.4309s	
1552/2700 (epoch 28.741), train_loss = 0.21772916, grad/param norm = 9.4043e-02, time/batch = 0.4295s	
1553/2700 (epoch 28.759), train_loss = 0.21103509, grad/param norm = 9.4836e-02, time/batch = 0.4184s	
1554/2700 (epoch 28.778), train_loss = 0.21357134, grad/param norm = 9.7221e-02, time/batch = 0.4151s	
1555/2700 (epoch 28.796), train_loss = 0.18549794, grad/param norm = 8.1600e-02, time/batch = 0.4399s	
1556/2700 (epoch 28.815), train_loss = 0.19383556, grad/param norm = 8.0943e-02, time/batch = 0.4332s	
1557/2700 (epoch 28.833), train_loss = 0.21813107, grad/param norm = 1.0958e-01, time/batch = 0.4415s	
1558/2700 (epoch 28.852), train_loss = 0.21283772, grad/param norm = 9.7116e-02, time/batch = 0.4015s	
1559/2700 (epoch 28.870), train_loss = 0.22156434, grad/param norm = 9.3603e-02, time/batch = 0.3458s	
1560/2700 (epoch 28.889), train_loss = 0.21354779, grad/param norm = 9.6832e-02, time/batch = 0.4256s	
1561/2700 (epoch 28.907), train_loss = 0.21798641, grad/param norm = 1.0391e-01, time/batch = 0.4256s	
1562/2700 (epoch 28.926), train_loss = 0.20548393, grad/param norm = 8.9019e-02, time/batch = 0.4246s	
1563/2700 (epoch 28.944), train_loss = 0.19490798, grad/param norm = 9.5266e-02, time/batch = 0.3866s	
1564/2700 (epoch 28.963), train_loss = 0.21088689, grad/param norm = 8.4837e-02, time/batch = 0.4273s	
1565/2700 (epoch 28.981), train_loss = 0.20267916, grad/param norm = 8.6085e-02, time/batch = 0.4502s	
decayed learning rate by a factor 0.97 to 0.0010875886858535	
1566/2700 (epoch 29.000), train_loss = 0.19305952, grad/param norm = 8.0206e-02, time/batch = 0.4559s	
1567/2700 (epoch 29.019), train_loss = 0.24432762, grad/param norm = 8.5897e-02, time/batch = 0.4565s	
1568/2700 (epoch 29.037), train_loss = 0.20918518, grad/param norm = 8.8541e-02, time/batch = 0.4226s	
1569/2700 (epoch 29.056), train_loss = 0.20470346, grad/param norm = 9.4727e-02, time/batch = 0.3826s	
1570/2700 (epoch 29.074), train_loss = 0.19575664, grad/param norm = 9.3801e-02, time/batch = 0.3738s	
1571/2700 (epoch 29.093), train_loss = 0.19252871, grad/param norm = 8.1827e-02, time/batch = 0.3756s	
1572/2700 (epoch 29.111), train_loss = 0.19442679, grad/param norm = 8.9870e-02, time/batch = 0.4276s	
1573/2700 (epoch 29.130), train_loss = 0.19449501, grad/param norm = 8.8771e-02, time/batch = 0.4159s	
1574/2700 (epoch 29.148), train_loss = 0.19820808, grad/param norm = 8.5928e-02, time/batch = 0.3887s	
1575/2700 (epoch 29.167), train_loss = 0.19794991, grad/param norm = 8.5479e-02, time/batch = 0.4399s	
1576/2700 (epoch 29.185), train_loss = 0.18551783, grad/param norm = 7.8237e-02, time/batch = 0.4622s	
1577/2700 (epoch 29.204), train_loss = 0.19418475, grad/param norm = 8.1450e-02, time/batch = 0.4364s	
1578/2700 (epoch 29.222), train_loss = 0.17707996, grad/param norm = 9.2655e-02, time/batch = 0.4101s	
1579/2700 (epoch 29.241), train_loss = 0.18001440, grad/param norm = 8.6731e-02, time/batch = 0.3821s	
1580/2700 (epoch 29.259), train_loss = 0.18914209, grad/param norm = 9.0831e-02, time/batch = 0.3881s	
1581/2700 (epoch 29.278), train_loss = 0.20433828, grad/param norm = 8.4583e-02, time/batch = 0.4033s	
1582/2700 (epoch 29.296), train_loss = 0.18454540, grad/param norm = 8.4833e-02, time/batch = 0.4359s	
1583/2700 (epoch 29.315), train_loss = 0.18141246, grad/param norm = 9.6654e-02, time/batch = 0.4473s	
1584/2700 (epoch 29.333), train_loss = 0.18261775, grad/param norm = 8.7889e-02, time/batch = 0.3698s	
1585/2700 (epoch 29.352), train_loss = 0.16904081, grad/param norm = 8.1662e-02, time/batch = 0.4348s	
1586/2700 (epoch 29.370), train_loss = 0.15948992, grad/param norm = 7.9762e-02, time/batch = 0.4514s	
1587/2700 (epoch 29.389), train_loss = 0.18243745, grad/param norm = 8.3149e-02, time/batch = 0.4672s	
1588/2700 (epoch 29.407), train_loss = 0.20985386, grad/param norm = 9.2783e-02, time/batch = 0.4117s	
1589/2700 (epoch 29.426), train_loss = 0.21306535, grad/param norm = 1.0649e-01, time/batch = 0.3828s	
1590/2700 (epoch 29.444), train_loss = 0.20196875, grad/param norm = 1.0890e-01, time/batch = 0.3909s	
1591/2700 (epoch 29.463), train_loss = 0.21667974, grad/param norm = 1.0183e-01, time/batch = 0.3940s	
1592/2700 (epoch 29.481), train_loss = 0.19211340, grad/param norm = 9.6217e-02, time/batch = 0.4163s	
1593/2700 (epoch 29.500), train_loss = 0.17800565, grad/param norm = 8.2952e-02, time/batch = 0.4456s	
1594/2700 (epoch 29.519), train_loss = 0.19100227, grad/param norm = 8.2796e-02, time/batch = 0.4144s	
1595/2700 (epoch 29.537), train_loss = 0.18260433, grad/param norm = 8.3066e-02, time/batch = 0.3962s	
1596/2700 (epoch 29.556), train_loss = 0.17203441, grad/param norm = 7.7474e-02, time/batch = 0.4061s	
1597/2700 (epoch 29.574), train_loss = 0.18475596, grad/param norm = 7.7463e-02, time/batch = 0.4670s	
1598/2700 (epoch 29.593), train_loss = 0.18957646, grad/param norm = 8.7740e-02, time/batch = 0.4282s	
1599/2700 (epoch 29.611), train_loss = 0.20935798, grad/param norm = 1.0491e-01, time/batch = 0.4014s	
1600/2700 (epoch 29.630), train_loss = 0.20338763, grad/param norm = 1.1555e-01, time/batch = 0.3818s	
1601/2700 (epoch 29.648), train_loss = 0.20992114, grad/param norm = 9.4248e-02, time/batch = 0.3854s	
1602/2700 (epoch 29.667), train_loss = 0.19204973, grad/param norm = 8.5830e-02, time/batch = 0.4052s	
1603/2700 (epoch 29.685), train_loss = 0.20201428, grad/param norm = 1.0089e-01, time/batch = 0.4396s	
1604/2700 (epoch 29.704), train_loss = 0.21081572, grad/param norm = 1.0213e-01, time/batch = 0.4236s	
1605/2700 (epoch 29.722), train_loss = 0.20330265, grad/param norm = 1.0049e-01, time/batch = 0.3846s	
1606/2700 (epoch 29.741), train_loss = 0.18249683, grad/param norm = 8.1804e-02, time/batch = 0.4091s	
1607/2700 (epoch 29.759), train_loss = 0.17553273, grad/param norm = 8.1781e-02, time/batch = 0.4277s	
1608/2700 (epoch 29.778), train_loss = 0.18022887, grad/param norm = 8.0016e-02, time/batch = 0.4285s	
1609/2700 (epoch 29.796), train_loss = 0.16297998, grad/param norm = 7.9204e-02, time/batch = 0.3877s	
1610/2700 (epoch 29.815), train_loss = 0.17535590, grad/param norm = 8.0981e-02, time/batch = 0.3868s	
1611/2700 (epoch 29.833), train_loss = 0.17411448, grad/param norm = 7.0793e-02, time/batch = 0.3892s	
1612/2700 (epoch 29.852), train_loss = 0.18509519, grad/param norm = 9.8361e-02, time/batch = 0.4165s	
1613/2700 (epoch 29.870), train_loss = 0.19770752, grad/param norm = 8.9776e-02, time/batch = 0.4282s	
1614/2700 (epoch 29.889), train_loss = 0.18312806, grad/param norm = 9.5282e-02, time/batch = 0.4052s	
1615/2700 (epoch 29.907), train_loss = 0.17566599, grad/param norm = 9.2048e-02, time/batch = 0.3807s	
1616/2700 (epoch 29.926), train_loss = 0.17796661, grad/param norm = 8.5706e-02, time/batch = 0.4076s	
1617/2700 (epoch 29.944), train_loss = 0.16796178, grad/param norm = 9.0608e-02, time/batch = 0.4323s	
1618/2700 (epoch 29.963), train_loss = 0.18911218, grad/param norm = 8.5553e-02, time/batch = 0.4379s	
1619/2700 (epoch 29.981), train_loss = 0.16922470, grad/param norm = 7.6330e-02, time/batch = 0.3814s	
decayed learning rate by a factor 0.97 to 0.0010549610252779	
1620/2700 (epoch 30.000), train_loss = 0.16681383, grad/param norm = 7.7791e-02, time/batch = 0.3869s	
1621/2700 (epoch 30.019), train_loss = 0.21994780, grad/param norm = 8.6292e-02, time/batch = 0.3585s	
1622/2700 (epoch 30.037), train_loss = 0.17990471, grad/param norm = 7.9636e-02, time/batch = 0.4310s	
1623/2700 (epoch 30.056), train_loss = 0.16089187, grad/param norm = 7.0988e-02, time/batch = 0.4308s	
1624/2700 (epoch 30.074), train_loss = 0.16664890, grad/param norm = 8.0698e-02, time/batch = 0.3995s	
1625/2700 (epoch 30.093), train_loss = 0.16381402, grad/param norm = 7.9276e-02, time/batch = 0.4000s	
1626/2700 (epoch 30.111), train_loss = 0.16419974, grad/param norm = 8.3086e-02, time/batch = 0.4259s	
1627/2700 (epoch 30.130), train_loss = 0.15758385, grad/param norm = 7.4625e-02, time/batch = 0.4480s	
1628/2700 (epoch 30.148), train_loss = 0.16571915, grad/param norm = 7.9395e-02, time/batch = 0.4651s	
1629/2700 (epoch 30.167), train_loss = 0.16600988, grad/param norm = 7.4920e-02, time/batch = 0.4758s	
1630/2700 (epoch 30.185), train_loss = 0.15923060, grad/param norm = 8.4221e-02, time/batch = 0.4057s	
1631/2700 (epoch 30.204), train_loss = 0.17141062, grad/param norm = 8.7728e-02, time/batch = 0.4144s	
1632/2700 (epoch 30.222), train_loss = 0.14907667, grad/param norm = 7.7001e-02, time/batch = 0.3609s	
1633/2700 (epoch 30.241), train_loss = 0.16136894, grad/param norm = 8.7106e-02, time/batch = 0.3691s	
1634/2700 (epoch 30.259), train_loss = 0.17799566, grad/param norm = 8.9948e-02, time/batch = 0.4333s	
1635/2700 (epoch 30.278), train_loss = 0.18167786, grad/param norm = 8.5280e-02, time/batch = 0.4184s	
1636/2700 (epoch 30.296), train_loss = 0.16725161, grad/param norm = 8.6483e-02, time/batch = 0.3895s	
1637/2700 (epoch 30.315), train_loss = 0.15674702, grad/param norm = 8.1313e-02, time/batch = 0.4317s	
1638/2700 (epoch 30.333), train_loss = 0.16169238, grad/param norm = 9.2396e-02, time/batch = 0.4559s	
1639/2700 (epoch 30.352), train_loss = 0.15401188, grad/param norm = 7.9433e-02, time/batch = 0.4723s	
1640/2700 (epoch 30.370), train_loss = 0.14656157, grad/param norm = 7.7850e-02, time/batch = 0.4607s	
1641/2700 (epoch 30.389), train_loss = 0.15298104, grad/param norm = 7.4728e-02, time/batch = 0.4444s	
1642/2700 (epoch 30.407), train_loss = 0.17092686, grad/param norm = 7.1785e-02, time/batch = 0.4054s	
1643/2700 (epoch 30.426), train_loss = 0.16306052, grad/param norm = 8.3291e-02, time/batch = 0.3929s	
1644/2700 (epoch 30.444), train_loss = 0.17138678, grad/param norm = 8.8806e-02, time/batch = 0.4041s	
1645/2700 (epoch 30.463), train_loss = 0.18704986, grad/param norm = 8.8644e-02, time/batch = 0.4033s	
1646/2700 (epoch 30.481), train_loss = 0.16088882, grad/param norm = 8.5495e-02, time/batch = 0.3957s	
1647/2700 (epoch 30.500), train_loss = 0.15167845, grad/param norm = 8.2604e-02, time/batch = 0.3969s	
1648/2700 (epoch 30.519), train_loss = 0.16530021, grad/param norm = 8.3049e-02, time/batch = 0.4386s	
1649/2700 (epoch 30.537), train_loss = 0.16158458, grad/param norm = 8.4880e-02, time/batch = 0.4608s	
1650/2700 (epoch 30.556), train_loss = 0.15274468, grad/param norm = 8.0965e-02, time/batch = 0.4734s	
1651/2700 (epoch 30.574), train_loss = 0.15655715, grad/param norm = 7.1432e-02, time/batch = 0.4505s	
1652/2700 (epoch 30.593), train_loss = 0.16126255, grad/param norm = 7.1684e-02, time/batch = 0.4286s	
1653/2700 (epoch 30.611), train_loss = 0.15967143, grad/param norm = 7.5802e-02, time/batch = 0.4092s	
1654/2700 (epoch 30.630), train_loss = 0.15868717, grad/param norm = 8.2160e-02, time/batch = 0.4074s	
1655/2700 (epoch 30.648), train_loss = 0.18031031, grad/param norm = 9.8030e-02, time/batch = 0.4186s	
1656/2700 (epoch 30.667), train_loss = 0.20096306, grad/param norm = 1.1190e-01, time/batch = 0.4247s	
1657/2700 (epoch 30.685), train_loss = 0.18708352, grad/param norm = 1.0461e-01, time/batch = 0.4139s	
1658/2700 (epoch 30.704), train_loss = 0.18719125, grad/param norm = 9.7218e-02, time/batch = 0.3513s	
1659/2700 (epoch 30.722), train_loss = 0.17849784, grad/param norm = 8.3395e-02, time/batch = 0.4478s	
1660/2700 (epoch 30.741), train_loss = 0.17393668, grad/param norm = 8.6828e-02, time/batch = 0.4606s	
1661/2700 (epoch 30.759), train_loss = 0.15355323, grad/param norm = 7.3509e-02, time/batch = 0.4466s	
1662/2700 (epoch 30.778), train_loss = 0.15433709, grad/param norm = 7.2864e-02, time/batch = 0.4258s	
1663/2700 (epoch 30.796), train_loss = 0.14288425, grad/param norm = 7.9774e-02, time/batch = 0.4255s	
1664/2700 (epoch 30.815), train_loss = 0.14931962, grad/param norm = 7.4876e-02, time/batch = 0.4105s	
1665/2700 (epoch 30.833), train_loss = 0.16192340, grad/param norm = 8.4966e-02, time/batch = 0.4281s	
1666/2700 (epoch 30.852), train_loss = 0.15148762, grad/param norm = 7.3877e-02, time/batch = 0.4358s	
1667/2700 (epoch 30.870), train_loss = 0.16691011, grad/param norm = 8.1444e-02, time/batch = 0.4288s	
1668/2700 (epoch 30.889), train_loss = 0.14753859, grad/param norm = 7.0806e-02, time/batch = 0.3903s	
1669/2700 (epoch 30.907), train_loss = 0.14306358, grad/param norm = 7.1395e-02, time/batch = 0.3696s	
1670/2700 (epoch 30.926), train_loss = 0.15967221, grad/param norm = 9.0800e-02, time/batch = 0.4121s	
1671/2700 (epoch 30.944), train_loss = 0.14069161, grad/param norm = 7.1075e-02, time/batch = 0.4344s	
1672/2700 (epoch 30.963), train_loss = 0.15794045, grad/param norm = 7.4206e-02, time/batch = 0.4454s	
1673/2700 (epoch 30.981), train_loss = 0.14244935, grad/param norm = 6.7418e-02, time/batch = 0.4057s	
decayed learning rate by a factor 0.97 to 0.0010233121945196	
1674/2700 (epoch 31.000), train_loss = 0.14052442, grad/param norm = 7.8513e-02, time/batch = 0.4105s	
1675/2700 (epoch 31.019), train_loss = 0.19605873, grad/param norm = 8.3164e-02, time/batch = 0.4294s	
1676/2700 (epoch 31.037), train_loss = 0.16298414, grad/param norm = 8.0268e-02, time/batch = 0.4267s	
1677/2700 (epoch 31.056), train_loss = 0.14388155, grad/param norm = 6.9598e-02, time/batch = 0.4405s	
1678/2700 (epoch 31.074), train_loss = 0.14065934, grad/param norm = 6.7269e-02, time/batch = 0.4119s	
1679/2700 (epoch 31.093), train_loss = 0.14185745, grad/param norm = 7.3661e-02, time/batch = 0.3757s	
1680/2700 (epoch 31.111), train_loss = 0.13512618, grad/param norm = 6.8011e-02, time/batch = 0.4077s	
1681/2700 (epoch 31.130), train_loss = 0.14160071, grad/param norm = 7.3964e-02, time/batch = 0.4201s	
1682/2700 (epoch 31.148), train_loss = 0.13649109, grad/param norm = 6.8397e-02, time/batch = 0.4325s	
1683/2700 (epoch 31.167), train_loss = 0.14949582, grad/param norm = 7.3420e-02, time/batch = 0.4235s	
1684/2700 (epoch 31.185), train_loss = 0.13782533, grad/param norm = 7.2020e-02, time/batch = 0.3964s	
1685/2700 (epoch 31.204), train_loss = 0.14488795, grad/param norm = 7.3884e-02, time/batch = 0.4406s	
1686/2700 (epoch 31.222), train_loss = 0.13377418, grad/param norm = 8.2857e-02, time/batch = 0.4382s	
1687/2700 (epoch 31.241), train_loss = 0.14255403, grad/param norm = 8.9528e-02, time/batch = 0.4413s	
1688/2700 (epoch 31.259), train_loss = 0.15940924, grad/param norm = 9.4803e-02, time/batch = 0.4394s	
1689/2700 (epoch 31.278), train_loss = 0.17231630, grad/param norm = 8.8692e-02, time/batch = 0.4003s	
1690/2700 (epoch 31.296), train_loss = 0.15032651, grad/param norm = 7.8516e-02, time/batch = 0.3810s	
1691/2700 (epoch 31.315), train_loss = 0.13342117, grad/param norm = 6.9221e-02, time/batch = 0.3657s	
1692/2700 (epoch 31.333), train_loss = 0.13801942, grad/param norm = 7.9748e-02, time/batch = 0.4124s	
1693/2700 (epoch 31.352), train_loss = 0.13522516, grad/param norm = 7.9887e-02, time/batch = 0.4505s	
1694/2700 (epoch 31.370), train_loss = 0.12463161, grad/param norm = 7.4681e-02, time/batch = 0.4436s	
1695/2700 (epoch 31.389), train_loss = 0.13672416, grad/param norm = 7.4151e-02, time/batch = 0.3612s	
1696/2700 (epoch 31.407), train_loss = 0.14612344, grad/param norm = 6.2431e-02, time/batch = 0.4518s	
1697/2700 (epoch 31.426), train_loss = 0.13858619, grad/param norm = 6.8725e-02, time/batch = 0.4712s	
1698/2700 (epoch 31.444), train_loss = 0.13843999, grad/param norm = 6.9129e-02, time/batch = 0.4417s	
1699/2700 (epoch 31.463), train_loss = 0.15543966, grad/param norm = 8.0415e-02, time/batch = 0.4331s	
1700/2700 (epoch 31.481), train_loss = 0.14095349, grad/param norm = 8.0824e-02, time/batch = 0.4009s	
1701/2700 (epoch 31.500), train_loss = 0.13590618, grad/param norm = 7.9087e-02, time/batch = 0.3793s	
1702/2700 (epoch 31.519), train_loss = 0.14112073, grad/param norm = 7.4271e-02, time/batch = 0.3873s	
1703/2700 (epoch 31.537), train_loss = 0.13599467, grad/param norm = 7.4202e-02, time/batch = 0.4069s	
1704/2700 (epoch 31.556), train_loss = 0.13528361, grad/param norm = 7.6360e-02, time/batch = 0.4413s	
1705/2700 (epoch 31.574), train_loss = 0.14394914, grad/param norm = 7.9206e-02, time/batch = 0.4186s	
1706/2700 (epoch 31.593), train_loss = 0.15032397, grad/param norm = 7.6366e-02, time/batch = 0.3861s	
1707/2700 (epoch 31.611), train_loss = 0.15695702, grad/param norm = 9.1649e-02, time/batch = 0.4225s	
1708/2700 (epoch 31.630), train_loss = 0.13945051, grad/param norm = 7.6264e-02, time/batch = 0.4733s	
1709/2700 (epoch 31.648), train_loss = 0.15007216, grad/param norm = 7.8862e-02, time/batch = 0.4830s	
1710/2700 (epoch 31.667), train_loss = 0.15278413, grad/param norm = 8.1379e-02, time/batch = 0.4618s	
1711/2700 (epoch 31.685), train_loss = 0.15364303, grad/param norm = 8.6208e-02, time/batch = 0.4357s	
1712/2700 (epoch 31.704), train_loss = 0.16001286, grad/param norm = 9.6325e-02, time/batch = 0.3940s	
1713/2700 (epoch 31.722), train_loss = 0.16453030, grad/param norm = 9.1755e-02, time/batch = 0.4090s	
1714/2700 (epoch 31.741), train_loss = 0.16289497, grad/param norm = 8.2718e-02, time/batch = 0.4244s	
1715/2700 (epoch 31.759), train_loss = 0.13411184, grad/param norm = 7.4331e-02, time/batch = 0.4261s	
1716/2700 (epoch 31.778), train_loss = 0.13808703, grad/param norm = 7.2874e-02, time/batch = 0.4125s	
1717/2700 (epoch 31.796), train_loss = 0.12987342, grad/param norm = 7.6782e-02, time/batch = 0.3629s	
1718/2700 (epoch 31.815), train_loss = 0.13666083, grad/param norm = 7.9826e-02, time/batch = 0.4104s	
1719/2700 (epoch 31.833), train_loss = 0.13203711, grad/param norm = 6.7806e-02, time/batch = 0.4440s	
1720/2700 (epoch 31.852), train_loss = 0.14101003, grad/param norm = 7.8537e-02, time/batch = 0.4797s	
1721/2700 (epoch 31.870), train_loss = 0.14740405, grad/param norm = 7.4573e-02, time/batch = 0.4565s	
1722/2700 (epoch 31.889), train_loss = 0.13630427, grad/param norm = 7.4026e-02, time/batch = 0.4245s	
1723/2700 (epoch 31.907), train_loss = 0.12820970, grad/param norm = 7.3939e-02, time/batch = 0.3999s	
1724/2700 (epoch 31.926), train_loss = 0.13905699, grad/param norm = 7.4086e-02, time/batch = 0.4084s	
1725/2700 (epoch 31.944), train_loss = 0.12162775, grad/param norm = 6.6992e-02, time/batch = 0.4156s	
1726/2700 (epoch 31.963), train_loss = 0.13414356, grad/param norm = 6.7693e-02, time/batch = 0.4300s	
1727/2700 (epoch 31.981), train_loss = 0.12149946, grad/param norm = 5.9428e-02, time/batch = 0.4038s	
decayed learning rate by a factor 0.97 to 0.00099261282868397	
1728/2700 (epoch 32.000), train_loss = 0.12537882, grad/param norm = 6.7880e-02, time/batch = 0.3677s	
1729/2700 (epoch 32.019), train_loss = 0.17433354, grad/param norm = 9.6890e-02, time/batch = 0.4228s	
1730/2700 (epoch 32.037), train_loss = 0.14840280, grad/param norm = 7.8045e-02, time/batch = 0.4555s	
1731/2700 (epoch 32.056), train_loss = 0.13479015, grad/param norm = 7.2925e-02, time/batch = 0.4404s	
1732/2700 (epoch 32.074), train_loss = 0.13558029, grad/param norm = 6.9527e-02, time/batch = 0.4325s	
1733/2700 (epoch 32.093), train_loss = 0.12424564, grad/param norm = 6.8848e-02, time/batch = 0.4079s	
1734/2700 (epoch 32.111), train_loss = 0.11918607, grad/param norm = 6.8605e-02, time/batch = 0.4114s	
1735/2700 (epoch 32.130), train_loss = 0.12556054, grad/param norm = 6.8839e-02, time/batch = 0.4237s	
1736/2700 (epoch 32.148), train_loss = 0.12298972, grad/param norm = 6.4202e-02, time/batch = 0.4312s	
1737/2700 (epoch 32.167), train_loss = 0.12742658, grad/param norm = 7.0412e-02, time/batch = 0.4151s	
1738/2700 (epoch 32.185), train_loss = 0.12692679, grad/param norm = 7.9177e-02, time/batch = 0.3801s	
1739/2700 (epoch 32.204), train_loss = 0.13180638, grad/param norm = 7.5285e-02, time/batch = 0.3925s	
1740/2700 (epoch 32.222), train_loss = 0.11299673, grad/param norm = 7.3218e-02, time/batch = 0.4454s	
1741/2700 (epoch 32.241), train_loss = 0.12026914, grad/param norm = 6.9667e-02, time/batch = 0.4483s	
1742/2700 (epoch 32.259), train_loss = 0.13397416, grad/param norm = 7.8501e-02, time/batch = 0.4571s	
1743/2700 (epoch 32.278), train_loss = 0.14731718, grad/param norm = 8.9325e-02, time/batch = 0.4607s	
1744/2700 (epoch 32.296), train_loss = 0.13885522, grad/param norm = 8.4794e-02, time/batch = 0.3744s	
1745/2700 (epoch 32.315), train_loss = 0.11931019, grad/param norm = 7.0007e-02, time/batch = 0.4462s	
1746/2700 (epoch 32.333), train_loss = 0.11450943, grad/param norm = 6.1620e-02, time/batch = 0.4473s	
1747/2700 (epoch 32.352), train_loss = 0.10732509, grad/param norm = 6.1865e-02, time/batch = 0.4094s	
1748/2700 (epoch 32.370), train_loss = 0.10625690, grad/param norm = 6.1947e-02, time/batch = 0.3740s	
1749/2700 (epoch 32.389), train_loss = 0.12029061, grad/param norm = 6.9620e-02, time/batch = 0.3809s	
1750/2700 (epoch 32.407), train_loss = 0.13863048, grad/param norm = 7.2166e-02, time/batch = 0.4266s	
1751/2700 (epoch 32.426), train_loss = 0.12841373, grad/param norm = 6.8523e-02, time/batch = 0.4515s	
1752/2700 (epoch 32.444), train_loss = 0.12036100, grad/param norm = 6.2103e-02, time/batch = 0.4551s	
1753/2700 (epoch 32.463), train_loss = 0.13373239, grad/param norm = 6.5710e-02, time/batch = 0.4516s	
1754/2700 (epoch 32.481), train_loss = 0.11493970, grad/param norm = 6.2033e-02, time/batch = 0.4228s	
1755/2700 (epoch 32.500), train_loss = 0.11470994, grad/param norm = 6.6043e-02, time/batch = 0.4070s	
1756/2700 (epoch 32.519), train_loss = 0.12781491, grad/param norm = 7.4857e-02, time/batch = 0.4094s	
1757/2700 (epoch 32.537), train_loss = 0.12463883, grad/param norm = 7.2713e-02, time/batch = 0.4495s	
1758/2700 (epoch 32.556), train_loss = 0.11180227, grad/param norm = 6.3448e-02, time/batch = 0.4247s	
1759/2700 (epoch 32.574), train_loss = 0.12355771, grad/param norm = 6.6217e-02, time/batch = 0.3684s	
1760/2700 (epoch 32.593), train_loss = 0.12853523, grad/param norm = 6.9046e-02, time/batch = 0.3718s	
1761/2700 (epoch 32.611), train_loss = 0.12753346, grad/param norm = 6.3001e-02, time/batch = 0.3869s	
1762/2700 (epoch 32.630), train_loss = 0.12529641, grad/param norm = 8.3240e-02, time/batch = 0.4521s	
1763/2700 (epoch 32.648), train_loss = 0.13149735, grad/param norm = 7.1499e-02, time/batch = 0.4626s	
1764/2700 (epoch 32.667), train_loss = 0.12708557, grad/param norm = 6.1341e-02, time/batch = 0.4388s	
1765/2700 (epoch 32.685), train_loss = 0.11926982, grad/param norm = 6.5575e-02, time/batch = 0.4064s	
1766/2700 (epoch 32.704), train_loss = 0.13243982, grad/param norm = 7.7647e-02, time/batch = 0.4225s	
1767/2700 (epoch 32.722), train_loss = 0.13165424, grad/param norm = 6.8573e-02, time/batch = 0.4375s	
1768/2700 (epoch 32.741), train_loss = 0.13106006, grad/param norm = 6.7011e-02, time/batch = 0.4334s	
1769/2700 (epoch 32.759), train_loss = 0.11920602, grad/param norm = 6.5745e-02, time/batch = 0.3943s	
1770/2700 (epoch 32.778), train_loss = 0.12344096, grad/param norm = 6.6860e-02, time/batch = 0.3677s	
1771/2700 (epoch 32.796), train_loss = 0.10686048, grad/param norm = 6.5655e-02, time/batch = 0.3672s	
1772/2700 (epoch 32.815), train_loss = 0.11178923, grad/param norm = 6.2882e-02, time/batch = 0.4129s	
1773/2700 (epoch 32.833), train_loss = 0.11255563, grad/param norm = 6.0042e-02, time/batch = 0.4611s	
1774/2700 (epoch 32.852), train_loss = 0.11270001, grad/param norm = 6.1310e-02, time/batch = 0.4631s	
1775/2700 (epoch 32.870), train_loss = 0.12912267, grad/param norm = 6.7554e-02, time/batch = 0.4455s	
1776/2700 (epoch 32.889), train_loss = 0.11502621, grad/param norm = 6.4327e-02, time/batch = 0.4236s	
1777/2700 (epoch 32.907), train_loss = 0.10910100, grad/param norm = 6.3560e-02, time/batch = 0.4186s	
1778/2700 (epoch 32.926), train_loss = 0.11488954, grad/param norm = 5.8693e-02, time/batch = 0.4380s	
1779/2700 (epoch 32.944), train_loss = 0.09734881, grad/param norm = 5.6142e-02, time/batch = 0.4358s	
1780/2700 (epoch 32.963), train_loss = 0.12071567, grad/param norm = 6.1801e-02, time/batch = 0.3986s	
1781/2700 (epoch 32.981), train_loss = 0.10471266, grad/param norm = 5.6101e-02, time/batch = 0.3735s	
decayed learning rate by a factor 0.97 to 0.00096283444382345	
1782/2700 (epoch 33.000), train_loss = 0.10352380, grad/param norm = 6.1568e-02, time/batch = 0.3894s	
1783/2700 (epoch 33.019), train_loss = 0.15504754, grad/param norm = 7.0108e-02, time/batch = 0.3934s	
1784/2700 (epoch 33.037), train_loss = 0.12605255, grad/param norm = 6.5783e-02, time/batch = 0.4558s	
1785/2700 (epoch 33.056), train_loss = 0.11084084, grad/param norm = 6.8268e-02, time/batch = 0.4344s	
1786/2700 (epoch 33.074), train_loss = 0.11033214, grad/param norm = 6.0883e-02, time/batch = 0.4182s	
1787/2700 (epoch 33.093), train_loss = 0.10620802, grad/param norm = 6.9505e-02, time/batch = 0.4227s	
1788/2700 (epoch 33.111), train_loss = 0.10335033, grad/param norm = 6.5290e-02, time/batch = 0.4462s	
1789/2700 (epoch 33.130), train_loss = 0.10186774, grad/param norm = 5.6323e-02, time/batch = 0.4370s	
1790/2700 (epoch 33.148), train_loss = 0.10361962, grad/param norm = 5.8873e-02, time/batch = 0.4429s	
1791/2700 (epoch 33.167), train_loss = 0.10936167, grad/param norm = 6.0639e-02, time/batch = 0.4559s	
1792/2700 (epoch 33.185), train_loss = 0.10509468, grad/param norm = 6.1832e-02, time/batch = 0.4106s	
1793/2700 (epoch 33.204), train_loss = 0.11450091, grad/param norm = 7.4532e-02, time/batch = 0.3630s	
1794/2700 (epoch 33.222), train_loss = 0.10022868, grad/param norm = 6.7422e-02, time/batch = 0.3852s	
1795/2700 (epoch 33.241), train_loss = 0.10388806, grad/param norm = 5.5951e-02, time/batch = 0.4425s	
1796/2700 (epoch 33.259), train_loss = 0.10825547, grad/param norm = 5.4729e-02, time/batch = 0.4183s	
1797/2700 (epoch 33.278), train_loss = 0.11722719, grad/param norm = 5.9355e-02, time/batch = 0.4061s	
1798/2700 (epoch 33.296), train_loss = 0.11275839, grad/param norm = 6.7243e-02, time/batch = 0.4405s	
1799/2700 (epoch 33.315), train_loss = 0.10799975, grad/param norm = 6.4252e-02, time/batch = 0.4619s	
1800/2700 (epoch 33.333), train_loss = 0.10879730, grad/param norm = 6.7209e-02, time/batch = 0.4746s	
1801/2700 (epoch 33.352), train_loss = 0.10255995, grad/param norm = 6.5807e-02, time/batch = 0.4551s	
1802/2700 (epoch 33.370), train_loss = 0.09537763, grad/param norm = 6.2631e-02, time/batch = 0.4352s	
1803/2700 (epoch 33.389), train_loss = 0.10514528, grad/param norm = 6.0569e-02, time/batch = 0.4170s	
1804/2700 (epoch 33.407), train_loss = 0.11484293, grad/param norm = 5.4509e-02, time/batch = 0.3853s	
1805/2700 (epoch 33.426), train_loss = 0.10899793, grad/param norm = 6.8152e-02, time/batch = 0.3584s	
1806/2700 (epoch 33.444), train_loss = 0.10571823, grad/param norm = 6.2497e-02, time/batch = 0.4433s	
1807/2700 (epoch 33.463), train_loss = 0.11744392, grad/param norm = 6.4268e-02, time/batch = 0.4085s	
1808/2700 (epoch 33.481), train_loss = 0.10249234, grad/param norm = 5.7197e-02, time/batch = 0.4091s	
1809/2700 (epoch 33.500), train_loss = 0.09571711, grad/param norm = 5.9360e-02, time/batch = 0.4453s	
1810/2700 (epoch 33.519), train_loss = 0.10822140, grad/param norm = 6.3311e-02, time/batch = 0.4632s	
1811/2700 (epoch 33.537), train_loss = 0.10470709, grad/param norm = 5.8150e-02, time/batch = 0.4625s	
1812/2700 (epoch 33.556), train_loss = 0.09647638, grad/param norm = 5.4609e-02, time/batch = 0.4487s	
1813/2700 (epoch 33.574), train_loss = 0.10544459, grad/param norm = 5.4549e-02, time/batch = 0.4300s	
1814/2700 (epoch 33.593), train_loss = 0.10546384, grad/param norm = 5.6860e-02, time/batch = 0.3901s	
1815/2700 (epoch 33.611), train_loss = 0.11426631, grad/param norm = 6.1373e-02, time/batch = 0.3902s	
1816/2700 (epoch 33.630), train_loss = 0.09885449, grad/param norm = 5.8333e-02, time/batch = 0.3965s	
1817/2700 (epoch 33.648), train_loss = 0.11030124, grad/param norm = 6.0403e-02, time/batch = 0.4510s	
1818/2700 (epoch 33.667), train_loss = 0.11463568, grad/param norm = 6.2300e-02, time/batch = 0.3730s	
1819/2700 (epoch 33.685), train_loss = 0.10180012, grad/param norm = 5.7946e-02, time/batch = 0.4479s	
1820/2700 (epoch 33.704), train_loss = 0.11006767, grad/param norm = 6.3393e-02, time/batch = 0.4669s	
1821/2700 (epoch 33.722), train_loss = 0.11045717, grad/param norm = 5.9673e-02, time/batch = 0.4563s	
1822/2700 (epoch 33.741), train_loss = 0.11020903, grad/param norm = 5.4544e-02, time/batch = 0.4580s	
1823/2700 (epoch 33.759), train_loss = 0.10125409, grad/param norm = 5.8580e-02, time/batch = 0.4388s	
1824/2700 (epoch 33.778), train_loss = 0.09872379, grad/param norm = 5.6717e-02, time/batch = 0.3976s	
1825/2700 (epoch 33.796), train_loss = 0.09448303, grad/param norm = 6.0768e-02, time/batch = 0.3992s	
1826/2700 (epoch 33.815), train_loss = 0.10787243, grad/param norm = 6.6553e-02, time/batch = 0.4113s	
1827/2700 (epoch 33.833), train_loss = 0.11046769, grad/param norm = 6.8345e-02, time/batch = 0.4112s	
1828/2700 (epoch 33.852), train_loss = 0.11024293, grad/param norm = 6.7381e-02, time/batch = 0.4290s	
1829/2700 (epoch 33.870), train_loss = 0.11152435, grad/param norm = 6.5235e-02, time/batch = 0.4001s	
1830/2700 (epoch 33.889), train_loss = 0.10363579, grad/param norm = 6.3715e-02, time/batch = 0.4142s	
1831/2700 (epoch 33.907), train_loss = 0.09675753, grad/param norm = 5.9300e-02, time/batch = 0.4556s	
1832/2700 (epoch 33.926), train_loss = 0.10138855, grad/param norm = 5.5201e-02, time/batch = 0.4558s	
1833/2700 (epoch 33.944), train_loss = 0.09138152, grad/param norm = 4.9855e-02, time/batch = 0.4398s	
1834/2700 (epoch 33.963), train_loss = 0.10015663, grad/param norm = 5.8589e-02, time/batch = 0.4161s	
1835/2700 (epoch 33.981), train_loss = 0.10431182, grad/param norm = 6.1372e-02, time/batch = 0.3844s	
decayed learning rate by a factor 0.97 to 0.00093394941050874	
1836/2700 (epoch 34.000), train_loss = 0.09043257, grad/param norm = 5.3436e-02, time/batch = 0.3972s	
1837/2700 (epoch 34.019), train_loss = 0.14645305, grad/param norm = 7.5676e-02, time/batch = 0.4181s	
1838/2700 (epoch 34.037), train_loss = 0.11615846, grad/param norm = 6.5291e-02, time/batch = 0.4219s	
1839/2700 (epoch 34.056), train_loss = 0.09734319, grad/param norm = 5.8016e-02, time/batch = 0.4178s	
1840/2700 (epoch 34.074), train_loss = 0.10540542, grad/param norm = 6.4179e-02, time/batch = 0.4137s	
1841/2700 (epoch 34.093), train_loss = 0.09685542, grad/param norm = 6.1100e-02, time/batch = 0.4160s	
1842/2700 (epoch 34.111), train_loss = 0.09042731, grad/param norm = 5.9945e-02, time/batch = 0.4384s	
1843/2700 (epoch 34.130), train_loss = 0.09582349, grad/param norm = 5.8668e-02, time/batch = 0.4555s	
1844/2700 (epoch 34.148), train_loss = 0.09189807, grad/param norm = 5.4521e-02, time/batch = 0.4111s	
1845/2700 (epoch 34.167), train_loss = 0.09589442, grad/param norm = 5.4192e-02, time/batch = 0.3577s	
1846/2700 (epoch 34.185), train_loss = 0.08911670, grad/param norm = 5.2383e-02, time/batch = 0.3896s	
1847/2700 (epoch 34.204), train_loss = 0.09672802, grad/param norm = 5.9223e-02, time/batch = 0.4380s	
1848/2700 (epoch 34.222), train_loss = 0.08492048, grad/param norm = 6.3808e-02, time/batch = 0.4528s	
1849/2700 (epoch 34.241), train_loss = 0.09788608, grad/param norm = 6.3922e-02, time/batch = 0.4237s	
1850/2700 (epoch 34.259), train_loss = 0.10205595, grad/param norm = 5.8986e-02, time/batch = 0.4035s	
1851/2700 (epoch 34.278), train_loss = 0.11399839, grad/param norm = 5.8553e-02, time/batch = 0.4065s	
1852/2700 (epoch 34.296), train_loss = 0.09615694, grad/param norm = 5.4250e-02, time/batch = 0.4315s	
1853/2700 (epoch 34.315), train_loss = 0.08424238, grad/param norm = 5.1664e-02, time/batch = 0.4459s	
1854/2700 (epoch 34.333), train_loss = 0.08369440, grad/param norm = 4.9508e-02, time/batch = 0.4475s	
1855/2700 (epoch 34.352), train_loss = 0.08875549, grad/param norm = 5.7774e-02, time/batch = 0.4020s	
1856/2700 (epoch 34.370), train_loss = 0.07936063, grad/param norm = 5.9719e-02, time/batch = 0.3631s	
1857/2700 (epoch 34.389), train_loss = 0.09670169, grad/param norm = 5.7520e-02, time/batch = 0.3968s	
1858/2700 (epoch 34.407), train_loss = 0.10584821, grad/param norm = 5.8655e-02, time/batch = 0.4430s	
1859/2700 (epoch 34.426), train_loss = 0.10150239, grad/param norm = 6.5129e-02, time/batch = 0.4531s	
1860/2700 (epoch 34.444), train_loss = 0.09278619, grad/param norm = 5.5686e-02, time/batch = 0.4209s	
1861/2700 (epoch 34.463), train_loss = 0.10471617, grad/param norm = 6.2133e-02, time/batch = 0.4245s	
1862/2700 (epoch 34.481), train_loss = 0.08971781, grad/param norm = 5.3153e-02, time/batch = 0.4034s	
1863/2700 (epoch 34.500), train_loss = 0.08705938, grad/param norm = 5.8065e-02, time/batch = 0.4303s	
1864/2700 (epoch 34.519), train_loss = 0.10290913, grad/param norm = 6.4259e-02, time/batch = 0.4580s	
1865/2700 (epoch 34.537), train_loss = 0.09298367, grad/param norm = 5.7045e-02, time/batch = 0.4556s	
1866/2700 (epoch 34.556), train_loss = 0.09003237, grad/param norm = 6.4455e-02, time/batch = 0.4246s	
1867/2700 (epoch 34.574), train_loss = 0.09977744, grad/param norm = 5.8187e-02, time/batch = 0.3358s	
1868/2700 (epoch 34.593), train_loss = 0.09433564, grad/param norm = 5.2326e-02, time/batch = 0.4067s	
1869/2700 (epoch 34.611), train_loss = 0.09809238, grad/param norm = 5.4888e-02, time/batch = 0.4297s	
1870/2700 (epoch 34.630), train_loss = 0.09057693, grad/param norm = 5.5097e-02, time/batch = 0.4047s	
1871/2700 (epoch 34.648), train_loss = 0.09772419, grad/param norm = 5.4032e-02, time/batch = 0.3717s	
1872/2700 (epoch 34.667), train_loss = 0.10388399, grad/param norm = 6.0990e-02, time/batch = 0.4127s	
1873/2700 (epoch 34.685), train_loss = 0.09375301, grad/param norm = 5.7655e-02, time/batch = 0.4553s	
1874/2700 (epoch 34.704), train_loss = 0.09692425, grad/param norm = 5.5742e-02, time/batch = 0.4684s	
1875/2700 (epoch 34.722), train_loss = 0.09485535, grad/param norm = 5.6811e-02, time/batch = 0.4730s	
1876/2700 (epoch 34.741), train_loss = 0.09746669, grad/param norm = 5.1803e-02, time/batch = 0.4372s	
1877/2700 (epoch 34.759), train_loss = 0.08566974, grad/param norm = 5.2126e-02, time/batch = 0.3844s	
1878/2700 (epoch 34.778), train_loss = 0.09140409, grad/param norm = 5.0648e-02, time/batch = 0.3780s	
1879/2700 (epoch 34.796), train_loss = 0.08325373, grad/param norm = 5.0184e-02, time/batch = 0.4061s	
1880/2700 (epoch 34.815), train_loss = 0.09292928, grad/param norm = 6.1936e-02, time/batch = 0.4300s	
1881/2700 (epoch 34.833), train_loss = 0.09552599, grad/param norm = 6.5873e-02, time/batch = 0.4026s	
1882/2700 (epoch 34.852), train_loss = 0.10611932, grad/param norm = 7.5654e-02, time/batch = 0.3724s	
1883/2700 (epoch 34.870), train_loss = 0.11137632, grad/param norm = 7.8574e-02, time/batch = 0.4280s	
1884/2700 (epoch 34.889), train_loss = 0.10349046, grad/param norm = 7.2262e-02, time/batch = 0.4536s	
1885/2700 (epoch 34.907), train_loss = 0.09851693, grad/param norm = 7.2263e-02, time/batch = 0.4755s	
1886/2700 (epoch 34.926), train_loss = 0.10867158, grad/param norm = 7.2031e-02, time/batch = 0.4821s	
1887/2700 (epoch 34.944), train_loss = 0.09987329, grad/param norm = 6.5179e-02, time/batch = 0.4158s	
1888/2700 (epoch 34.963), train_loss = 0.09940834, grad/param norm = 5.9216e-02, time/batch = 0.3823s	
1889/2700 (epoch 34.981), train_loss = 0.08845049, grad/param norm = 5.4505e-02, time/batch = 0.3864s	
decayed learning rate by a factor 0.97 to 0.00090593092819348	
1890/2700 (epoch 35.000), train_loss = 0.08553788, grad/param norm = 5.3231e-02, time/batch = 0.4168s	
1891/2700 (epoch 35.019), train_loss = 0.12661900, grad/param norm = 6.2542e-02, time/batch = 0.4075s	
1892/2700 (epoch 35.037), train_loss = 0.10499367, grad/param norm = 5.7144e-02, time/batch = 0.3984s	
1893/2700 (epoch 35.056), train_loss = 0.09132595, grad/param norm = 6.0460e-02, time/batch = 0.3920s	
1894/2700 (epoch 35.074), train_loss = 0.09309489, grad/param norm = 6.0010e-02, time/batch = 0.4371s	
1895/2700 (epoch 35.093), train_loss = 0.08042054, grad/param norm = 5.1152e-02, time/batch = 0.4601s	
1896/2700 (epoch 35.111), train_loss = 0.07983638, grad/param norm = 5.6399e-02, time/batch = 0.4528s	
1897/2700 (epoch 35.130), train_loss = 0.08373971, grad/param norm = 5.3528e-02, time/batch = 0.4435s	
1898/2700 (epoch 35.148), train_loss = 0.08112315, grad/param norm = 4.8475e-02, time/batch = 0.3615s	
1899/2700 (epoch 35.167), train_loss = 0.08754957, grad/param norm = 5.5165e-02, time/batch = 0.3803s	
1900/2700 (epoch 35.185), train_loss = 0.08521790, grad/param norm = 6.4541e-02, time/batch = 0.4285s	
1901/2700 (epoch 35.204), train_loss = 0.09190304, grad/param norm = 6.0375e-02, time/batch = 0.4267s	
1902/2700 (epoch 35.222), train_loss = 0.07797425, grad/param norm = 6.1986e-02, time/batch = 0.4429s	
1903/2700 (epoch 35.241), train_loss = 0.08854949, grad/param norm = 6.1561e-02, time/batch = 0.4148s	
1904/2700 (epoch 35.259), train_loss = 0.10220317, grad/param norm = 7.3239e-02, time/batch = 0.3815s	
1905/2700 (epoch 35.278), train_loss = 0.09979566, grad/param norm = 6.2090e-02, time/batch = 0.4646s	
1906/2700 (epoch 35.296), train_loss = 0.09316583, grad/param norm = 5.8794e-02, time/batch = 0.4639s	
1907/2700 (epoch 35.315), train_loss = 0.08019745, grad/param norm = 5.1621e-02, time/batch = 0.4240s	
1908/2700 (epoch 35.333), train_loss = 0.08006330, grad/param norm = 5.2557e-02, time/batch = 0.3800s	
1909/2700 (epoch 35.352), train_loss = 0.07560569, grad/param norm = 5.0599e-02, time/batch = 0.3805s	
1910/2700 (epoch 35.370), train_loss = 0.06843627, grad/param norm = 4.7981e-02, time/batch = 0.4202s	
1911/2700 (epoch 35.389), train_loss = 0.08287694, grad/param norm = 5.4367e-02, time/batch = 0.4194s	
1912/2700 (epoch 35.407), train_loss = 0.10143358, grad/param norm = 6.2097e-02, time/batch = 0.4306s	
1913/2700 (epoch 35.426), train_loss = 0.09319175, grad/param norm = 6.6613e-02, time/batch = 0.4082s	
1914/2700 (epoch 35.444), train_loss = 0.08672257, grad/param norm = 5.4092e-02, time/batch = 0.4084s	
1915/2700 (epoch 35.463), train_loss = 0.09484313, grad/param norm = 6.1880e-02, time/batch = 0.4316s	
1916/2700 (epoch 35.481), train_loss = 0.08867489, grad/param norm = 5.6503e-02, time/batch = 0.4346s	
1917/2700 (epoch 35.500), train_loss = 0.07949092, grad/param norm = 5.2126e-02, time/batch = 0.4673s	
1918/2700 (epoch 35.519), train_loss = 0.09311145, grad/param norm = 5.4411e-02, time/batch = 0.4574s	
1919/2700 (epoch 35.537), train_loss = 0.08424193, grad/param norm = 4.9178e-02, time/batch = 0.4261s	
1920/2700 (epoch 35.556), train_loss = 0.07777348, grad/param norm = 5.2380e-02, time/batch = 0.3865s	
1921/2700 (epoch 35.574), train_loss = 0.09179722, grad/param norm = 5.6047e-02, time/batch = 0.3832s	
1922/2700 (epoch 35.593), train_loss = 0.08796930, grad/param norm = 4.8405e-02, time/batch = 0.3937s	
1923/2700 (epoch 35.611), train_loss = 0.08979407, grad/param norm = 4.6908e-02, time/batch = 0.4163s	
1924/2700 (epoch 35.630), train_loss = 0.07655637, grad/param norm = 5.1450e-02, time/batch = 0.4425s	
1925/2700 (epoch 35.648), train_loss = 0.08863272, grad/param norm = 4.8527e-02, time/batch = 0.4158s	
1926/2700 (epoch 35.667), train_loss = 0.08346531, grad/param norm = 5.1061e-02, time/batch = 0.3945s	
1927/2700 (epoch 35.685), train_loss = 0.08107540, grad/param norm = 4.7443e-02, time/batch = 0.4388s	
1928/2700 (epoch 35.704), train_loss = 0.07999406, grad/param norm = 4.8528e-02, time/batch = 0.4455s	
1929/2700 (epoch 35.722), train_loss = 0.08130180, grad/param norm = 4.4903e-02, time/batch = 0.4257s	
1930/2700 (epoch 35.741), train_loss = 0.08412954, grad/param norm = 4.0669e-02, time/batch = 0.3911s	
1931/2700 (epoch 35.759), train_loss = 0.07954480, grad/param norm = 4.5149e-02, time/batch = 0.3963s	
1932/2700 (epoch 35.778), train_loss = 0.08367300, grad/param norm = 4.3893e-02, time/batch = 0.3738s	
1933/2700 (epoch 35.796), train_loss = 0.07035398, grad/param norm = 5.0383e-02, time/batch = 0.4073s	
1934/2700 (epoch 35.815), train_loss = 0.07488691, grad/param norm = 4.6254e-02, time/batch = 0.4383s	
1935/2700 (epoch 35.833), train_loss = 0.07790738, grad/param norm = 5.1579e-02, time/batch = 0.4370s	
1936/2700 (epoch 35.852), train_loss = 0.08155615, grad/param norm = 5.2796e-02, time/batch = 0.4058s	
1937/2700 (epoch 35.870), train_loss = 0.08952552, grad/param norm = 6.2045e-02, time/batch = 0.4008s	
1938/2700 (epoch 35.889), train_loss = 0.07915194, grad/param norm = 4.9120e-02, time/batch = 0.4332s	
1939/2700 (epoch 35.907), train_loss = 0.07558784, grad/param norm = 5.3423e-02, time/batch = 0.4422s	
1940/2700 (epoch 35.926), train_loss = 0.08817304, grad/param norm = 6.2522e-02, time/batch = 0.4639s	
1941/2700 (epoch 35.944), train_loss = 0.08380108, grad/param norm = 6.0297e-02, time/batch = 0.4464s	
1942/2700 (epoch 35.963), train_loss = 0.08989413, grad/param norm = 6.1056e-02, time/batch = 0.4079s	
1943/2700 (epoch 35.981), train_loss = 0.08387584, grad/param norm = 5.9471e-02, time/batch = 0.4118s	
decayed learning rate by a factor 0.97 to 0.00087875300034768	
1944/2700 (epoch 36.000), train_loss = 0.08214131, grad/param norm = 5.7027e-02, time/batch = 0.4226s	
1945/2700 (epoch 36.019), train_loss = 0.11719034, grad/param norm = 6.0864e-02, time/batch = 0.4335s	
1946/2700 (epoch 36.037), train_loss = 0.09363481, grad/param norm = 5.4782e-02, time/batch = 0.4379s	
1947/2700 (epoch 36.056), train_loss = 0.07766188, grad/param norm = 4.8820e-02, time/batch = 0.4003s	
1948/2700 (epoch 36.074), train_loss = 0.08235056, grad/param norm = 4.7162e-02, time/batch = 0.3532s	
1949/2700 (epoch 36.093), train_loss = 0.07304391, grad/param norm = 5.1033e-02, time/batch = 0.4228s	
1950/2700 (epoch 36.111), train_loss = 0.06673854, grad/param norm = 4.5991e-02, time/batch = 0.4557s	
1951/2700 (epoch 36.130), train_loss = 0.07794030, grad/param norm = 5.6654e-02, time/batch = 0.4532s	
1952/2700 (epoch 36.148), train_loss = 0.07413377, grad/param norm = 5.1357e-02, time/batch = 0.4544s	
1953/2700 (epoch 36.167), train_loss = 0.08081968, grad/param norm = 4.6457e-02, time/batch = 0.4180s	
1954/2700 (epoch 36.185), train_loss = 0.07119148, grad/param norm = 4.5502e-02, time/batch = 0.4105s	
1955/2700 (epoch 36.204), train_loss = 0.07229100, grad/param norm = 4.8232e-02, time/batch = 0.4184s	
1956/2700 (epoch 36.222), train_loss = 0.06242378, grad/param norm = 4.3582e-02, time/batch = 0.4202s	
1957/2700 (epoch 36.241), train_loss = 0.07254210, grad/param norm = 4.7785e-02, time/batch = 0.4353s	
1958/2700 (epoch 36.259), train_loss = 0.07822511, grad/param norm = 4.5832e-02, time/batch = 0.4044s	
1959/2700 (epoch 36.278), train_loss = 0.08899217, grad/param norm = 5.2733e-02, time/batch = 0.3652s	
1960/2700 (epoch 36.296), train_loss = 0.07563953, grad/param norm = 4.8111e-02, time/batch = 0.4227s	
1961/2700 (epoch 36.315), train_loss = 0.07331360, grad/param norm = 4.4240e-02, time/batch = 0.4341s	
1962/2700 (epoch 36.333), train_loss = 0.06866263, grad/param norm = 5.1919e-02, time/batch = 0.4560s	
1963/2700 (epoch 36.352), train_loss = 0.06765852, grad/param norm = 4.8860e-02, time/batch = 0.4717s	
1964/2700 (epoch 36.370), train_loss = 0.06290623, grad/param norm = 4.7120e-02, time/batch = 0.4649s	
1965/2700 (epoch 36.389), train_loss = 0.07206980, grad/param norm = 4.6995e-02, time/batch = 0.3745s	
1966/2700 (epoch 36.407), train_loss = 0.08171414, grad/param norm = 4.6741e-02, time/batch = 0.4312s	
1967/2700 (epoch 36.426), train_loss = 0.08057486, grad/param norm = 5.7593e-02, time/batch = 0.4338s	
1968/2700 (epoch 36.444), train_loss = 0.07481021, grad/param norm = 4.8850e-02, time/batch = 0.4336s	
1969/2700 (epoch 36.463), train_loss = 0.08505015, grad/param norm = 5.3994e-02, time/batch = 0.3889s	
1970/2700 (epoch 36.481), train_loss = 0.07669045, grad/param norm = 5.6158e-02, time/batch = 0.3589s	
1971/2700 (epoch 36.500), train_loss = 0.06867792, grad/param norm = 5.3426e-02, time/batch = 0.4088s	
1972/2700 (epoch 36.519), train_loss = 0.08535910, grad/param norm = 5.4399e-02, time/batch = 0.4469s	
1973/2700 (epoch 36.537), train_loss = 0.07258830, grad/param norm = 4.7954e-02, time/batch = 0.4712s	
1974/2700 (epoch 36.556), train_loss = 0.07396765, grad/param norm = 5.0637e-02, time/batch = 0.4770s	
1975/2700 (epoch 36.574), train_loss = 0.08191110, grad/param norm = 4.9874e-02, time/batch = 0.4535s	
1976/2700 (epoch 36.593), train_loss = 0.08141574, grad/param norm = 5.2829e-02, time/batch = 0.4267s	
1977/2700 (epoch 36.611), train_loss = 0.08823760, grad/param norm = 5.2144e-02, time/batch = 0.3842s	
1978/2700 (epoch 36.630), train_loss = 0.07248454, grad/param norm = 5.2015e-02, time/batch = 0.4457s	
1979/2700 (epoch 36.648), train_loss = 0.08525942, grad/param norm = 5.1406e-02, time/batch = 0.4414s	
1980/2700 (epoch 36.667), train_loss = 0.07889658, grad/param norm = 4.1563e-02, time/batch = 0.4161s	
1981/2700 (epoch 36.685), train_loss = 0.07216069, grad/param norm = 4.2987e-02, time/batch = 0.3843s	
1982/2700 (epoch 36.704), train_loss = 0.07744163, grad/param norm = 4.8642e-02, time/batch = 0.3880s	
1983/2700 (epoch 36.722), train_loss = 0.07388163, grad/param norm = 4.6065e-02, time/batch = 0.3821s	
1984/2700 (epoch 36.741), train_loss = 0.07515637, grad/param norm = 4.4615e-02, time/batch = 0.4405s	
1985/2700 (epoch 36.759), train_loss = 0.06760186, grad/param norm = 4.3782e-02, time/batch = 0.4643s	
1986/2700 (epoch 36.778), train_loss = 0.06845481, grad/param norm = 4.0974e-02, time/batch = 0.4661s	
1987/2700 (epoch 36.796), train_loss = 0.06990175, grad/param norm = 4.8784e-02, time/batch = 0.4462s	
1988/2700 (epoch 36.815), train_loss = 0.07052957, grad/param norm = 4.8101e-02, time/batch = 0.4104s	
1989/2700 (epoch 36.833), train_loss = 0.07119764, grad/param norm = 4.2912e-02, time/batch = 0.3943s	
1990/2700 (epoch 36.852), train_loss = 0.07042424, grad/param norm = 4.1053e-02, time/batch = 0.4520s	
1991/2700 (epoch 36.870), train_loss = 0.07564762, grad/param norm = 4.2430e-02, time/batch = 0.4464s	
1992/2700 (epoch 36.889), train_loss = 0.07265939, grad/param norm = 4.6509e-02, time/batch = 0.4083s	
1993/2700 (epoch 36.907), train_loss = 0.06873919, grad/param norm = 5.3491e-02, time/batch = 0.3771s	
1994/2700 (epoch 36.926), train_loss = 0.07846906, grad/param norm = 5.3739e-02, time/batch = 0.3318s	
1995/2700 (epoch 36.944), train_loss = 0.06864951, grad/param norm = 5.1059e-02, time/batch = 0.3909s	
1996/2700 (epoch 36.963), train_loss = 0.07888493, grad/param norm = 5.5209e-02, time/batch = 0.4248s	
1997/2700 (epoch 36.981), train_loss = 0.07853553, grad/param norm = 5.2304e-02, time/batch = 0.4540s	
decayed learning rate by a factor 0.97 to 0.00085239041033725	
1998/2700 (epoch 37.000), train_loss = 0.07013991, grad/param norm = 5.3715e-02, time/batch = 0.4594s	
1999/2700 (epoch 37.019), train_loss = 0.11045554, grad/param norm = 6.2799e-02, time/batch = 0.4325s	
evaluating loss over split index 2	
1/3...	
2/3...	
3/3...	
saving checkpoint to cv/lm_lstm_epoch37.04_2.7356.t7	
2000/2700 (epoch 37.037), train_loss = 0.08547394, grad/param norm = 5.0917e-02, time/batch = 0.4210s	
2001/2700 (epoch 37.056), train_loss = 0.92866512, grad/param norm = 2.1134e-01, time/batch = 0.4016s	
2002/2700 (epoch 37.074), train_loss = 0.14722988, grad/param norm = 1.2523e-01, time/batch = 0.3981s	
2003/2700 (epoch 37.093), train_loss = 0.10702411, grad/param norm = 7.7465e-02, time/batch = 0.3994s	
2004/2700 (epoch 37.111), train_loss = 0.08915424, grad/param norm = 7.2596e-02, time/batch = 0.4351s	
2005/2700 (epoch 37.130), train_loss = 0.07972927, grad/param norm = 5.9175e-02, time/batch = 0.4611s	
2006/2700 (epoch 37.148), train_loss = 0.07460681, grad/param norm = 5.3052e-02, time/batch = 0.4749s	
2007/2700 (epoch 37.167), train_loss = 0.08981870, grad/param norm = 6.0887e-02, time/batch = 0.4762s	
2008/2700 (epoch 37.185), train_loss = 0.07804420, grad/param norm = 5.5129e-02, time/batch = 0.4528s	
2009/2700 (epoch 37.204), train_loss = 0.07717357, grad/param norm = 5.3014e-02, time/batch = 0.4214s	
2010/2700 (epoch 37.222), train_loss = 0.06183055, grad/param norm = 5.2749e-02, time/batch = 0.4120s	
2011/2700 (epoch 37.241), train_loss = 0.06952162, grad/param norm = 4.3693e-02, time/batch = 0.4019s	
2012/2700 (epoch 37.259), train_loss = 0.07483278, grad/param norm = 4.3946e-02, time/batch = 0.3909s	
2013/2700 (epoch 37.278), train_loss = 0.08445952, grad/param norm = 5.3703e-02, time/batch = 0.4211s	
2014/2700 (epoch 37.296), train_loss = 0.07765237, grad/param norm = 5.8286e-02, time/batch = 0.3945s	
2015/2700 (epoch 37.315), train_loss = 0.06976854, grad/param norm = 5.0161e-02, time/batch = 0.4145s	
2016/2700 (epoch 37.333), train_loss = 0.07404045, grad/param norm = 5.7266e-02, time/batch = 0.4544s	
2017/2700 (epoch 37.352), train_loss = 0.06207986, grad/param norm = 4.5350e-02, time/batch = 0.4706s	
2018/2700 (epoch 37.370), train_loss = 0.06039749, grad/param norm = 4.6088e-02, time/batch = 0.4814s	
2019/2700 (epoch 37.389), train_loss = 0.06805612, grad/param norm = 4.7615e-02, time/batch = 0.4717s	
2020/2700 (epoch 37.407), train_loss = 0.07489630, grad/param norm = 4.9936e-02, time/batch = 0.4358s	
2021/2700 (epoch 37.426), train_loss = 0.07081324, grad/param norm = 5.1216e-02, time/batch = 0.4227s	
2022/2700 (epoch 37.444), train_loss = 0.07230340, grad/param norm = 4.5384e-02, time/batch = 0.3914s	
2023/2700 (epoch 37.463), train_loss = 0.07465634, grad/param norm = 5.3917e-02, time/batch = 0.3698s	
2024/2700 (epoch 37.481), train_loss = 0.06806063, grad/param norm = 5.0261e-02, time/batch = 0.4260s	
2025/2700 (epoch 37.500), train_loss = 0.06853201, grad/param norm = 5.8591e-02, time/batch = 0.4224s	
2026/2700 (epoch 37.519), train_loss = 0.07473486, grad/param norm = 5.1497e-02, time/batch = 0.3928s	
2027/2700 (epoch 37.537), train_loss = 0.07106697, grad/param norm = 4.7220e-02, time/batch = 0.4155s	
2028/2700 (epoch 37.556), train_loss = 0.06263789, grad/param norm = 5.2860e-02, time/batch = 0.4539s	
2029/2700 (epoch 37.574), train_loss = 0.07522885, grad/param norm = 5.0423e-02, time/batch = 0.4691s	
2030/2700 (epoch 37.593), train_loss = 0.07423313, grad/param norm = 5.0035e-02, time/batch = 0.4820s	
2031/2700 (epoch 37.611), train_loss = 0.08114101, grad/param norm = 5.2708e-02, time/batch = 0.4604s	
2032/2700 (epoch 37.630), train_loss = 0.06712017, grad/param norm = 5.0182e-02, time/batch = 0.4450s	
2033/2700 (epoch 37.648), train_loss = 0.07900999, grad/param norm = 4.9491e-02, time/batch = 0.4069s	
2034/2700 (epoch 37.667), train_loss = 0.07576748, grad/param norm = 4.7325e-02, time/batch = 0.3994s	
2035/2700 (epoch 37.685), train_loss = 0.06749072, grad/param norm = 4.0185e-02, time/batch = 0.3992s	
2036/2700 (epoch 37.704), train_loss = 0.06352118, grad/param norm = 4.0259e-02, time/batch = 0.4381s	
2037/2700 (epoch 37.722), train_loss = 0.06806822, grad/param norm = 4.0909e-02, time/batch = 0.3958s	
2038/2700 (epoch 37.741), train_loss = 0.07268404, grad/param norm = 4.3219e-02, time/batch = 0.3774s	
2039/2700 (epoch 37.759), train_loss = 0.06574625, grad/param norm = 4.7504e-02, time/batch = 0.4269s	
2040/2700 (epoch 37.778), train_loss = 0.06765223, grad/param norm = 4.2894e-02, time/batch = 0.4581s	
2041/2700 (epoch 37.796), train_loss = 0.06062288, grad/param norm = 4.2058e-02, time/batch = 0.4500s	
2042/2700 (epoch 37.815), train_loss = 0.06173661, grad/param norm = 4.4379e-02, time/batch = 0.4564s	
2043/2700 (epoch 37.833), train_loss = 0.06444915, grad/param norm = 4.2361e-02, time/batch = 0.4588s	
2044/2700 (epoch 37.852), train_loss = 0.06415884, grad/param norm = 3.8535e-02, time/batch = 0.4313s	
2045/2700 (epoch 37.870), train_loss = 0.06476400, grad/param norm = 4.1055e-02, time/batch = 0.4166s	
2046/2700 (epoch 37.889), train_loss = 0.06173434, grad/param norm = 4.2627e-02, time/batch = 0.4070s	
2047/2700 (epoch 37.907), train_loss = 0.05483126, grad/param norm = 4.1655e-02, time/batch = 0.4032s	
2048/2700 (epoch 37.926), train_loss = 0.07178142, grad/param norm = 5.5483e-02, time/batch = 0.4416s	
2049/2700 (epoch 37.944), train_loss = 0.06565124, grad/param norm = 5.1692e-02, time/batch = 0.3976s	
2050/2700 (epoch 37.963), train_loss = 0.06906351, grad/param norm = 4.8623e-02, time/batch = 0.3819s	
2051/2700 (epoch 37.981), train_loss = 0.05862548, grad/param norm = 4.2432e-02, time/batch = 0.3937s	
decayed learning rate by a factor 0.97 to 0.00082681869802713	
2052/2700 (epoch 38.000), train_loss = 0.05898276, grad/param norm = 4.4175e-02, time/batch = 0.4455s	
2053/2700 (epoch 38.019), train_loss = 0.09775410, grad/param norm = 5.3693e-02, time/batch = 0.4606s	
2054/2700 (epoch 38.037), train_loss = 0.08067665, grad/param norm = 4.8477e-02, time/batch = 0.4666s	
2055/2700 (epoch 38.056), train_loss = 0.07055409, grad/param norm = 4.7527e-02, time/batch = 0.4407s	
2056/2700 (epoch 38.074), train_loss = 0.07495629, grad/param norm = 6.1167e-02, time/batch = 0.4296s	
2057/2700 (epoch 38.093), train_loss = 0.06799440, grad/param norm = 5.0078e-02, time/batch = 0.4252s	
2058/2700 (epoch 38.111), train_loss = 0.06070771, grad/param norm = 4.8344e-02, time/batch = 0.4262s	
2059/2700 (epoch 38.130), train_loss = 0.05978572, grad/param norm = 4.9385e-02, time/batch = 0.4194s	
2060/2700 (epoch 38.148), train_loss = 0.05974583, grad/param norm = 3.9850e-02, time/batch = 0.4434s	
2061/2700 (epoch 38.167), train_loss = 0.07013459, grad/param norm = 4.2140e-02, time/batch = 0.4459s	
2062/2700 (epoch 38.185), train_loss = 0.05495496, grad/param norm = 3.8107e-02, time/batch = 0.3922s	
2063/2700 (epoch 38.204), train_loss = 0.06002314, grad/param norm = 3.7660e-02, time/batch = 0.3835s	
2064/2700 (epoch 38.222), train_loss = 0.04401989, grad/param norm = 3.3748e-02, time/batch = 0.3865s	
2065/2700 (epoch 38.241), train_loss = 0.05878646, grad/param norm = 3.6480e-02, time/batch = 0.4341s	
2066/2700 (epoch 38.259), train_loss = 0.06092278, grad/param norm = 3.5730e-02, time/batch = 0.4483s	
2067/2700 (epoch 38.278), train_loss = 0.06479216, grad/param norm = 3.6862e-02, time/batch = 0.4592s	
2068/2700 (epoch 38.296), train_loss = 0.06105715, grad/param norm = 3.9386e-02, time/batch = 0.4364s	
2069/2700 (epoch 38.315), train_loss = 0.06060977, grad/param norm = 4.0663e-02, time/batch = 0.4204s	
2070/2700 (epoch 38.333), train_loss = 0.05636990, grad/param norm = 4.4820e-02, time/batch = 0.4282s	
2071/2700 (epoch 38.352), train_loss = 0.05524911, grad/param norm = 4.5484e-02, time/batch = 0.4264s	
2072/2700 (epoch 38.370), train_loss = 0.04978113, grad/param norm = 3.7523e-02, time/batch = 0.4611s	
2073/2700 (epoch 38.389), train_loss = 0.05518808, grad/param norm = 3.5027e-02, time/batch = 0.4401s	
2074/2700 (epoch 38.407), train_loss = 0.06549510, grad/param norm = 3.7043e-02, time/batch = 0.3904s	
2075/2700 (epoch 38.426), train_loss = 0.06066437, grad/param norm = 3.8477e-02, time/batch = 0.3633s	
2076/2700 (epoch 38.444), train_loss = 0.05631710, grad/param norm = 3.0847e-02, time/batch = 0.4129s	
2077/2700 (epoch 38.463), train_loss = 0.06270717, grad/param norm = 4.1816e-02, time/batch = 0.4379s	
2078/2700 (epoch 38.481), train_loss = 0.06073060, grad/param norm = 4.1481e-02, time/batch = 0.4617s	
2079/2700 (epoch 38.500), train_loss = 0.05704551, grad/param norm = 3.9562e-02, time/batch = 0.4428s	
2080/2700 (epoch 38.519), train_loss = 0.06474989, grad/param norm = 4.4319e-02, time/batch = 0.4131s	
2081/2700 (epoch 38.537), train_loss = 0.06048030, grad/param norm = 4.2635e-02, time/batch = 0.3989s	
2082/2700 (epoch 38.556), train_loss = 0.06104669, grad/param norm = 4.9636e-02, time/batch = 0.4311s	
2083/2700 (epoch 38.574), train_loss = 0.06463022, grad/param norm = 3.9778e-02, time/batch = 0.4735s	
2084/2700 (epoch 38.593), train_loss = 0.06137714, grad/param norm = 4.4198e-02, time/batch = 0.4702s	
2085/2700 (epoch 38.611), train_loss = 0.06925727, grad/param norm = 3.9373e-02, time/batch = 0.4366s	
2086/2700 (epoch 38.630), train_loss = 0.05556572, grad/param norm = 3.5502e-02, time/batch = 0.3869s	
2087/2700 (epoch 38.648), train_loss = 0.06817630, grad/param norm = 4.6595e-02, time/batch = 0.3680s	
2088/2700 (epoch 38.667), train_loss = 0.06571378, grad/param norm = 4.2690e-02, time/batch = 0.4084s	
2089/2700 (epoch 38.685), train_loss = 0.05614245, grad/param norm = 3.3634e-02, time/batch = 0.4597s	
2090/2700 (epoch 38.704), train_loss = 0.06349134, grad/param norm = 4.0378e-02, time/batch = 0.4584s	
2091/2700 (epoch 38.722), train_loss = 0.06219639, grad/param norm = 3.8443e-02, time/batch = 0.4496s	
2092/2700 (epoch 38.741), train_loss = 0.06775081, grad/param norm = 4.0828e-02, time/batch = 0.4241s	
2093/2700 (epoch 38.759), train_loss = 0.05410839, grad/param norm = 3.5711e-02, time/batch = 0.4059s	
2094/2700 (epoch 38.778), train_loss = 0.05895357, grad/param norm = 3.3407e-02, time/batch = 0.4274s	
2095/2700 (epoch 38.796), train_loss = 0.05154366, grad/param norm = 3.7426e-02, time/batch = 0.4504s	
2096/2700 (epoch 38.815), train_loss = 0.05397544, grad/param norm = 3.2716e-02, time/batch = 0.4681s	
2097/2700 (epoch 38.833), train_loss = 0.05659042, grad/param norm = 3.4293e-02, time/batch = 0.4400s	
2098/2700 (epoch 38.852), train_loss = 0.05412589, grad/param norm = 3.2326e-02, time/batch = 0.4021s	
2099/2700 (epoch 38.870), train_loss = 0.05710832, grad/param norm = 3.3299e-02, time/batch = 0.3410s	
2100/2700 (epoch 38.889), train_loss = 0.05149498, grad/param norm = 3.1079e-02, time/batch = 0.4369s	
2101/2700 (epoch 38.907), train_loss = 0.04720491, grad/param norm = 3.4581e-02, time/batch = 0.4340s	
2102/2700 (epoch 38.926), train_loss = 0.06270052, grad/param norm = 3.9779e-02, time/batch = 0.4492s	
2103/2700 (epoch 38.944), train_loss = 0.05442914, grad/param norm = 3.9277e-02, time/batch = 0.4520s	
2104/2700 (epoch 38.963), train_loss = 0.06555267, grad/param norm = 4.4689e-02, time/batch = 0.4278s	
2105/2700 (epoch 38.981), train_loss = 0.05551341, grad/param norm = 3.8744e-02, time/batch = 0.4159s	
decayed learning rate by a factor 0.97 to 0.00080201413708631	
2106/2700 (epoch 39.000), train_loss = 0.05678536, grad/param norm = 4.2312e-02, time/batch = 0.4308s	
2107/2700 (epoch 39.019), train_loss = 0.08176469, grad/param norm = 4.3893e-02, time/batch = 0.4434s	
2108/2700 (epoch 39.037), train_loss = 0.06433367, grad/param norm = 4.0327e-02, time/batch = 0.4606s	
2109/2700 (epoch 39.056), train_loss = 0.06403108, grad/param norm = 3.7259e-02, time/batch = 0.4348s	
2110/2700 (epoch 39.074), train_loss = 0.06592240, grad/param norm = 4.4785e-02, time/batch = 0.3377s	
2111/2700 (epoch 39.093), train_loss = 0.05689655, grad/param norm = 4.3974e-02, time/batch = 0.3787s	
2112/2700 (epoch 39.111), train_loss = 0.05397210, grad/param norm = 4.3658e-02, time/batch = 0.4214s	
2113/2700 (epoch 39.130), train_loss = 0.05504710, grad/param norm = 4.4727e-02, time/batch = 0.4597s	
2114/2700 (epoch 39.148), train_loss = 0.06001994, grad/param norm = 4.0606e-02, time/batch = 0.4706s	
2115/2700 (epoch 39.167), train_loss = 0.06492032, grad/param norm = 4.3696e-02, time/batch = 0.4689s	
2116/2700 (epoch 39.185), train_loss = 0.05399041, grad/param norm = 4.0772e-02, time/batch = 0.4469s	
2117/2700 (epoch 39.204), train_loss = 0.05485553, grad/param norm = 3.6514e-02, time/batch = 0.4212s	
2118/2700 (epoch 39.222), train_loss = 0.04329930, grad/param norm = 3.5257e-02, time/batch = 0.4125s	
2119/2700 (epoch 39.241), train_loss = 0.05049004, grad/param norm = 3.3562e-02, time/batch = 0.4116s	
2120/2700 (epoch 39.259), train_loss = 0.05856336, grad/param norm = 3.6525e-02, time/batch = 0.4477s	
2121/2700 (epoch 39.278), train_loss = 0.05869295, grad/param norm = 3.5792e-02, time/batch = 0.3836s	
2122/2700 (epoch 39.296), train_loss = 0.05365504, grad/param norm = 3.5458e-02, time/batch = 0.4100s	
2123/2700 (epoch 39.315), train_loss = 0.05507602, grad/param norm = 4.0251e-02, time/batch = 0.3916s	
2124/2700 (epoch 39.333), train_loss = 0.05122087, grad/param norm = 3.6701e-02, time/batch = 0.4278s	
2125/2700 (epoch 39.352), train_loss = 0.05121327, grad/param norm = 4.4754e-02, time/batch = 0.4604s	
2126/2700 (epoch 39.370), train_loss = 0.04382106, grad/param norm = 4.1756e-02, time/batch = 0.4725s	
2127/2700 (epoch 39.389), train_loss = 0.05504707, grad/param norm = 3.7178e-02, time/batch = 0.4780s	
2128/2700 (epoch 39.407), train_loss = 0.06679101, grad/param norm = 3.8270e-02, time/batch = 0.4588s	
2129/2700 (epoch 39.426), train_loss = 0.05800768, grad/param norm = 4.1075e-02, time/batch = 0.4125s	
2130/2700 (epoch 39.444), train_loss = 0.05943069, grad/param norm = 4.1927e-02, time/batch = 0.3934s	
2131/2700 (epoch 39.463), train_loss = 0.06160067, grad/param norm = 3.7832e-02, time/batch = 0.4031s	
2132/2700 (epoch 39.481), train_loss = 0.05515070, grad/param norm = 4.0390e-02, time/batch = 0.3833s	
2133/2700 (epoch 39.500), train_loss = 0.05226788, grad/param norm = 4.4742e-02, time/batch = 0.4198s	
2134/2700 (epoch 39.519), train_loss = 0.06599267, grad/param norm = 4.4977e-02, time/batch = 0.3965s	
2135/2700 (epoch 39.537), train_loss = 0.06015851, grad/param norm = 3.9658e-02, time/batch = 0.4230s	
2136/2700 (epoch 39.556), train_loss = 0.05477364, grad/param norm = 4.6886e-02, time/batch = 0.4572s	
2137/2700 (epoch 39.574), train_loss = 0.06636996, grad/param norm = 4.6811e-02, time/batch = 0.4719s	
2138/2700 (epoch 39.593), train_loss = 0.05851792, grad/param norm = 4.3420e-02, time/batch = 0.4805s	
2139/2700 (epoch 39.611), train_loss = 0.07289850, grad/param norm = 4.9793e-02, time/batch = 0.4558s	
2140/2700 (epoch 39.630), train_loss = 0.05326372, grad/param norm = 4.4681e-02, time/batch = 0.4076s	
2141/2700 (epoch 39.648), train_loss = 0.06364739, grad/param norm = 4.4829e-02, time/batch = 0.4089s	
2142/2700 (epoch 39.667), train_loss = 0.06146892, grad/param norm = 3.9104e-02, time/batch = 0.3892s	
2143/2700 (epoch 39.685), train_loss = 0.05601189, grad/param norm = 4.0472e-02, time/batch = 0.3679s	
2144/2700 (epoch 39.704), train_loss = 0.05880075, grad/param norm = 4.1601e-02, time/batch = 0.4476s	
2145/2700 (epoch 39.722), train_loss = 0.05471129, grad/param norm = 3.8510e-02, time/batch = 0.4178s	
2146/2700 (epoch 39.741), train_loss = 0.06226620, grad/param norm = 4.0125e-02, time/batch = 0.4022s	
2147/2700 (epoch 39.759), train_loss = 0.05485263, grad/param norm = 4.5602e-02, time/batch = 0.4371s	
2148/2700 (epoch 39.778), train_loss = 0.05814574, grad/param norm = 3.8609e-02, time/batch = 0.4605s	
2149/2700 (epoch 39.796), train_loss = 0.05347521, grad/param norm = 4.3240e-02, time/batch = 0.4681s	
2150/2700 (epoch 39.815), train_loss = 0.05026632, grad/param norm = 3.7247e-02, time/batch = 0.4787s	
2151/2700 (epoch 39.833), train_loss = 0.05496934, grad/param norm = 3.8872e-02, time/batch = 0.4593s	
2152/2700 (epoch 39.852), train_loss = 0.05031389, grad/param norm = 3.4245e-02, time/batch = 0.4268s	
2153/2700 (epoch 39.870), train_loss = 0.05373573, grad/param norm = 3.7096e-02, time/batch = 0.4082s	
2154/2700 (epoch 39.889), train_loss = 0.05046522, grad/param norm = 3.1409e-02, time/batch = 0.3558s	
2155/2700 (epoch 39.907), train_loss = 0.04551197, grad/param norm = 3.8016e-02, time/batch = 0.4123s	
2156/2700 (epoch 39.926), train_loss = 0.05785982, grad/param norm = 4.4345e-02, time/batch = 0.4474s	
2157/2700 (epoch 39.944), train_loss = 0.04692579, grad/param norm = 3.4181e-02, time/batch = 0.4170s	
2158/2700 (epoch 39.963), train_loss = 0.05576737, grad/param norm = 4.0105e-02, time/batch = 0.4064s	
2159/2700 (epoch 39.981), train_loss = 0.05137777, grad/param norm = 3.9542e-02, time/batch = 0.4381s	
decayed learning rate by a factor 0.97 to 0.00077795371297373	
2160/2700 (epoch 40.000), train_loss = 0.04845463, grad/param norm = 3.7397e-02, time/batch = 0.4629s	
2161/2700 (epoch 40.019), train_loss = 0.08106206, grad/param norm = 5.0699e-02, time/batch = 0.4572s	
2162/2700 (epoch 40.037), train_loss = 0.06526955, grad/param norm = 5.0800e-02, time/batch = 0.4678s	
2163/2700 (epoch 40.056), train_loss = 0.05920990, grad/param norm = 4.1122e-02, time/batch = 0.4746s	
2164/2700 (epoch 40.074), train_loss = 0.05687191, grad/param norm = 3.7426e-02, time/batch = 0.4417s	
2165/2700 (epoch 40.093), train_loss = 0.05444811, grad/param norm = 4.4766e-02, time/batch = 0.3806s	
2166/2700 (epoch 40.111), train_loss = 0.04839633, grad/param norm = 4.0993e-02, time/batch = 0.3739s	
2167/2700 (epoch 40.130), train_loss = 0.05110313, grad/param norm = 4.6945e-02, time/batch = 0.4107s	
2168/2700 (epoch 40.148), train_loss = 0.05518584, grad/param norm = 4.7876e-02, time/batch = 0.4334s	
2169/2700 (epoch 40.167), train_loss = 0.06405950, grad/param norm = 5.1131e-02, time/batch = 0.4080s	
2170/2700 (epoch 40.185), train_loss = 0.05596097, grad/param norm = 5.0646e-02, time/batch = 0.4115s	
2171/2700 (epoch 40.204), train_loss = 0.05378523, grad/param norm = 4.4302e-02, time/batch = 0.4234s	
2172/2700 (epoch 40.222), train_loss = 0.04195155, grad/param norm = 3.9087e-02, time/batch = 0.4523s	
2173/2700 (epoch 40.241), train_loss = 0.05213289, grad/param norm = 4.3284e-02, time/batch = 0.4709s	
2174/2700 (epoch 40.259), train_loss = 0.05751100, grad/param norm = 4.1743e-02, time/batch = 0.4816s	
2175/2700 (epoch 40.278), train_loss = 0.06278259, grad/param norm = 3.8868e-02, time/batch = 0.4597s	
2176/2700 (epoch 40.296), train_loss = 0.05610463, grad/param norm = 4.3648e-02, time/batch = 0.4110s	
2177/2700 (epoch 40.315), train_loss = 0.05010516, grad/param norm = 3.9754e-02, time/batch = 0.3784s	
2178/2700 (epoch 40.333), train_loss = 0.04759582, grad/param norm = 4.1273e-02, time/batch = 0.3900s	
2179/2700 (epoch 40.352), train_loss = 0.05271643, grad/param norm = 4.4940e-02, time/batch = 0.4167s	
2180/2700 (epoch 40.370), train_loss = 0.04200994, grad/param norm = 3.9532e-02, time/batch = 0.4236s	
2181/2700 (epoch 40.389), train_loss = 0.04920203, grad/param norm = 3.6716e-02, time/batch = 0.3979s	
2182/2700 (epoch 40.407), train_loss = 0.06134389, grad/param norm = 3.8608e-02, time/batch = 0.3990s	
2183/2700 (epoch 40.426), train_loss = 0.05431195, grad/param norm = 4.3228e-02, time/batch = 0.4383s	
2184/2700 (epoch 40.444), train_loss = 0.05324458, grad/param norm = 3.8018e-02, time/batch = 0.4583s	
2185/2700 (epoch 40.463), train_loss = 0.06109050, grad/param norm = 4.9553e-02, time/batch = 0.4741s	
2186/2700 (epoch 40.481), train_loss = 0.05300007, grad/param norm = 4.7618e-02, time/batch = 0.4705s	
2187/2700 (epoch 40.500), train_loss = 0.04846954, grad/param norm = 4.3925e-02, time/batch = 0.4343s	
2188/2700 (epoch 40.519), train_loss = 0.06559913, grad/param norm = 6.8370e-02, time/batch = 0.3943s	
2189/2700 (epoch 40.537), train_loss = 0.06635842, grad/param norm = 5.7485e-02, time/batch = 0.3796s	
2190/2700 (epoch 40.556), train_loss = 0.05379999, grad/param norm = 4.9277e-02, time/batch = 0.4169s	
2191/2700 (epoch 40.574), train_loss = 0.06086306, grad/param norm = 4.2741e-02, time/batch = 0.4041s	
2192/2700 (epoch 40.593), train_loss = 0.05793688, grad/param norm = 4.4615e-02, time/batch = 0.3876s	
2193/2700 (epoch 40.611), train_loss = 0.06558175, grad/param norm = 4.4165e-02, time/batch = 0.4119s	
2194/2700 (epoch 40.630), train_loss = 0.05159556, grad/param norm = 4.3971e-02, time/batch = 0.4421s	
2195/2700 (epoch 40.648), train_loss = 0.05724809, grad/param norm = 3.9196e-02, time/batch = 0.4617s	
2196/2700 (epoch 40.667), train_loss = 0.05677318, grad/param norm = 3.8233e-02, time/batch = 0.4743s	
2197/2700 (epoch 40.685), train_loss = 0.05161078, grad/param norm = 3.6425e-02, time/batch = 0.4734s	
2198/2700 (epoch 40.704), train_loss = 0.05216890, grad/param norm = 4.2599e-02, time/batch = 0.3848s	
2199/2700 (epoch 40.722), train_loss = 0.05399956, grad/param norm = 4.5652e-02, time/batch = 0.3520s	
2200/2700 (epoch 40.741), train_loss = 0.05799852, grad/param norm = 3.9120e-02, time/batch = 0.3505s	
2201/2700 (epoch 40.759), train_loss = 0.05523621, grad/param norm = 5.2469e-02, time/batch = 0.4090s	
2202/2700 (epoch 40.778), train_loss = 0.05444024, grad/param norm = 4.7468e-02, time/batch = 0.4443s	
2203/2700 (epoch 40.796), train_loss = 0.05283855, grad/param norm = 4.6951e-02, time/batch = 0.4554s	
2204/2700 (epoch 40.815), train_loss = 0.05551502, grad/param norm = 4.5266e-02, time/batch = 0.3751s	
2205/2700 (epoch 40.833), train_loss = 0.04980717, grad/param norm = 3.7229e-02, time/batch = 0.4462s	
2206/2700 (epoch 40.852), train_loss = 0.04990265, grad/param norm = 3.8060e-02, time/batch = 0.4641s	
2207/2700 (epoch 40.870), train_loss = 0.05276749, grad/param norm = 3.7888e-02, time/batch = 0.4783s	
2208/2700 (epoch 40.889), train_loss = 0.05194872, grad/param norm = 3.7057e-02, time/batch = 0.4765s	
2209/2700 (epoch 40.907), train_loss = 0.04326141, grad/param norm = 3.5002e-02, time/batch = 0.4499s	
2210/2700 (epoch 40.926), train_loss = 0.04910310, grad/param norm = 3.9910e-02, time/batch = 0.4233s	
2211/2700 (epoch 40.944), train_loss = 0.04552520, grad/param norm = 3.9461e-02, time/batch = 0.3806s	
2212/2700 (epoch 40.963), train_loss = 0.05104230, grad/param norm = 4.0517e-02, time/batch = 0.3795s	
2213/2700 (epoch 40.981), train_loss = 0.04869990, grad/param norm = 3.9364e-02, time/batch = 0.4091s	
decayed learning rate by a factor 0.97 to 0.00075461510158451	
2214/2700 (epoch 41.000), train_loss = 0.04407938, grad/param norm = 3.7585e-02, time/batch = 0.4425s	
2215/2700 (epoch 41.019), train_loss = 0.07702528, grad/param norm = 5.3004e-02, time/batch = 0.4242s	
2216/2700 (epoch 41.037), train_loss = 0.05996117, grad/param norm = 4.8671e-02, time/batch = 0.3788s	
2217/2700 (epoch 41.056), train_loss = 0.05596078, grad/param norm = 4.3407e-02, time/batch = 0.4592s	
2218/2700 (epoch 41.074), train_loss = 0.05419869, grad/param norm = 3.9582e-02, time/batch = 0.4736s	
2219/2700 (epoch 41.093), train_loss = 0.04761725, grad/param norm = 4.3746e-02, time/batch = 0.4670s	
2220/2700 (epoch 41.111), train_loss = 0.04560872, grad/param norm = 4.1997e-02, time/batch = 0.4493s	
2221/2700 (epoch 41.130), train_loss = 0.04630546, grad/param norm = 4.0715e-02, time/batch = 0.4189s	
2222/2700 (epoch 41.148), train_loss = 0.04683047, grad/param norm = 4.0391e-02, time/batch = 0.4028s	
2223/2700 (epoch 41.167), train_loss = 0.06046941, grad/param norm = 4.6538e-02, time/batch = 0.3944s	
2224/2700 (epoch 41.185), train_loss = 0.04819995, grad/param norm = 4.6127e-02, time/batch = 0.4059s	
2225/2700 (epoch 41.204), train_loss = 0.05015375, grad/param norm = 3.8881e-02, time/batch = 0.4273s	
2226/2700 (epoch 41.222), train_loss = 0.04301690, grad/param norm = 4.6652e-02, time/batch = 0.4355s	
2227/2700 (epoch 41.241), train_loss = 0.04902609, grad/param norm = 4.1394e-02, time/batch = 0.4008s	
2228/2700 (epoch 41.259), train_loss = 0.05697264, grad/param norm = 4.2747e-02, time/batch = 0.3975s	
2229/2700 (epoch 41.278), train_loss = 0.06209010, grad/param norm = 4.5330e-02, time/batch = 0.4656s	
2230/2700 (epoch 41.296), train_loss = 0.05063566, grad/param norm = 3.7572e-02, time/batch = 0.4793s	
2231/2700 (epoch 41.315), train_loss = 0.05262898, grad/param norm = 3.7262e-02, time/batch = 0.4594s	
2232/2700 (epoch 41.333), train_loss = 0.04135458, grad/param norm = 3.4735e-02, time/batch = 0.4265s	
2233/2700 (epoch 41.352), train_loss = 0.04448696, grad/param norm = 3.9614e-02, time/batch = 0.4122s	
2234/2700 (epoch 41.370), train_loss = 0.03920438, grad/param norm = 3.6710e-02, time/batch = 0.4015s	
2235/2700 (epoch 41.389), train_loss = 0.04599027, grad/param norm = 4.0096e-02, time/batch = 0.4141s	
2236/2700 (epoch 41.407), train_loss = 0.05605497, grad/param norm = 4.0121e-02, time/batch = 0.4203s	
2237/2700 (epoch 41.426), train_loss = 0.05061346, grad/param norm = 4.1224e-02, time/batch = 0.4304s	
2238/2700 (epoch 41.444), train_loss = 0.05077084, grad/param norm = 3.8184e-02, time/batch = 0.4036s	
2239/2700 (epoch 41.463), train_loss = 0.05390102, grad/param norm = 4.6663e-02, time/batch = 0.4056s	
2240/2700 (epoch 41.481), train_loss = 0.05233625, grad/param norm = 5.2491e-02, time/batch = 0.4309s	
2241/2700 (epoch 41.500), train_loss = 0.04513178, grad/param norm = 4.8438e-02, time/batch = 0.4539s	
2242/2700 (epoch 41.519), train_loss = 0.06645115, grad/param norm = 5.7511e-02, time/batch = 0.4513s	
2243/2700 (epoch 41.537), train_loss = 0.05915870, grad/param norm = 5.2281e-02, time/batch = 0.4201s	
2244/2700 (epoch 41.556), train_loss = 0.05094310, grad/param norm = 4.5542e-02, time/batch = 0.4046s	
2245/2700 (epoch 41.574), train_loss = 0.05995038, grad/param norm = 4.2129e-02, time/batch = 0.4066s	
2246/2700 (epoch 41.593), train_loss = 0.05150188, grad/param norm = 3.8583e-02, time/batch = 0.4121s	
2247/2700 (epoch 41.611), train_loss = 0.05734499, grad/param norm = 3.3888e-02, time/batch = 0.4326s	
2248/2700 (epoch 41.630), train_loss = 0.04715823, grad/param norm = 3.4351e-02, time/batch = 0.4168s	
2249/2700 (epoch 41.648), train_loss = 0.04982604, grad/param norm = 3.6422e-02, time/batch = 0.3953s	
2250/2700 (epoch 41.667), train_loss = 0.05180525, grad/param norm = 3.0984e-02, time/batch = 0.4193s	
2251/2700 (epoch 41.685), train_loss = 0.04623768, grad/param norm = 3.4346e-02, time/batch = 0.4317s	
2252/2700 (epoch 41.704), train_loss = 0.04467385, grad/param norm = 3.5141e-02, time/batch = 0.4393s	
2253/2700 (epoch 41.722), train_loss = 0.05031352, grad/param norm = 4.7328e-02, time/batch = 0.4556s	
2254/2700 (epoch 41.741), train_loss = 0.05597914, grad/param norm = 4.3499e-02, time/batch = 0.4230s	
2255/2700 (epoch 41.759), train_loss = 0.04869370, grad/param norm = 4.5876e-02, time/batch = 0.4363s	
2256/2700 (epoch 41.778), train_loss = 0.05312266, grad/param norm = 4.5227e-02, time/batch = 0.4232s	
2257/2700 (epoch 41.796), train_loss = 0.04412925, grad/param norm = 3.8697e-02, time/batch = 0.4201s	
2258/2700 (epoch 41.815), train_loss = 0.04595108, grad/param norm = 3.4867e-02, time/batch = 0.4266s	
2259/2700 (epoch 41.833), train_loss = 0.04744584, grad/param norm = 4.5881e-02, time/batch = 0.4432s	
2260/2700 (epoch 41.852), train_loss = 0.04576209, grad/param norm = 4.3561e-02, time/batch = 0.4104s	
2261/2700 (epoch 41.870), train_loss = 0.04707978, grad/param norm = 3.5207e-02, time/batch = 0.3860s	
2262/2700 (epoch 41.889), train_loss = 0.04711020, grad/param norm = 3.7402e-02, time/batch = 0.3682s	
2263/2700 (epoch 41.907), train_loss = 0.04061411, grad/param norm = 3.7834e-02, time/batch = 0.4291s	
2264/2700 (epoch 41.926), train_loss = 0.04774430, grad/param norm = 3.9451e-02, time/batch = 0.4457s	
2265/2700 (epoch 41.944), train_loss = 0.04010067, grad/param norm = 3.1596e-02, time/batch = 0.4269s	
2266/2700 (epoch 41.963), train_loss = 0.04792732, grad/param norm = 3.5630e-02, time/batch = 0.4326s	
2267/2700 (epoch 41.981), train_loss = 0.04201887, grad/param norm = 3.2442e-02, time/batch = 0.4195s	
decayed learning rate by a factor 0.97 to 0.00073197664853698	
2268/2700 (epoch 42.000), train_loss = 0.04148711, grad/param norm = 3.3048e-02, time/batch = 0.4340s	
2269/2700 (epoch 42.019), train_loss = 0.06213303, grad/param norm = 3.5021e-02, time/batch = 0.4307s	
2270/2700 (epoch 42.037), train_loss = 0.05342129, grad/param norm = 4.4779e-02, time/batch = 0.4365s	
2271/2700 (epoch 42.056), train_loss = 0.04611432, grad/param norm = 3.5103e-02, time/batch = 0.4460s	
2272/2700 (epoch 42.074), train_loss = 0.04435071, grad/param norm = 3.2730e-02, time/batch = 0.3992s	
2273/2700 (epoch 42.093), train_loss = 0.04034650, grad/param norm = 3.1476e-02, time/batch = 0.3738s	
2274/2700 (epoch 42.111), train_loss = 0.03638534, grad/param norm = 3.0620e-02, time/batch = 0.3906s	
2275/2700 (epoch 42.130), train_loss = 0.03964415, grad/param norm = 3.7190e-02, time/batch = 0.4469s	
2276/2700 (epoch 42.148), train_loss = 0.04166317, grad/param norm = 4.9463e-02, time/batch = 0.4467s	
2277/2700 (epoch 42.167), train_loss = 0.05558268, grad/param norm = 5.2412e-02, time/batch = 0.4205s	
2278/2700 (epoch 42.185), train_loss = 0.04870574, grad/param norm = 5.3587e-02, time/batch = 0.4168s	
2279/2700 (epoch 42.204), train_loss = 0.04650029, grad/param norm = 4.4649e-02, time/batch = 0.4433s	
2280/2700 (epoch 42.222), train_loss = 0.04026344, grad/param norm = 4.1827e-02, time/batch = 0.4501s	
2281/2700 (epoch 42.241), train_loss = 0.04491399, grad/param norm = 3.3929e-02, time/batch = 0.4612s	
2282/2700 (epoch 42.259), train_loss = 0.04980193, grad/param norm = 3.3241e-02, time/batch = 0.4601s	
2283/2700 (epoch 42.278), train_loss = 0.05157385, grad/param norm = 3.7083e-02, time/batch = 0.4439s	
2284/2700 (epoch 42.296), train_loss = 0.04582786, grad/param norm = 3.7452e-02, time/batch = 0.4066s	
2285/2700 (epoch 42.315), train_loss = 0.04525321, grad/param norm = 3.4281e-02, time/batch = 0.3692s	
2286/2700 (epoch 42.333), train_loss = 0.03773133, grad/param norm = 3.0799e-02, time/batch = 0.4028s	
2287/2700 (epoch 42.352), train_loss = 0.04038869, grad/param norm = 3.3959e-02, time/batch = 0.4252s	
2288/2700 (epoch 42.370), train_loss = 0.03137407, grad/param norm = 2.9113e-02, time/batch = 0.4566s	
2289/2700 (epoch 42.389), train_loss = 0.03993736, grad/param norm = 3.3264e-02, time/batch = 0.3729s	
2290/2700 (epoch 42.407), train_loss = 0.04782704, grad/param norm = 3.0609e-02, time/batch = 0.4546s	
2291/2700 (epoch 42.426), train_loss = 0.04107319, grad/param norm = 3.8234e-02, time/batch = 0.4521s	
2292/2700 (epoch 42.444), train_loss = 0.04416952, grad/param norm = 3.2477e-02, time/batch = 0.4626s	
2293/2700 (epoch 42.463), train_loss = 0.04278911, grad/param norm = 3.2992e-02, time/batch = 0.4694s	
2294/2700 (epoch 42.481), train_loss = 0.04239024, grad/param norm = 3.6937e-02, time/batch = 0.4406s	
2295/2700 (epoch 42.500), train_loss = 0.04242780, grad/param norm = 5.0417e-02, time/batch = 0.4110s	
2296/2700 (epoch 42.519), train_loss = 0.05430979, grad/param norm = 4.9796e-02, time/batch = 0.3959s	
2297/2700 (epoch 42.537), train_loss = 0.05488867, grad/param norm = 4.6242e-02, time/batch = 0.4081s	
2298/2700 (epoch 42.556), train_loss = 0.04583216, grad/param norm = 4.2755e-02, time/batch = 0.4135s	
2299/2700 (epoch 42.574), train_loss = 0.05251787, grad/param norm = 4.1930e-02, time/batch = 0.4479s	
2300/2700 (epoch 42.593), train_loss = 0.04745899, grad/param norm = 5.5452e-02, time/batch = 0.4219s	
2301/2700 (epoch 42.611), train_loss = 0.05176999, grad/param norm = 4.0869e-02, time/batch = 0.3689s	
2302/2700 (epoch 42.630), train_loss = 0.04869896, grad/param norm = 4.0084e-02, time/batch = 0.4510s	
2303/2700 (epoch 42.648), train_loss = 0.04818261, grad/param norm = 3.3123e-02, time/batch = 0.4695s	
2304/2700 (epoch 42.667), train_loss = 0.05099173, grad/param norm = 3.2868e-02, time/batch = 0.4720s	
2305/2700 (epoch 42.685), train_loss = 0.04434566, grad/param norm = 3.9099e-02, time/batch = 0.4393s	
2306/2700 (epoch 42.704), train_loss = 0.04734366, grad/param norm = 3.4220e-02, time/batch = 0.4017s	
2307/2700 (epoch 42.722), train_loss = 0.04707338, grad/param norm = 4.2913e-02, time/batch = 0.3833s	
2308/2700 (epoch 42.741), train_loss = 0.05059493, grad/param norm = 3.9169e-02, time/batch = 0.4054s	
2309/2700 (epoch 42.759), train_loss = 0.04679275, grad/param norm = 5.4527e-02, time/batch = 0.4246s	
2310/2700 (epoch 42.778), train_loss = 0.05116909, grad/param norm = 4.5190e-02, time/batch = 0.4397s	
2311/2700 (epoch 42.796), train_loss = 0.04750239, grad/param norm = 4.9527e-02, time/batch = 0.4429s	
2312/2700 (epoch 42.815), train_loss = 0.05174262, grad/param norm = 4.8475e-02, time/batch = 0.3948s	
2313/2700 (epoch 42.833), train_loss = 0.05073169, grad/param norm = 4.4474e-02, time/batch = 0.4054s	
2314/2700 (epoch 42.852), train_loss = 0.04677174, grad/param norm = 3.6258e-02, time/batch = 0.4633s	
2315/2700 (epoch 42.870), train_loss = 0.04543427, grad/param norm = 4.2600e-02, time/batch = 0.4704s	
2316/2700 (epoch 42.889), train_loss = 0.04664604, grad/param norm = 4.3052e-02, time/batch = 0.4564s	
2317/2700 (epoch 42.907), train_loss = 0.04406508, grad/param norm = 4.2339e-02, time/batch = 0.4184s	
2318/2700 (epoch 42.926), train_loss = 0.04596772, grad/param norm = 3.6479e-02, time/batch = 0.3825s	
2319/2700 (epoch 42.944), train_loss = 0.03789795, grad/param norm = 3.3148e-02, time/batch = 0.3912s	
2320/2700 (epoch 42.963), train_loss = 0.04338243, grad/param norm = 3.1931e-02, time/batch = 0.4111s	
2321/2700 (epoch 42.981), train_loss = 0.03843325, grad/param norm = 3.1756e-02, time/batch = 0.4358s	
decayed learning rate by a factor 0.97 to 0.00071001734908087	
2322/2700 (epoch 43.000), train_loss = 0.03750470, grad/param norm = 3.5695e-02, time/batch = 0.4317s	
2323/2700 (epoch 43.019), train_loss = 0.06045255, grad/param norm = 3.4993e-02, time/batch = 0.4140s	
2324/2700 (epoch 43.037), train_loss = 0.05126840, grad/param norm = 3.8950e-02, time/batch = 0.4140s	
2325/2700 (epoch 43.056), train_loss = 0.04153113, grad/param norm = 3.2814e-02, time/batch = 0.4373s	
2326/2700 (epoch 43.074), train_loss = 0.04169214, grad/param norm = 3.4790e-02, time/batch = 0.4758s	
2327/2700 (epoch 43.093), train_loss = 0.03552864, grad/param norm = 3.2251e-02, time/batch = 0.4788s	
2328/2700 (epoch 43.111), train_loss = 0.03202505, grad/param norm = 2.8607e-02, time/batch = 0.4367s	
2329/2700 (epoch 43.130), train_loss = 0.03555414, grad/param norm = 3.7090e-02, time/batch = 0.3847s	
2330/2700 (epoch 43.148), train_loss = 0.04149697, grad/param norm = 4.5905e-02, time/batch = 0.3733s	
2331/2700 (epoch 43.167), train_loss = 0.05237669, grad/param norm = 5.5899e-02, time/batch = 0.3715s	
2332/2700 (epoch 43.185), train_loss = 0.04334588, grad/param norm = 4.7496e-02, time/batch = 0.4499s	
2333/2700 (epoch 43.204), train_loss = 0.04171147, grad/param norm = 3.6355e-02, time/batch = 0.4599s	
2334/2700 (epoch 43.222), train_loss = 0.03454816, grad/param norm = 3.8174e-02, time/batch = 0.4379s	
2335/2700 (epoch 43.241), train_loss = 0.04162536, grad/param norm = 3.5561e-02, time/batch = 0.4198s	
2336/2700 (epoch 43.259), train_loss = 0.04177980, grad/param norm = 3.3064e-02, time/batch = 0.4253s	
2337/2700 (epoch 43.278), train_loss = 0.04457136, grad/param norm = 3.4926e-02, time/batch = 0.4387s	
2338/2700 (epoch 43.296), train_loss = 0.03713645, grad/param norm = 2.9662e-02, time/batch = 0.4600s	
2339/2700 (epoch 43.315), train_loss = 0.03970649, grad/param norm = 3.9429e-02, time/batch = 0.4489s	
2340/2700 (epoch 43.333), train_loss = 0.03417631, grad/param norm = 3.0361e-02, time/batch = 0.3958s	
2341/2700 (epoch 43.352), train_loss = 0.03561180, grad/param norm = 3.4582e-02, time/batch = 0.3949s	
2342/2700 (epoch 43.370), train_loss = 0.03128349, grad/param norm = 3.7040e-02, time/batch = 0.3306s	
2343/2700 (epoch 43.389), train_loss = 0.03872326, grad/param norm = 3.5111e-02, time/batch = 0.4486s	
2344/2700 (epoch 43.407), train_loss = 0.04484799, grad/param norm = 3.4888e-02, time/batch = 0.4618s	
2345/2700 (epoch 43.426), train_loss = 0.04348174, grad/param norm = 3.8395e-02, time/batch = 0.4688s	
2346/2700 (epoch 43.444), train_loss = 0.04088882, grad/param norm = 3.1265e-02, time/batch = 0.4468s	
2347/2700 (epoch 43.463), train_loss = 0.04125882, grad/param norm = 3.1433e-02, time/batch = 0.4273s	
2348/2700 (epoch 43.481), train_loss = 0.03760925, grad/param norm = 3.0099e-02, time/batch = 0.4201s	
2349/2700 (epoch 43.500), train_loss = 0.03456222, grad/param norm = 3.3638e-02, time/batch = 0.4231s	
2350/2700 (epoch 43.519), train_loss = 0.04370444, grad/param norm = 4.0605e-02, time/batch = 0.4515s	
2351/2700 (epoch 43.537), train_loss = 0.04407626, grad/param norm = 4.0738e-02, time/batch = 0.4570s	
2352/2700 (epoch 43.556), train_loss = 0.04245607, grad/param norm = 4.8293e-02, time/batch = 0.4378s	
2353/2700 (epoch 43.574), train_loss = 0.05309741, grad/param norm = 4.1928e-02, time/batch = 0.3574s	
2354/2700 (epoch 43.593), train_loss = 0.04529020, grad/param norm = 4.4131e-02, time/batch = 0.3793s	
2355/2700 (epoch 43.611), train_loss = 0.05166163, grad/param norm = 4.5183e-02, time/batch = 0.4349s	
2356/2700 (epoch 43.630), train_loss = 0.04364569, grad/param norm = 4.4004e-02, time/batch = 0.4610s	
2357/2700 (epoch 43.648), train_loss = 0.05005559, grad/param norm = 4.5760e-02, time/batch = 0.4670s	
2358/2700 (epoch 43.667), train_loss = 0.04675806, grad/param norm = 3.7629e-02, time/batch = 0.4500s	
2359/2700 (epoch 43.685), train_loss = 0.04127220, grad/param norm = 4.0365e-02, time/batch = 0.4300s	
2360/2700 (epoch 43.704), train_loss = 0.04527144, grad/param norm = 4.3258e-02, time/batch = 0.4130s	
2361/2700 (epoch 43.722), train_loss = 0.04300643, grad/param norm = 3.5752e-02, time/batch = 0.3951s	
2362/2700 (epoch 43.741), train_loss = 0.04596695, grad/param norm = 3.2757e-02, time/batch = 0.4587s	
2363/2700 (epoch 43.759), train_loss = 0.04578498, grad/param norm = 3.5141e-02, time/batch = 0.4646s	
2364/2700 (epoch 43.778), train_loss = 0.04296095, grad/param norm = 3.7680e-02, time/batch = 0.3709s	
2365/2700 (epoch 43.796), train_loss = 0.04024325, grad/param norm = 3.5166e-02, time/batch = 0.3739s	
2366/2700 (epoch 43.815), train_loss = 0.04081184, grad/param norm = 3.6415e-02, time/batch = 0.4257s	
2367/2700 (epoch 43.833), train_loss = 0.03862929, grad/param norm = 3.6255e-02, time/batch = 0.4562s	
2368/2700 (epoch 43.852), train_loss = 0.04494418, grad/param norm = 3.5284e-02, time/batch = 0.4739s	
2369/2700 (epoch 43.870), train_loss = 0.03962367, grad/param norm = 3.5325e-02, time/batch = 0.4780s	
2370/2700 (epoch 43.889), train_loss = 0.04081810, grad/param norm = 3.6954e-02, time/batch = 0.4466s	
2371/2700 (epoch 43.907), train_loss = 0.03824744, grad/param norm = 3.9693e-02, time/batch = 0.4345s	
2372/2700 (epoch 43.926), train_loss = 0.04008308, grad/param norm = 3.4079e-02, time/batch = 0.3922s	
2373/2700 (epoch 43.944), train_loss = 0.03835030, grad/param norm = 3.8776e-02, time/batch = 0.4024s	
2374/2700 (epoch 43.963), train_loss = 0.04138057, grad/param norm = 3.2994e-02, time/batch = 0.4624s	
2375/2700 (epoch 43.981), train_loss = 0.03553888, grad/param norm = 3.1702e-02, time/batch = 0.3966s	
decayed learning rate by a factor 0.97 to 0.00068871682860844	
2376/2700 (epoch 44.000), train_loss = 0.03277826, grad/param norm = 3.1269e-02, time/batch = 0.4020s	
2377/2700 (epoch 44.019), train_loss = 0.05219246, grad/param norm = 3.3911e-02, time/batch = 0.3964s	
2378/2700 (epoch 44.037), train_loss = 0.04482135, grad/param norm = 3.2145e-02, time/batch = 0.4398s	
2379/2700 (epoch 44.056), train_loss = 0.03835189, grad/param norm = 2.7753e-02, time/batch = 0.4636s	
2380/2700 (epoch 44.074), train_loss = 0.03846983, grad/param norm = 2.8009e-02, time/batch = 0.4767s	
2381/2700 (epoch 44.093), train_loss = 0.03579987, grad/param norm = 3.9266e-02, time/batch = 0.4571s	
2382/2700 (epoch 44.111), train_loss = 0.03159495, grad/param norm = 3.1979e-02, time/batch = 0.4405s	
2383/2700 (epoch 44.130), train_loss = 0.03239561, grad/param norm = 4.3780e-02, time/batch = 0.4165s	
2384/2700 (epoch 44.148), train_loss = 0.03935550, grad/param norm = 3.7268e-02, time/batch = 0.4019s	
2385/2700 (epoch 44.167), train_loss = 0.05044678, grad/param norm = 4.5865e-02, time/batch = 0.3983s	
2386/2700 (epoch 44.185), train_loss = 0.03663561, grad/param norm = 4.4169e-02, time/batch = 0.4024s	
2387/2700 (epoch 44.204), train_loss = 0.03606881, grad/param norm = 3.5219e-02, time/batch = 0.4278s	
2388/2700 (epoch 44.222), train_loss = 0.03177341, grad/param norm = 3.9849e-02, time/batch = 0.4006s	
2389/2700 (epoch 44.241), train_loss = 0.03952752, grad/param norm = 3.9283e-02, time/batch = 0.4280s	
2390/2700 (epoch 44.259), train_loss = 0.04555729, grad/param norm = 3.6799e-02, time/batch = 0.4565s	
2391/2700 (epoch 44.278), train_loss = 0.03994803, grad/param norm = 3.0014e-02, time/batch = 0.4549s	
2392/2700 (epoch 44.296), train_loss = 0.03880688, grad/param norm = 3.1092e-02, time/batch = 0.4651s	
2393/2700 (epoch 44.315), train_loss = 0.04124462, grad/param norm = 3.5715e-02, time/batch = 0.4703s	
2394/2700 (epoch 44.333), train_loss = 0.03193018, grad/param norm = 2.5894e-02, time/batch = 0.4396s	
2395/2700 (epoch 44.352), train_loss = 0.03842586, grad/param norm = 4.1344e-02, time/batch = 0.3909s	
2396/2700 (epoch 44.370), train_loss = 0.03424506, grad/param norm = 4.0723e-02, time/batch = 0.3761s	
2397/2700 (epoch 44.389), train_loss = 0.03523471, grad/param norm = 3.7273e-02, time/batch = 0.3908s	
2398/2700 (epoch 44.407), train_loss = 0.04384382, grad/param norm = 3.2281e-02, time/batch = 0.4378s	
2399/2700 (epoch 44.426), train_loss = 0.03837243, grad/param norm = 3.5851e-02, time/batch = 0.4171s	
2400/2700 (epoch 44.444), train_loss = 0.04144059, grad/param norm = 3.6249e-02, time/batch = 0.4133s	
2401/2700 (epoch 44.463), train_loss = 0.03744910, grad/param norm = 3.4730e-02, time/batch = 0.4186s	
2402/2700 (epoch 44.481), train_loss = 0.03202419, grad/param norm = 2.7905e-02, time/batch = 0.4503s	
2403/2700 (epoch 44.500), train_loss = 0.03438174, grad/param norm = 3.4385e-02, time/batch = 0.4706s	
2404/2700 (epoch 44.519), train_loss = 0.03864444, grad/param norm = 2.7460e-02, time/batch = 0.4769s	
2405/2700 (epoch 44.537), train_loss = 0.04440063, grad/param norm = 4.6461e-02, time/batch = 0.4447s	
2406/2700 (epoch 44.556), train_loss = 0.03279822, grad/param norm = 3.0113e-02, time/batch = 0.3997s	
2407/2700 (epoch 44.574), train_loss = 0.04595628, grad/param norm = 3.3787e-02, time/batch = 0.3780s	
2408/2700 (epoch 44.593), train_loss = 0.04362884, grad/param norm = 4.3861e-02, time/batch = 0.4003s	
2409/2700 (epoch 44.611), train_loss = 0.05015569, grad/param norm = 4.9277e-02, time/batch = 0.4511s	
2410/2700 (epoch 44.630), train_loss = 0.03733759, grad/param norm = 3.5451e-02, time/batch = 0.4019s	
2411/2700 (epoch 44.648), train_loss = 0.04508329, grad/param norm = 3.7422e-02, time/batch = 0.3956s	
2412/2700 (epoch 44.667), train_loss = 0.04583780, grad/param norm = 3.6217e-02, time/batch = 0.4245s	
2413/2700 (epoch 44.685), train_loss = 0.03969218, grad/param norm = 3.1351e-02, time/batch = 0.4593s	
2414/2700 (epoch 44.704), train_loss = 0.04124565, grad/param norm = 3.2281e-02, time/batch = 0.4744s	
2415/2700 (epoch 44.722), train_loss = 0.04119673, grad/param norm = 3.5511e-02, time/batch = 0.4840s	
2416/2700 (epoch 44.741), train_loss = 0.04459504, grad/param norm = 3.3708e-02, time/batch = 0.4581s	
2417/2700 (epoch 44.759), train_loss = 0.03541395, grad/param norm = 2.9336e-02, time/batch = 0.4138s	
2418/2700 (epoch 44.778), train_loss = 0.03814144, grad/param norm = 2.7082e-02, time/batch = 0.3864s	
2419/2700 (epoch 44.796), train_loss = 0.03412844, grad/param norm = 3.0524e-02, time/batch = 0.3785s	
2420/2700 (epoch 44.815), train_loss = 0.03296067, grad/param norm = 2.8381e-02, time/batch = 0.4548s	
2421/2700 (epoch 44.833), train_loss = 0.03346708, grad/param norm = 2.8107e-02, time/batch = 0.4336s	
2422/2700 (epoch 44.852), train_loss = 0.03593418, grad/param norm = 3.3713e-02, time/batch = 0.3927s	
2423/2700 (epoch 44.870), train_loss = 0.04035395, grad/param norm = 3.8255e-02, time/batch = 0.4145s	
2424/2700 (epoch 44.889), train_loss = 0.03796552, grad/param norm = 3.2687e-02, time/batch = 0.4538s	
2425/2700 (epoch 44.907), train_loss = 0.03471422, grad/param norm = 3.1280e-02, time/batch = 0.4703s	
2426/2700 (epoch 44.926), train_loss = 0.03698577, grad/param norm = 3.1980e-02, time/batch = 0.4818s	
2427/2700 (epoch 44.944), train_loss = 0.03298963, grad/param norm = 2.8730e-02, time/batch = 0.4712s	
2428/2700 (epoch 44.963), train_loss = 0.03309406, grad/param norm = 3.3313e-02, time/batch = 0.4321s	
2429/2700 (epoch 44.981), train_loss = 0.03337894, grad/param norm = 3.8081e-02, time/batch = 0.4015s	
decayed learning rate by a factor 0.97 to 0.00066805532375019	
2430/2700 (epoch 45.000), train_loss = 0.03253625, grad/param norm = 2.7139e-02, time/batch = 0.3616s	
2431/2700 (epoch 45.019), train_loss = 0.04736075, grad/param norm = 3.5373e-02, time/batch = 0.4026s	
2432/2700 (epoch 45.037), train_loss = 0.04140697, grad/param norm = 2.9829e-02, time/batch = 0.4302s	
2433/2700 (epoch 45.056), train_loss = 0.03505928, grad/param norm = 2.6293e-02, time/batch = 0.4450s	
2434/2700 (epoch 45.074), train_loss = 0.03207780, grad/param norm = 2.6472e-02, time/batch = 0.3683s	
2435/2700 (epoch 45.093), train_loss = 0.03161474, grad/param norm = 2.8883e-02, time/batch = 0.4498s	
2436/2700 (epoch 45.111), train_loss = 0.02706438, grad/param norm = 2.7993e-02, time/batch = 0.4710s	
2437/2700 (epoch 45.130), train_loss = 0.03213800, grad/param norm = 3.9249e-02, time/batch = 0.4814s	
2438/2700 (epoch 45.148), train_loss = 0.03305109, grad/param norm = 2.7676e-02, time/batch = 0.4676s	
2439/2700 (epoch 45.167), train_loss = 0.04127654, grad/param norm = 3.8115e-02, time/batch = 0.4431s	
2440/2700 (epoch 45.185), train_loss = 0.03570410, grad/param norm = 3.7840e-02, time/batch = 0.4182s	
2441/2700 (epoch 45.204), train_loss = 0.03479176, grad/param norm = 3.9409e-02, time/batch = 0.3860s	
2442/2700 (epoch 45.222), train_loss = 0.02963638, grad/param norm = 3.7181e-02, time/batch = 0.3914s	
2443/2700 (epoch 45.241), train_loss = 0.03951184, grad/param norm = 3.9929e-02, time/batch = 0.4135s	
2444/2700 (epoch 45.259), train_loss = 0.04165212, grad/param norm = 3.5514e-02, time/batch = 0.4402s	
2445/2700 (epoch 45.278), train_loss = 0.03723654, grad/param norm = 3.1066e-02, time/batch = 0.4150s	
2446/2700 (epoch 45.296), train_loss = 0.03480213, grad/param norm = 3.6965e-02, time/batch = 0.3950s	
2447/2700 (epoch 45.315), train_loss = 0.03841071, grad/param norm = 4.6218e-02, time/batch = 0.3854s	
2448/2700 (epoch 45.333), train_loss = 0.03131932, grad/param norm = 3.1192e-02, time/batch = 0.3669s	
2449/2700 (epoch 45.352), train_loss = 0.03517749, grad/param norm = 3.5394e-02, time/batch = 0.4755s	
2450/2700 (epoch 45.370), train_loss = 0.03003808, grad/param norm = 3.9020e-02, time/batch = 0.4537s	
2451/2700 (epoch 45.389), train_loss = 0.03861465, grad/param norm = 4.2097e-02, time/batch = 0.4240s	
2452/2700 (epoch 45.407), train_loss = 0.04069015, grad/param norm = 3.2823e-02, time/batch = 0.3893s	
2453/2700 (epoch 45.426), train_loss = 0.03993248, grad/param norm = 4.0145e-02, time/batch = 0.4100s	
2454/2700 (epoch 45.444), train_loss = 0.04272109, grad/param norm = 3.8824e-02, time/batch = 0.4200s	
2455/2700 (epoch 45.463), train_loss = 0.04031059, grad/param norm = 4.3382e-02, time/batch = 0.4251s	
2456/2700 (epoch 45.481), train_loss = 0.04520707, grad/param norm = 4.8205e-02, time/batch = 0.4104s	
2457/2700 (epoch 45.500), train_loss = 0.03786263, grad/param norm = 4.0565e-02, time/batch = 0.3847s	
2458/2700 (epoch 45.519), train_loss = 0.04262977, grad/param norm = 3.5018e-02, time/batch = 0.4204s	
2459/2700 (epoch 45.537), train_loss = 0.04429827, grad/param norm = 3.8872e-02, time/batch = 0.4539s	
2460/2700 (epoch 45.556), train_loss = 0.03444525, grad/param norm = 3.5561e-02, time/batch = 0.4576s	
2461/2700 (epoch 45.574), train_loss = 0.04110928, grad/param norm = 3.2283e-02, time/batch = 0.4437s	
2462/2700 (epoch 45.593), train_loss = 0.03914501, grad/param norm = 3.7458e-02, time/batch = 0.4050s	
2463/2700 (epoch 45.611), train_loss = 0.04520797, grad/param norm = 3.7979e-02, time/batch = 0.4111s	
2464/2700 (epoch 45.630), train_loss = 0.03951853, grad/param norm = 4.0082e-02, time/batch = 0.4170s	
2465/2700 (epoch 45.648), train_loss = 0.03672034, grad/param norm = 3.0976e-02, time/batch = 0.4212s	
2466/2700 (epoch 45.667), train_loss = 0.04581259, grad/param norm = 3.1888e-02, time/batch = 0.4317s	
2467/2700 (epoch 45.685), train_loss = 0.03635688, grad/param norm = 3.0516e-02, time/batch = 0.4125s	
2468/2700 (epoch 45.704), train_loss = 0.04099554, grad/param norm = 4.0065e-02, time/batch = 0.3865s	
2469/2700 (epoch 45.722), train_loss = 0.03969689, grad/param norm = 4.1179e-02, time/batch = 0.4173s	
2470/2700 (epoch 45.741), train_loss = 0.04363580, grad/param norm = 3.4069e-02, time/batch = 0.4550s	
2471/2700 (epoch 45.759), train_loss = 0.03234129, grad/param norm = 3.2241e-02, time/batch = 0.4519s	
2472/2700 (epoch 45.778), train_loss = 0.03915659, grad/param norm = 3.5326e-02, time/batch = 0.4476s	
2473/2700 (epoch 45.796), train_loss = 0.04238307, grad/param norm = 4.9567e-02, time/batch = 0.4093s	
2474/2700 (epoch 45.815), train_loss = 0.03461063, grad/param norm = 4.1016e-02, time/batch = 0.4169s	
2475/2700 (epoch 45.833), train_loss = 0.03286310, grad/param norm = 2.9098e-02, time/batch = 0.4202s	
2476/2700 (epoch 45.852), train_loss = 0.03653720, grad/param norm = 3.4899e-02, time/batch = 0.4239s	
2477/2700 (epoch 45.870), train_loss = 0.03739687, grad/param norm = 3.4819e-02, time/batch = 0.4386s	
2478/2700 (epoch 45.889), train_loss = 0.03303997, grad/param norm = 3.1919e-02, time/batch = 0.4182s	
2479/2700 (epoch 45.907), train_loss = 0.03542681, grad/param norm = 3.6676e-02, time/batch = 0.3860s	
2480/2700 (epoch 45.926), train_loss = 0.03475665, grad/param norm = 3.0130e-02, time/batch = 0.4039s	
2481/2700 (epoch 45.944), train_loss = 0.03365349, grad/param norm = 3.2000e-02, time/batch = 0.4127s	
2482/2700 (epoch 45.963), train_loss = 0.03660688, grad/param norm = 3.4385e-02, time/batch = 0.4506s	
2483/2700 (epoch 45.981), train_loss = 0.02973550, grad/param norm = 3.1121e-02, time/batch = 0.4659s	
decayed learning rate by a factor 0.97 to 0.00064801366403768	
2484/2700 (epoch 46.000), train_loss = 0.02875513, grad/param norm = 2.8783e-02, time/batch = 0.4521s	
2485/2700 (epoch 46.019), train_loss = 0.04436061, grad/param norm = 3.5795e-02, time/batch = 0.3798s	
2486/2700 (epoch 46.037), train_loss = 0.03411036, grad/param norm = 3.2408e-02, time/batch = 0.4539s	
2487/2700 (epoch 46.056), train_loss = 0.03602085, grad/param norm = 3.0919e-02, time/batch = 0.4572s	
2488/2700 (epoch 46.074), train_loss = 0.03347783, grad/param norm = 2.5771e-02, time/batch = 0.4434s	
2489/2700 (epoch 46.093), train_loss = 0.02999143, grad/param norm = 2.5433e-02, time/batch = 0.4186s	
2490/2700 (epoch 46.111), train_loss = 0.02521651, grad/param norm = 2.7757e-02, time/batch = 0.3940s	
2491/2700 (epoch 46.130), train_loss = 0.02740022, grad/param norm = 3.0535e-02, time/batch = 0.3591s	
2492/2700 (epoch 46.148), train_loss = 0.02968107, grad/param norm = 2.7349e-02, time/batch = 0.3833s	
2493/2700 (epoch 46.167), train_loss = 0.03889190, grad/param norm = 3.5130e-02, time/batch = 0.4348s	
2494/2700 (epoch 46.185), train_loss = 0.02802122, grad/param norm = 3.1937e-02, time/batch = 0.4521s	
2495/2700 (epoch 46.204), train_loss = 0.03222483, grad/param norm = 3.2714e-02, time/batch = 0.4179s	
2496/2700 (epoch 46.222), train_loss = 0.02831505, grad/param norm = 3.7737e-02, time/batch = 0.4216s	
2497/2700 (epoch 46.241), train_loss = 0.03367878, grad/param norm = 3.4327e-02, time/batch = 0.4162s	
2498/2700 (epoch 46.259), train_loss = 0.03848944, grad/param norm = 3.3208e-02, time/batch = 0.4670s	
2499/2700 (epoch 46.278), train_loss = 0.03413255, grad/param norm = 3.1006e-02, time/batch = 0.4721s	
2500/2700 (epoch 46.296), train_loss = 0.03505645, grad/param norm = 3.4113e-02, time/batch = 0.4459s	
2501/2700 (epoch 46.315), train_loss = 0.03562951, grad/param norm = 3.5074e-02, time/batch = 0.4379s	
2502/2700 (epoch 46.333), train_loss = 0.03266592, grad/param norm = 3.3303e-02, time/batch = 0.4026s	
2503/2700 (epoch 46.352), train_loss = 0.03052568, grad/param norm = 2.6755e-02, time/batch = 0.3996s	
2504/2700 (epoch 46.370), train_loss = 0.02687196, grad/param norm = 3.2394e-02, time/batch = 0.4104s	
2505/2700 (epoch 46.389), train_loss = 0.03435762, grad/param norm = 4.0547e-02, time/batch = 0.4223s	
2506/2700 (epoch 46.407), train_loss = 0.04354564, grad/param norm = 3.2479e-02, time/batch = 0.4075s	
2507/2700 (epoch 46.426), train_loss = 0.03418437, grad/param norm = 3.4490e-02, time/batch = 0.4226s	
2508/2700 (epoch 46.444), train_loss = 0.03549827, grad/param norm = 2.9917e-02, time/batch = 0.4080s	
2509/2700 (epoch 46.463), train_loss = 0.03294940, grad/param norm = 3.3781e-02, time/batch = 0.4261s	
2510/2700 (epoch 46.481), train_loss = 0.03334649, grad/param norm = 4.2197e-02, time/batch = 0.4732s	
2511/2700 (epoch 46.500), train_loss = 0.03437652, grad/param norm = 4.8154e-02, time/batch = 0.4592s	
2512/2700 (epoch 46.519), train_loss = 0.04025246, grad/param norm = 3.7065e-02, time/batch = 0.4510s	
2513/2700 (epoch 46.537), train_loss = 0.03826016, grad/param norm = 3.7723e-02, time/batch = 0.4325s	
2514/2700 (epoch 46.556), train_loss = 0.03296495, grad/param norm = 3.7240e-02, time/batch = 0.3880s	
2515/2700 (epoch 46.574), train_loss = 0.03943476, grad/param norm = 3.1938e-02, time/batch = 0.3832s	
2516/2700 (epoch 46.593), train_loss = 0.03304195, grad/param norm = 3.3006e-02, time/batch = 0.4116s	
2517/2700 (epoch 46.611), train_loss = 0.04211313, grad/param norm = 3.6267e-02, time/batch = 0.4320s	
2518/2700 (epoch 46.630), train_loss = 0.03288870, grad/param norm = 3.4288e-02, time/batch = 0.4340s	
2519/2700 (epoch 46.648), train_loss = 0.03693940, grad/param norm = 4.1291e-02, time/batch = 0.4247s	
2520/2700 (epoch 46.667), train_loss = 0.03618834, grad/param norm = 2.9440e-02, time/batch = 0.4246s	
2521/2700 (epoch 46.685), train_loss = 0.03578560, grad/param norm = 2.6476e-02, time/batch = 0.4068s	
2522/2700 (epoch 46.704), train_loss = 0.03430798, grad/param norm = 3.0458e-02, time/batch = 0.4620s	
2523/2700 (epoch 46.722), train_loss = 0.03210442, grad/param norm = 3.1752e-02, time/batch = 0.4769s	
2524/2700 (epoch 46.741), train_loss = 0.03695800, grad/param norm = 3.1089e-02, time/batch = 0.4256s	
2525/2700 (epoch 46.759), train_loss = 0.03701025, grad/param norm = 3.5004e-02, time/batch = 0.3767s	
2526/2700 (epoch 46.778), train_loss = 0.03653298, grad/param norm = 3.7007e-02, time/batch = 0.3743s	
2527/2700 (epoch 46.796), train_loss = 0.03735748, grad/param norm = 4.3482e-02, time/batch = 0.4236s	
2528/2700 (epoch 46.815), train_loss = 0.03303463, grad/param norm = 3.5310e-02, time/batch = 0.4398s	
2529/2700 (epoch 46.833), train_loss = 0.03554187, grad/param norm = 4.1307e-02, time/batch = 0.4519s	
2530/2700 (epoch 46.852), train_loss = 0.03567981, grad/param norm = 3.3469e-02, time/batch = 0.4334s	
2531/2700 (epoch 46.870), train_loss = 0.03376666, grad/param norm = 3.2029e-02, time/batch = 0.4116s	
2532/2700 (epoch 46.889), train_loss = 0.03034835, grad/param norm = 2.6712e-02, time/batch = 0.4048s	
2533/2700 (epoch 46.907), train_loss = 0.02970754, grad/param norm = 2.8856e-02, time/batch = 0.4389s	
2534/2700 (epoch 46.926), train_loss = 0.03003311, grad/param norm = 3.4782e-02, time/batch = 0.4745s	
2535/2700 (epoch 46.944), train_loss = 0.02916692, grad/param norm = 2.3033e-02, time/batch = 0.4609s	
2536/2700 (epoch 46.963), train_loss = 0.03134370, grad/param norm = 2.8034e-02, time/batch = 0.4199s	
2537/2700 (epoch 46.981), train_loss = 0.02703491, grad/param norm = 2.5649e-02, time/batch = 0.3773s	
decayed learning rate by a factor 0.97 to 0.00062857325411655	
2538/2700 (epoch 47.000), train_loss = 0.02404348, grad/param norm = 2.4401e-02, time/batch = 0.3823s	
2539/2700 (epoch 47.019), train_loss = 0.04081306, grad/param norm = 2.9221e-02, time/batch = 0.4161s	
2540/2700 (epoch 47.037), train_loss = 0.03372995, grad/param norm = 3.3650e-02, time/batch = 0.4620s	
2541/2700 (epoch 47.056), train_loss = 0.03473859, grad/param norm = 2.7119e-02, time/batch = 0.4499s	
2542/2700 (epoch 47.074), train_loss = 0.03295065, grad/param norm = 3.3623e-02, time/batch = 0.4139s	
2543/2700 (epoch 47.093), train_loss = 0.02795069, grad/param norm = 2.5728e-02, time/batch = 0.4020s	
2544/2700 (epoch 47.111), train_loss = 0.02523516, grad/param norm = 2.7422e-02, time/batch = 0.4371s	
2545/2700 (epoch 47.130), train_loss = 0.02585564, grad/param norm = 2.7340e-02, time/batch = 0.4546s	
2546/2700 (epoch 47.148), train_loss = 0.02940354, grad/param norm = 2.7350e-02, time/batch = 0.4645s	
2547/2700 (epoch 47.167), train_loss = 0.03677717, grad/param norm = 3.7446e-02, time/batch = 0.4182s	
2548/2700 (epoch 47.185), train_loss = 0.02665556, grad/param norm = 2.6998e-02, time/batch = 0.3835s	
2549/2700 (epoch 47.204), train_loss = 0.02867571, grad/param norm = 3.0656e-02, time/batch = 0.3785s	
2550/2700 (epoch 47.222), train_loss = 0.02207271, grad/param norm = 2.9070e-02, time/batch = 0.4152s	
2551/2700 (epoch 47.241), train_loss = 0.02861088, grad/param norm = 3.3209e-02, time/batch = 0.4406s	
2552/2700 (epoch 47.259), train_loss = 0.03372739, grad/param norm = 2.9267e-02, time/batch = 0.4528s	
2553/2700 (epoch 47.278), train_loss = 0.03393657, grad/param norm = 3.6953e-02, time/batch = 0.4353s	
2554/2700 (epoch 47.296), train_loss = 0.03364114, grad/param norm = 4.0053e-02, time/batch = 0.4131s	
2555/2700 (epoch 47.315), train_loss = 0.03321526, grad/param norm = 3.0112e-02, time/batch = 0.4264s	
2556/2700 (epoch 47.333), train_loss = 0.02821822, grad/param norm = 3.6743e-02, time/batch = 0.4522s	
2557/2700 (epoch 47.352), train_loss = 0.03033745, grad/param norm = 3.3771e-02, time/batch = 0.4463s	
2558/2700 (epoch 47.370), train_loss = 0.02217256, grad/param norm = 2.5093e-02, time/batch = 0.4393s	
2559/2700 (epoch 47.389), train_loss = 0.03382691, grad/param norm = 4.6363e-02, time/batch = 0.3921s	
2560/2700 (epoch 47.407), train_loss = 0.04048785, grad/param norm = 4.0968e-02, time/batch = 0.3745s	
2561/2700 (epoch 47.426), train_loss = 0.03401893, grad/param norm = 3.2132e-02, time/batch = 0.3606s	
2562/2700 (epoch 47.444), train_loss = 0.03668836, grad/param norm = 3.0090e-02, time/batch = 0.4472s	
2563/2700 (epoch 47.463), train_loss = 0.03039291, grad/param norm = 2.7815e-02, time/batch = 0.4615s	
2564/2700 (epoch 47.481), train_loss = 0.02844050, grad/param norm = 3.4326e-02, time/batch = 0.4364s	
2565/2700 (epoch 47.500), train_loss = 0.02754832, grad/param norm = 3.2543e-02, time/batch = 0.4216s	
2566/2700 (epoch 47.519), train_loss = 0.03739896, grad/param norm = 3.4918e-02, time/batch = 0.4257s	
2567/2700 (epoch 47.537), train_loss = 0.03314820, grad/param norm = 3.5931e-02, time/batch = 0.4450s	
2568/2700 (epoch 47.556), train_loss = 0.03338023, grad/param norm = 3.5994e-02, time/batch = 0.4476s	
2569/2700 (epoch 47.574), train_loss = 0.04066788, grad/param norm = 4.1198e-02, time/batch = 0.4548s	
2570/2700 (epoch 47.593), train_loss = 0.04099633, grad/param norm = 4.4600e-02, time/batch = 0.4262s	
2571/2700 (epoch 47.611), train_loss = 0.04263906, grad/param norm = 4.2447e-02, time/batch = 0.4318s	
2572/2700 (epoch 47.630), train_loss = 0.03367533, grad/param norm = 3.9159e-02, time/batch = 0.3624s	
2573/2700 (epoch 47.648), train_loss = 0.03549098, grad/param norm = 4.2599e-02, time/batch = 0.3805s	
2574/2700 (epoch 47.667), train_loss = 0.03760516, grad/param norm = 3.5322e-02, time/batch = 0.4277s	
2575/2700 (epoch 47.685), train_loss = 0.03726804, grad/param norm = 3.8205e-02, time/batch = 0.4543s	
2576/2700 (epoch 47.704), train_loss = 0.03531005, grad/param norm = 3.2484e-02, time/batch = 0.4312s	
2577/2700 (epoch 47.722), train_loss = 0.03303102, grad/param norm = 3.0641e-02, time/batch = 0.4085s	
2578/2700 (epoch 47.741), train_loss = 0.03343374, grad/param norm = 3.3117e-02, time/batch = 0.4280s	
2579/2700 (epoch 47.759), train_loss = 0.03251350, grad/param norm = 3.1817e-02, time/batch = 0.4510s	
2580/2700 (epoch 47.778), train_loss = 0.03284640, grad/param norm = 3.2242e-02, time/batch = 0.4627s	
2581/2700 (epoch 47.796), train_loss = 0.03286666, grad/param norm = 4.1670e-02, time/batch = 0.4484s	
2582/2700 (epoch 47.815), train_loss = 0.03200480, grad/param norm = 3.5797e-02, time/batch = 0.4466s	
2583/2700 (epoch 47.833), train_loss = 0.03048789, grad/param norm = 2.8870e-02, time/batch = 0.4058s	
2584/2700 (epoch 47.852), train_loss = 0.03261368, grad/param norm = 2.9058e-02, time/batch = 0.3733s	
2585/2700 (epoch 47.870), train_loss = 0.03190492, grad/param norm = 3.6068e-02, time/batch = 0.3893s	
2586/2700 (epoch 47.889), train_loss = 0.03246099, grad/param norm = 2.8016e-02, time/batch = 0.4323s	
2587/2700 (epoch 47.907), train_loss = 0.02845253, grad/param norm = 3.5117e-02, time/batch = 0.4478s	
2588/2700 (epoch 47.926), train_loss = 0.03114885, grad/param norm = 2.6966e-02, time/batch = 0.4208s	
2589/2700 (epoch 47.944), train_loss = 0.02783658, grad/param norm = 2.8907e-02, time/batch = 0.4071s	
2590/2700 (epoch 47.963), train_loss = 0.02923937, grad/param norm = 2.6077e-02, time/batch = 0.4314s	
2591/2700 (epoch 47.981), train_loss = 0.02706935, grad/param norm = 2.6673e-02, time/batch = 0.4439s	
decayed learning rate by a factor 0.97 to 0.00060971605649306	
2592/2700 (epoch 48.000), train_loss = 0.02263934, grad/param norm = 2.2784e-02, time/batch = 0.4586s	
2593/2700 (epoch 48.019), train_loss = 0.03686661, grad/param norm = 3.2221e-02, time/batch = 0.4707s	
2594/2700 (epoch 48.037), train_loss = 0.03251795, grad/param norm = 3.0685e-02, time/batch = 0.3516s	
2595/2700 (epoch 48.056), train_loss = 0.03054068, grad/param norm = 2.8919e-02, time/batch = 0.3863s	
2596/2700 (epoch 48.074), train_loss = 0.02936107, grad/param norm = 2.6151e-02, time/batch = 0.4247s	
2597/2700 (epoch 48.093), train_loss = 0.02584764, grad/param norm = 2.6991e-02, time/batch = 0.4494s	
2598/2700 (epoch 48.111), train_loss = 0.02219852, grad/param norm = 2.4144e-02, time/batch = 0.4196s	
2599/2700 (epoch 48.130), train_loss = 0.02464478, grad/param norm = 3.7297e-02, time/batch = 0.4031s	
2600/2700 (epoch 48.148), train_loss = 0.02751440, grad/param norm = 3.3501e-02, time/batch = 0.4299s	
2601/2700 (epoch 48.167), train_loss = 0.03485512, grad/param norm = 3.7694e-02, time/batch = 0.4423s	
2602/2700 (epoch 48.185), train_loss = 0.02465351, grad/param norm = 2.7626e-02, time/batch = 0.4577s	
2603/2700 (epoch 48.204), train_loss = 0.02755874, grad/param norm = 3.1634e-02, time/batch = 0.4723s	
2604/2700 (epoch 48.222), train_loss = 0.02539830, grad/param norm = 3.2646e-02, time/batch = 0.4635s	
2605/2700 (epoch 48.241), train_loss = 0.02671317, grad/param norm = 2.8313e-02, time/batch = 0.3911s	
2606/2700 (epoch 48.259), train_loss = 0.03014426, grad/param norm = 2.8911e-02, time/batch = 0.3599s	
2607/2700 (epoch 48.278), train_loss = 0.03025583, grad/param norm = 3.3938e-02, time/batch = 0.4208s	
2608/2700 (epoch 48.296), train_loss = 0.02705056, grad/param norm = 3.5676e-02, time/batch = 0.4349s	
2609/2700 (epoch 48.315), train_loss = 0.03314492, grad/param norm = 3.1434e-02, time/batch = 0.4066s	
2610/2700 (epoch 48.333), train_loss = 0.02997355, grad/param norm = 3.4173e-02, time/batch = 0.4060s	
2611/2700 (epoch 48.352), train_loss = 0.02876209, grad/param norm = 3.6237e-02, time/batch = 0.4208s	
2612/2700 (epoch 48.370), train_loss = 0.02297321, grad/param norm = 3.3493e-02, time/batch = 0.4493s	
2613/2700 (epoch 48.389), train_loss = 0.02978917, grad/param norm = 3.5546e-02, time/batch = 0.4721s	
2614/2700 (epoch 48.407), train_loss = 0.03627915, grad/param norm = 3.5509e-02, time/batch = 0.4805s	
2615/2700 (epoch 48.426), train_loss = 0.03350030, grad/param norm = 3.1765e-02, time/batch = 0.4540s	
2616/2700 (epoch 48.444), train_loss = 0.03213382, grad/param norm = 2.8000e-02, time/batch = 0.4223s	
2617/2700 (epoch 48.463), train_loss = 0.03217696, grad/param norm = 3.2093e-02, time/batch = 0.3937s	
2618/2700 (epoch 48.481), train_loss = 0.02654448, grad/param norm = 2.8989e-02, time/batch = 0.3708s	
2619/2700 (epoch 48.500), train_loss = 0.02556626, grad/param norm = 2.8124e-02, time/batch = 0.4301s	
2620/2700 (epoch 48.519), train_loss = 0.03576106, grad/param norm = 3.0616e-02, time/batch = 0.4242s	
2621/2700 (epoch 48.537), train_loss = 0.03927780, grad/param norm = 4.4941e-02, time/batch = 0.3888s	
2622/2700 (epoch 48.556), train_loss = 0.02886630, grad/param norm = 2.9023e-02, time/batch = 0.3663s	
2623/2700 (epoch 48.574), train_loss = 0.03567637, grad/param norm = 3.1884e-02, time/batch = 0.4345s	
2624/2700 (epoch 48.593), train_loss = 0.03070682, grad/param norm = 3.3819e-02, time/batch = 0.4589s	
2625/2700 (epoch 48.611), train_loss = 0.03407548, grad/param norm = 3.1063e-02, time/batch = 0.4697s	
2626/2700 (epoch 48.630), train_loss = 0.03633796, grad/param norm = 3.7208e-02, time/batch = 0.4614s	
2627/2700 (epoch 48.648), train_loss = 0.03504651, grad/param norm = 3.4266e-02, time/batch = 0.4457s	
2628/2700 (epoch 48.667), train_loss = 0.03511960, grad/param norm = 3.1207e-02, time/batch = 0.4277s	
2629/2700 (epoch 48.685), train_loss = 0.02902912, grad/param norm = 2.5979e-02, time/batch = 0.4102s	
2630/2700 (epoch 48.704), train_loss = 0.02713145, grad/param norm = 2.4842e-02, time/batch = 0.4029s	
2631/2700 (epoch 48.722), train_loss = 0.02970207, grad/param norm = 2.4892e-02, time/batch = 0.4525s	
2632/2700 (epoch 48.741), train_loss = 0.03436411, grad/param norm = 2.7230e-02, time/batch = 0.4303s	
2633/2700 (epoch 48.759), train_loss = 0.02414001, grad/param norm = 2.5392e-02, time/batch = 0.3956s	
2634/2700 (epoch 48.778), train_loss = 0.03020371, grad/param norm = 3.1208e-02, time/batch = 0.3618s	
2635/2700 (epoch 48.796), train_loss = 0.03127719, grad/param norm = 3.0290e-02, time/batch = 0.4044s	
2636/2700 (epoch 48.815), train_loss = 0.02963524, grad/param norm = 4.3650e-02, time/batch = 0.4530s	
2637/2700 (epoch 48.833), train_loss = 0.02838347, grad/param norm = 3.7803e-02, time/batch = 0.4567s	
2638/2700 (epoch 48.852), train_loss = 0.03197338, grad/param norm = 3.6641e-02, time/batch = 0.4578s	
2639/2700 (epoch 48.870), train_loss = 0.03130636, grad/param norm = 2.9714e-02, time/batch = 0.4303s	
2640/2700 (epoch 48.889), train_loss = 0.02736588, grad/param norm = 2.9338e-02, time/batch = 0.4124s	
2641/2700 (epoch 48.907), train_loss = 0.02789789, grad/param norm = 3.9953e-02, time/batch = 0.4070s	
2642/2700 (epoch 48.926), train_loss = 0.02928052, grad/param norm = 2.8077e-02, time/batch = 0.4289s	
2643/2700 (epoch 48.944), train_loss = 0.02859216, grad/param norm = 3.2753e-02, time/batch = 0.4613s	
2644/2700 (epoch 48.963), train_loss = 0.02869830, grad/param norm = 2.5488e-02, time/batch = 0.4389s	
2645/2700 (epoch 48.981), train_loss = 0.02792674, grad/param norm = 2.8050e-02, time/batch = 0.3950s	
decayed learning rate by a factor 0.97 to 0.00059142457479826	
2646/2700 (epoch 49.000), train_loss = 0.02298743, grad/param norm = 3.0734e-02, time/batch = 0.3695s	
2647/2700 (epoch 49.019), train_loss = 0.04242513, grad/param norm = 4.6079e-02, time/batch = 0.4005s	
2648/2700 (epoch 49.037), train_loss = 0.03364142, grad/param norm = 3.7295e-02, time/batch = 0.4370s	
2649/2700 (epoch 49.056), train_loss = 0.03030896, grad/param norm = 2.9847e-02, time/batch = 0.4661s	
2650/2700 (epoch 49.074), train_loss = 0.03106972, grad/param norm = 2.7141e-02, time/batch = 0.4410s	
2651/2700 (epoch 49.093), train_loss = 0.02422861, grad/param norm = 3.2080e-02, time/batch = 0.4210s	
2652/2700 (epoch 49.111), train_loss = 0.02628718, grad/param norm = 3.6244e-02, time/batch = 0.3864s	
2653/2700 (epoch 49.130), train_loss = 0.02695991, grad/param norm = 3.5507e-02, time/batch = 0.4341s	
2654/2700 (epoch 49.148), train_loss = 0.02802650, grad/param norm = 3.3019e-02, time/batch = 0.4414s	
2655/2700 (epoch 49.167), train_loss = 0.03172932, grad/param norm = 2.9899e-02, time/batch = 0.4530s	
2656/2700 (epoch 49.185), train_loss = 0.02480523, grad/param norm = 2.7201e-02, time/batch = 0.4114s	
2657/2700 (epoch 49.204), train_loss = 0.02738873, grad/param norm = 3.0248e-02, time/batch = 0.3731s	
2658/2700 (epoch 49.222), train_loss = 0.02130869, grad/param norm = 2.9215e-02, time/batch = 0.3805s	
2659/2700 (epoch 49.241), train_loss = 0.02238871, grad/param norm = 2.5167e-02, time/batch = 0.4254s	
2660/2700 (epoch 49.259), train_loss = 0.03262617, grad/param norm = 3.1946e-02, time/batch = 0.4660s	
2661/2700 (epoch 49.278), train_loss = 0.03247712, grad/param norm = 3.7951e-02, time/batch = 0.4541s	
2662/2700 (epoch 49.296), train_loss = 0.03009140, grad/param norm = 4.2539e-02, time/batch = 0.4402s	
2663/2700 (epoch 49.315), train_loss = 0.03114958, grad/param norm = 3.0543e-02, time/batch = 0.4245s	
2664/2700 (epoch 49.333), train_loss = 0.02463748, grad/param norm = 2.6453e-02, time/batch = 0.4112s	
2665/2700 (epoch 49.352), train_loss = 0.02976568, grad/param norm = 3.3786e-02, time/batch = 0.4335s	
2666/2700 (epoch 49.370), train_loss = 0.02440648, grad/param norm = 3.2781e-02, time/batch = 0.4473s	
2667/2700 (epoch 49.389), train_loss = 0.02616708, grad/param norm = 3.1040e-02, time/batch = 0.4698s	
2668/2700 (epoch 49.407), train_loss = 0.03511314, grad/param norm = 4.2981e-02, time/batch = 0.4356s	
2669/2700 (epoch 49.426), train_loss = 0.02939481, grad/param norm = 3.6658e-02, time/batch = 0.3875s	
2670/2700 (epoch 49.444), train_loss = 0.03279831, grad/param norm = 3.0287e-02, time/batch = 0.3480s	
2671/2700 (epoch 49.463), train_loss = 0.02750386, grad/param norm = 2.6987e-02, time/batch = 0.4170s	
2672/2700 (epoch 49.481), train_loss = 0.02455859, grad/param norm = 2.7275e-02, time/batch = 0.4493s	
2673/2700 (epoch 49.500), train_loss = 0.02801561, grad/param norm = 3.9469e-02, time/batch = 0.4643s	
2674/2700 (epoch 49.519), train_loss = 0.03134995, grad/param norm = 2.8176e-02, time/batch = 0.4448s	
2675/2700 (epoch 49.537), train_loss = 0.03602862, grad/param norm = 3.6357e-02, time/batch = 0.4280s	
2676/2700 (epoch 49.556), train_loss = 0.03684477, grad/param norm = 4.1044e-02, time/batch = 0.4178s	
2677/2700 (epoch 49.574), train_loss = 0.03720267, grad/param norm = 3.0681e-02, time/batch = 0.4381s	
2678/2700 (epoch 49.593), train_loss = 0.02823518, grad/param norm = 3.6222e-02, time/batch = 0.4349s	
2679/2700 (epoch 49.611), train_loss = 0.03375781, grad/param norm = 3.2639e-02, time/batch = 0.4519s	
2680/2700 (epoch 49.630), train_loss = 0.02875545, grad/param norm = 2.4838e-02, time/batch = 0.4069s	
2681/2700 (epoch 49.648), train_loss = 0.02852319, grad/param norm = 3.4961e-02, time/batch = 0.3589s	
2682/2700 (epoch 49.667), train_loss = 0.03042321, grad/param norm = 2.7077e-02, time/batch = 0.3774s	
2683/2700 (epoch 49.685), train_loss = 0.03383369, grad/param norm = 2.7702e-02, time/batch = 0.4299s	
2684/2700 (epoch 49.704), train_loss = 0.02659822, grad/param norm = 2.5554e-02, time/batch = 0.4581s	
2685/2700 (epoch 49.722), train_loss = 0.02688744, grad/param norm = 2.0168e-02, time/batch = 0.4687s	
2686/2700 (epoch 49.741), train_loss = 0.02731204, grad/param norm = 2.4781e-02, time/batch = 0.4626s	
2687/2700 (epoch 49.759), train_loss = 0.02455186, grad/param norm = 2.7650e-02, time/batch = 0.4393s	
2688/2700 (epoch 49.778), train_loss = 0.02780548, grad/param norm = 2.3963e-02, time/batch = 0.4155s	
2689/2700 (epoch 49.796), train_loss = 0.02832405, grad/param norm = 3.2469e-02, time/batch = 0.4207s	
2690/2700 (epoch 49.815), train_loss = 0.02846308, grad/param norm = 4.5853e-02, time/batch = 0.4178s	
2691/2700 (epoch 49.833), train_loss = 0.02956304, grad/param norm = 3.4678e-02, time/batch = 0.4491s	
2692/2700 (epoch 49.852), train_loss = 0.03106625, grad/param norm = 4.8995e-02, time/batch = 0.3793s	
2693/2700 (epoch 49.870), train_loss = 0.03170927, grad/param norm = 3.8630e-02, time/batch = 0.3579s	
2694/2700 (epoch 49.889), train_loss = 0.02674071, grad/param norm = 2.8925e-02, time/batch = 0.3418s	
2695/2700 (epoch 49.907), train_loss = 0.02666059, grad/param norm = 3.1427e-02, time/batch = 0.3429s	
2696/2700 (epoch 49.926), train_loss = 0.03161706, grad/param norm = 3.8300e-02, time/batch = 0.3533s	
2697/2700 (epoch 49.944), train_loss = 0.03152667, grad/param norm = 3.9587e-02, time/batch = 0.2951s	
2698/2700 (epoch 49.963), train_loss = 0.03497361, grad/param norm = 3.4038e-02, time/batch = 0.2387s	
2699/2700 (epoch 49.981), train_loss = 0.02561951, grad/param norm = 3.1665e-02, time/batch = 0.3268s	
decayed learning rate by a factor 0.97 to 0.00057368183755432	
evaluating loss over split index 2	
1/3...	
2/3...	
3/3...	
saving checkpoint to cv/lm_lstm_epoch50.00_3.1279.t7	
2700/2700 (epoch 50.000), train_loss = 0.02552949, grad/param norm = 3.9864e-02, time/batch = 0.3547s	
